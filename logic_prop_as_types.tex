\chapter{Logic and Proposition as Types}

\section{First Order Logic}
Logic is the study of reasoning, branching into various systems.
We refer to \textbf{classical logic} as the one that underpins much
of traditional mathematics.
It's the logic of the ancient Greeks (not fair) and truthtables, and it remains
used nowadays for pedagogical reasons.
We first introduce \textbf{propositional logic}, which is the simplest
form of classical logic.
Later we will extend this to \textbf{predicate (or first-order) logic}, which includes
\textbf{predicates} and \textbf{quantifiers}.
In this setting, a \textbf{proposition} is a statement that is either true or false,
and a \textbf{proof} is a logical argument that establishes the truth of a
proposition.
Propositions can be combined with logical \textbf{connectives} such as ``and'' ($\wedge$),
``or'' ($\vee$), ``not'' ($\neg$),``false'' ($\bot$), ,``true'' ($\top$) ``implies'' ($\Rightarrow$),  and ``if and only if'' ($\Leftrightarrow$).
These connectives allow the creation of complex or compound propositions.
\newpage
Here how connectives are defined in Lean:
\begin{example}[LogicaL connectives in Lean]\mbox{}
  \begin{lstlisting}[language=lean]
    #check And (a b : Prop) : Prop
    #check Or (a b : Prop) : Prop
    #check True : Prop
    #check False : Prop
    #check Not (a : Prop) : Prop
    #check Iff (a b : Prop) : Prop
  \end{lstlisting}
  \lstinline[language=lean]|Prop| stands for proposition, and it is an
  essential component of Lean’s type system.
  For now, we can think of it as a special type whose
  inhabitants are proofs; somewhat
  paradoxically, a type of types.
\end{example}
Logic is often formalized through a framework known as the \textbf{natural deduction system},
developed by Gentzen in the 1930s (\cite{wadler2015propositions}).
This approach brings logic closer to a computable, algorithmic system.
It specifies rules for deriving
\textbf{conclusions} from \textbf{premises} (assumptions from other propositions),
called \textbf{inference rules}.
\begin{example}[Deductive style rule]
  Here is an hypothetical example of inference rule.
  \begin{prooftree}
    \AxiomC{$P_1$}
    \AxiomC{$P_2$}
    \AxiomC{$\cdots$}
    \AxiomC{$P_n$}
    \QuaternaryInfC{$C$}
  \end{prooftree}
  Where the $P_1, P_2, \ldots, P_n$, above the line, are hypothetical premises and, the hypothetical conclusion $C$ is below the line.
\end{example}
The inference rules needed are:
\begin{itemize}
  \item \textbf{Introduction rules} specify how to form compound propositions from simpler ones, and
  \item \textbf{Elimination rules} specify how to use compound propositions to derive information about their components.
\end{itemize}
Let's look at how we can define some connectives first using natural deduction.
\paragraph{Conjunction ($\land$)}
\paragraph{Introduction Rule}
\begin{prooftree}
  \AxiomC{$A$}
  \AxiomC{$B$}
  \RightLabel{$\land$-Intro}
  \BinaryInfC{$A \land B$}
\end{prooftree}
\paragraph{Elimination Rule}
\mbox{}\\[0.5em]
\noindent
\begin{minipage}[t]{0.48\textwidth}
  \vspace{0pt}
  \begin{prooftree}
    \AxiomC{$A \land B$}
    \RightLabel{$\land$-Elim$_1$}
    \UnaryInfC{$A$}
  \end{prooftree}
\end{minipage}\hfill
\begin{minipage}[t]{0.48\textwidth}
  \vspace{0pt}
  \begin{prooftree}
    \AxiomC{$A \land B$}
    \RightLabel{$\land$-Elim$_2$}
    \UnaryInfC{$B$}
  \end{prooftree}
\end{minipage}
\paragraph{Disjunction ($\lor$)}
\paragraph{Introduction Rule}
\mbox{}\\[0.5em]
\noindent
\begin{minipage}[t]{0.48\textwidth}
  \vspace{0pt}
  \begin{prooftree}
    \AxiomC{$A$}
    \RightLabel{$\lor$-Intro$_1$}
    \UnaryInfC{$A \lor B$}
  \end{prooftree}
\end{minipage}\hfill
\begin{minipage}[t]{0.48\textwidth}
  \vspace{0pt}
  \begin{prooftree}
    \AxiomC{$B$}
    \RightLabel{$\lor$-Intro$_2$}
    \UnaryInfC{$A \lor B$}
  \end{prooftree}
\end{minipage}
\paragraph{ Elimination (Proof by cases)}
\begin{prooftree}
  \AxiomC{$A \lor B$}
  \AxiomC{$[A] \vdash C$}
  \AxiomC{$[B] \vdash C$}
  \RightLabel{$\lor$-Elim}
  \TrinaryInfC{$C$}
\end{prooftree}
\paragraph{Implication ($\to$)}
\paragraph{Introduction Rule}
\begin{prooftree}
  \AxiomC{$[A] \vdash B$}
  \RightLabel{$\to$-Intro}
  \UnaryInfC{$A \to B$}
\end{prooftree}
\paragraph{ Elimination (Modus Ponens)}
\begin{prooftree}
  \AxiomC{$A \to B$}
  \AxiomC{$A$}
  \RightLabel{$\to$-Elim}
  \BinaryInfC{$B$}
\end{prooftree}
\begin{notation}
  We use $A \vdash B$ (called turnstile) to designate a
  deduction of $B$ from $A$.
  It is employed in Gentzen’s \textbf{sequent calculus}
  (\cite{girard1989proofs})
  and moslty used in type theory.
  The square brackets around a premise $[A]$ mean that the premise $A$ is meant to
  be \textbf{discharged} at the conclusion. The classical example is the
  introduction rule for the implication connective.
  To prove an implication $A \to B$, we assume $A$
  (shown as $[A]$), derive $B$ under this assumption, and then discharge the
  assumption $A$ to conclude that $A \to B$ holds without the assumption.
  The turnstile is predominantly used in judgments and type theory with
  the meaning of ``entails that''.
\end{notation}
\section{Primitive Types}
Type theory employs this porocedure too,
by referring to deduction
rules as \textbf{judments}.
A type judgment has the form $\Gamma \vdash t : T$,
meaning: under \textbf{context} $\Gamma$ (a list of typed variables),
the term $t$ has type $T$.
Using formal inference rules in the type judgment
system, such as \textbf{introduction} and \textbf{elimination} rules,
we can construct new compound types from existing ones.
\begin{example}[Judgment style rule]
  \mbox{}
  \begin{prooftree}
    \AxiomC{$\Gamma \vdash$}
    \AxiomC{$p_1:P_1$}
    \AxiomC{$p_2:P_2$}
    \AxiomC{$\cdots$}
    \AxiomC{$P_n$}
    \QuinaryInfC{$C$}
  \end{prooftree}
\end{example}
Technically, there are two more inference rules that we will not consider in this setting:
\textbf{formation rules}, used to declare that a type is well-formed, and
\textbf{computation rules}, which specify how a term will be evaluated.
Moreover, without going too deep into the jargon,
one specific judgment is
$\Gamma \vdash A \equiv B\ \text{type}$, which means ``types $A$ and $B$ are
\textbf{judgmentally (or definitionally) equal} in context $\Gamma$.''
Similarly for terms, $\Gamma \vdash t_1 \equiv t_2 : A$ means ``terms $t_1$ and $t_2$ are
judgmentally equal of type $A$ in context $\Gamma$.''
In Lean, the operator \lstinline[language=lean]|:=|
stands for definitional equality and is used by the kernel to verify proof equality.

Let's now construct new types from given types $A$ and $B$.
\paragraph{Product Type}
As a fundamental example, $A \times B$
denotes the type of pairs $(a, b)$ where $a : A$ and $b : B$,
called the \textbf{product type}.
\paragraph{Introduction Rule (pairing)}
\begin{prooftree}
  \AxiomC{$a : A$}
  \AxiomC{$b : B$}
  \BinaryInfC{$(a, b) : A \times B$}
\end{prooftree}

\noindent In Lean:
\begin{lstlisting}[language=lean]
Prod.mk a b : Prod A B   -- or A × B
(a, b) : A × B           
⟨a, b⟩ : A × B           
\end{lstlisting}
\paragraph{Elimination Rules (projections)}\mbox{}\\[0.5em]
\noindent
\begin{minipage}[t]{0.48\textwidth}
  \vspace{0pt}
  \begin{prooftree}
    \AxiomC{$p : A \times B$}
    \UnaryInfC{$\mathsf{fst}(p) : A$}
  \end{prooftree}
\end{minipage}\hfill
\begin{minipage}[t]{0.48\textwidth}
  \vspace{0pt}
  \begin{prooftree}
    \AxiomC{$p : A \times B$}
    \UnaryInfC{$\mathsf{snd}(p) : B$}
  \end{prooftree}
\end{minipage}

\noindent In Lean:
\begin{lstlisting}[language=lean]
  p.1 : A       -- or Prod.fst p
  p.2 : B       -- or Prod.snd p
\end{lstlisting}
\paragraph{Sum Type}
The \textbf{sum type} $A + B$ (also called a coproduct or disjoint union) consists of values that are
either of type $A$ (tagged with $\mathsf{inl}$) or
of type $B$ (tagged with $\mathsf{inr}$).
\paragraph{Introduction Rules (injections)}
\mbox{}\\[0.5em]
\noindent
\begin{minipage}[t]{0.48\textwidth}
  \vspace{0pt}
  \begin{prooftree}
    \AxiomC{$a : A$}
    \UnaryInfC{$\mathsf{inl}(a) : A + B$}
  \end{prooftree}
\end{minipage}\hfill
\begin{minipage}[t]{0.48\textwidth}
  \vspace{0pt}
  \begin{prooftree}
    \AxiomC{$b : B$}
    \UnaryInfC{$\mathsf{inr}(b) : A + B$}
  \end{prooftree}
\end{minipage}

\noindent In Lean:
\begin{lstlisting}[language=lean]
Sum.inl a : Sum A B   -- or A ⊕ B
Sum.inr b : Sum A B
\end{lstlisting}
\paragraph{Elimination Rule (case analysis)}
\begin{prooftree}
  \AxiomC{$p : A + B$}
  \AxiomC{$\begin{array}{c}  f : (A \implies C) \end{array}$}
  \AxiomC{$\begin{array}{c}  g : (B \implies C) \end{array}$}
  \TrinaryInfC{$\mathsf{cases}(p, f, g) : C$}
\end{prooftree}
In Lean, we can use the \lstinline[language=lean]|cases|:
\begin{lstlisting}[language=lean]
example (p : Sum A B) (f : A → C) (g : B → C) : C := by
  cases p with
  | inl x => f x
  | inr y => g y
\end{lstlisting}
\paragraph{Function Types}
The type of the form $A \to B$, used in the sum elimination rule
represents functions from $A$ to $B$.
\paragraph{Introduction Rule (lambda abstraction)}
\begin{prooftree}
  \AxiomC{$\begin{array}{c} x : A  \vdash  \Phi : B \end{array}$}
  \UnaryInfC{$\lambda x.\Phi : A \to B$}
\end{prooftree}
In Lean, lambda abstraction is written using \lstinline[language=lean]|fun| or \lstinline[language=lean]|λ|:
\begin{lstlisting}[language=lean]
fun (x : A) => Φ : A → B
-- or using λ notation
λ (x : A) => Φ : A → B
-- Example: identity function
def id : A → A := fun x => x
-- or
def id : A → A := λ x => x
\end{lstlisting}
\paragraph{Elimination Rule (application)}
\begin{prooftree}
  \AxiomC{$f : A \to B$}
  \AxiomC{$a : A$}
  \BinaryInfC{$f(a) : B$}
\end{prooftree}
In Lean, function application is written using juxtaposition:
\begin{lstlisting}[language=lean]
example (f : A → B) (a : A) : B := f a
\end{lstlisting}
Functions are a primitive concept in type theory,
and we provide a brief introduction here.
We can \textbf{apply} a function $f : A \to B$
to an element $a : A$ to obtain an element of $B$,
denoted $f(a)$. In type theory, it is common to omit
the parentheses and write the application simply
as $f\, a$.

There are two equivalent ways to construct function
types: either by direct definition or by using
$\lambda$-abstraction.
Introducing a function by definition means that
we introduce a function by giving it a name (let's say, $f$) and saying we define $f : A \to B$ by giving an equation
\begin{equation} \label{lambda_definition}
  f(x) \coloneqq \Phi
\end{equation}
where $x$ is a variable and $\Phi$ is an expression
which may use $x$. In order for this to be valid,
we have to check that $\Phi : B$ assuming $x : A$.
Now we can compute $f(a)$ by replacing the variable $x$ in $\Phi$ with $a$. As an example,
consider the function $f : \mathbb{N} \to \mathbb{N}$ which
is defined by $f(x) \coloneqq x + x$. Then $f(2)$ is \textbf{definitionally equal} to $2 + 2$.
If we don't want to introduce a name for the
function, we can use \textbf{$\lambda$-abstraction}.
Given an expression $\Phi$ of type $B$ which
may use $x : A$, as above, we write $\lambda(x : A). \Phi$ to
indicate the same function defined by
(\ref{lambda_definition}). Thus, we have
$$ (\lambda(x : A). \Phi) : A \to B. $$
% \begin{example}
%   The previously defined function  has the typing
%   judgment
%   $$ (\lambda(x : \mathbb{N}). x + x) : \mathbb{N} \to \mathbb{N}. $$
%   As another example, for any types $A$ and $B$
%   and any element $y : B$, we have a
%   \textbf{constant function}
%   $$ (\lambda(x : A). y) : A \to B. $$
%   The \textbf{identity function} on any
%   type $A$ is given by
%   $$ (\lambda(x : A). x) : A \to A. $$
% \end{example}
By convention, the ``scope'' of the variable
binding ``$\lambda x.$''
is the entire rest of the expression,
unless delimited with parentheses.
Thus, for instance, $\lambda x. x + x$
should be parsed as $\lambda x.(x + x)$,
not as $(\lambda x. x) + x$ .
Now a $\lambda$-abstraction is a function,
so we can apply it to an argument $a : A$.
We then have the following computation
rule ($\beta$-reduction), which is a
\textbf{definitional equality}:
$$ (\lambda x. \Phi)(a) \equiv \Phi' $$
where $\Phi'$ is the expression $\Phi$ in
which all occurrences of $x$ have been
replaced by $a$.
Continuing the above example, we have
$(\lambda x. x + x)(2) \equiv 2 + 2. $
% Note that from any function $f : A \to B$,
% we can construct a lambda abstraction
% function $\lambda x. f(x)$.
% Since this is by definition ``the function
% that applies $f$ to its argument'' we consider
% it to be definitionally
% equal to $f$ (\textbf{$\eta$-conversion}):
% $$ f \equiv (\lambda x. f(x)). $$
% This equality is the uniqueness principle
% for function types, because it shows that $f$
% is uniquely determined by its values.
% The introduction of functions by definitions
% with explicit parameters can be reduced to
% simple definitions by using $\lambda$-abstraction:
% i.e., we can read a definition of $f : A \to B$ by
% $$ f(x) \coloneqq \Phi
% $$
% as
% $$ f \coloneqq \lambda x. \Phi. $$
When performing calculations involving variables, we must carefully preserve the \textbf{binding structure} of expressions during substitution. Consider the function $f : \mathbb{N} \to (\mathbb{N} \to \mathbb{N})$ defined as:
$$ f(x) \coloneqq \lambda y. x + y $$
Suppose we have assumed $y : \mathbb{N}$ somewhere in our context. What is $f(y)$?
A naive approach would replace $x$ with $y$ directly in the expression $\lambda y. x + y$, yielding $\lambda y. y + y$. However, this substitution is \textbf{semantically incorrect} because it causes \textbf{variable capture}: the free variable $y$ (referring to our assumption) becomes bound by the $\lambda$-abstraction, fundamentally altering the expression's meaning.
The correct approach uses \textbf{$\alpha$-conversion} (variable renaming).
Since bound variables have only local scope, we can consistently rename them while preserving binding structure.
The expression $\lambda y. x + y$ is judgmentally equal to $\lambda z. x + z$ for any fresh variable $z$. Therefore:
$$ f(y) \equiv \lambda z. y + z $$
This phenomenon parallels familiar mathematical practice: if $f(x) \coloneqq \int_1^2 \frac{dt}{x-t}$,
then $f(t)$ equals $\int_1^2 \frac{ds}{t-s}$, not the ill-defined $\int_1^2 \frac{dt}{t-t}$.
Lambda abstractions bind dummy variables exactly as integrals do.
For functions of multiple variables, we employ \textbf{currying} (named after mathematician Haskell Curry). Instead of using product types, we represent a two-argument function as a function returning another function.
A function taking inputs $a : A$ and $b : B$ to produce output in $C$ has type:
$$ f : A \to (B \to C) \equiv A \to B \to C $$
where the arrow associates to the right by convention.
Given $a : A$ and $b : B$, we apply $f$ sequentially: first to $a$, then the result to $b$, obtaining $f(a)(b) : C$.
To simplify notation and avoid excessive parentheses, we adopt several conventions. We write $f(a)(b)$ as $f(a, b)$ for abbreviated application. Without parentheses entirely, $f \, a \, b$ means $(f \, a) \, b$ following left-associative application. For multi-parameter definitions, we write $f(x, y) \coloneqq \Phi$ where $\Phi : C$ under assumptions $x : A$ and $y : B$.
Using $\lambda$-abstraction, such definitions correspond to:
$$ f \coloneqq \lambda x. \lambda y. \Phi $$
Alternative notation using map symbols:
$$ f \coloneqq x \mapsto y \mapsto \Phi $$
This currying approach extends naturally to functions of three or more arguments, allowing us to represent any multi-argument function as a sequence of single-argument functions.
\begin{example}\mbox{}
  \begin{lstlisting}[language=lean]
def add : Nat -> (Nat -> Nat) := fun x => (fun y => x + y)
#eval add 3 4   -- Output: 7
\end{lstlisting}
  Theoretically, lambda evaluation proceeds in steps:
  \begin{align*}
    \text{add } 3\, 4 & \equiv (\text{add } 3)\, 4                         \\
                      & \equiv ((\lambda x.\, \lambda y.\, x + y)\, 3)\, 4 \\
                      & \equiv ((\lambda y.\, x + y)[x := 3])\, 4          \\
                      & \equiv (\lambda y.\, 3 + y)\, 4                    \\
                      & \equiv (3 + y)[y := 4]                             \\
                      & \equiv 3 + 4                                       \\
                      & \equiv 7
  \end{align*}
\end{example}
% \paragraph{Computation Rules}
% \[
%   \mathsf{match}\ \mathsf{inl}(a) \ \mathsf{with} \ \dots \equiv f(a)
%   \qquad
%   \mathsf{match}\ \mathsf{inr}(b) \ \mathsf{with} \ \dots \equiv g(b)
% \]

% \section{Judgments and Propositions}

% Logic is often formalized through a framework that distinguishes clearly between
% \textbf{judgments} and \textbf{propositions}, following Martin-Löf's foundational approach
% (\cite{martin-lof-1983}, \cite{plato:intuitionistic-type-theory}).
% A \textbf{judgment} represents something we may know — an object of knowledge that becomes
% evident once we have a proof of it.

% The most fundamental form of judgment in logic is ``\textit{A is true}'', where \( A \) is a
% proposition. This is formally written as \( A\ \text{true} \).  
% When we derive such judgments under assumptions, we write
% $$
% \Gamma \vdash A\ \text{true},
% $$
% where \( \Gamma \) represents our hypothetical assumptions
% (\cite{pfenning-natded}).

% \begin{example}[Judgment-Based Inference Rules]
% All logical reasoning can be expressed through \textbf{introduction} and
% \textbf{elimination} rules for judgments.
% For instance, conjunction is characterized as follows.

% \textbf{Introduction:} how to establish the judgment \( A \land B\ \text{true} \):

% \begin{prooftree}
%   \AxiomC{\(A\ \text{true}\)}
%   \AxiomC{\(B\ \text{true}\)}
%   \RightLabel{\(\land\text{-I}\)}
%   \BinaryInfC{\(A \land B\ \text{true}\)}
% \end{prooftree}

% \textbf{Elimination:} how to use the judgment \( A \land B\ \text{true} \):

% \begin{minipage}[t]{0.45\textwidth}
% \begin{prooftree}
%   \AxiomC{\(A \land B\ \text{true}\)}
%   \RightLabel{\(\land\text{-E}_1\)}
%   \UnaryInfC{\(A\ \text{true}\)}
% \end{prooftree}
% \end{minipage}\hfill
% \begin{minipage}[t]{0.45\textwidth}
% \begin{prooftree}
%   \AxiomC{\(A \land B\ \text{true}\)}
%   \RightLabel{\(\land\text{-E}_2\)}
%   \UnaryInfC{\(B\ \text{true}\)}
% \end{prooftree}
% \end{minipage}
% \end{example}

% \begin{notation}[The Turnstile Symbol]
% The symbol \( \vdash \) (the \textit{turnstile}) separates assumptions from conclusions
% in judgments.  
% $$
% \Gamma \vdash J
% $$
% means that the judgment \( J \) follows from the assumptions \( \Gamma \), or equivalently,
% that \( J \) is evident given evidence for \( \Gamma \).

% When assumptions are \textit{discharged} during reasoning, we indicate this with square
% brackets \([A]\).
% For example, to establish an implication \( A \rightarrow B \), we assume \( A \)
% (written \([A]\)), derive \( B \) under this assumption, then discharge \( A \):

% \begin{prooftree}
%   \AxiomC{\([A\ \text{true}]^{u}\)}
%   \AxiomC{\(\vdots\)}
%   \AxiomC{\(B\ \text{true}\)}
%   \RightLabel{\(\rightarrow\text{-I}^{u}\)}
%   \TrinaryInfC{\(A \rightarrow B\ \text{true}\)}
% \end{prooftree}
% \end{notation}

% This judgment-based framework extends naturally to \textbf{type theory}, where additional
% judgment forms are introduced.  
% While logic focuses on the judgment \( A\ \text{true} \),
% type theory introduces several fundamental forms 
% (\cite{plato:intuitionistic-type-theory}):

% \begin{align}
%   &\Gamma \vdash A\ \text{type}
%     && \text{(\(A\) is a well-formed type)} \notag \\
%   &\Gamma \vdash t : A
%     && \text{(\(t\) is a term of type \(A\))} \notag \\
%   &\Gamma \vdash A \equiv B\ \text{type}
%     && \text{(types \(A\) and \(B\) are judgmentally equal)} \notag \\
%   &\Gamma \vdash t_1 \equiv t_2 : A
%     && \text{(terms \(t_1\) and \(t_2\) are judgmentally equal)} \notag \\
% \end{align}

% The \textbf{context} \( \Gamma \) represents a list of assumptions about variables and
% their types:
% $$
% \Gamma = x_1 : A_1,\, x_2 : A_2,\, \ldots,\, x_n : A_n.
% $$

% \begin{example}[Unified Introduction/Elimination Pattern]
% Both logical connectives and type constructors follow the same
% \textit{introduction/elimination} pattern.
% For function types (corresponding to implication):

% \textbf{Formation:}
% \begin{prooftree}
%   \AxiomC{\(\Gamma \vdash A\ \text{type}\)}
%   \AxiomC{\(\Gamma \vdash B\ \text{type}\)}
%   \BinaryInfC{\(\Gamma \vdash A \rightarrow B\ \text{type}\)}
% \end{prooftree}

% \textbf{Introduction:}
% \begin{prooftree}
%   \AxiomC{\(\Gamma, x : A \vdash b : B\)}
%   \RightLabel{\(\rightarrow\text{-I}\)}
%   \UnaryInfC{\(\Gamma \vdash \lambda x : A.\, b : A \rightarrow B\)}
% \end{prooftree}

% \textbf{Elimination:}
% \begin{prooftree}
%   \AxiomC{\(\Gamma \vdash f : A \rightarrow B\)}
%   \AxiomC{\(\Gamma \vdash a : A\)}
%   \RightLabel{\(\rightarrow\text{-E}\)}
%   \BinaryInfC{\(\Gamma \vdash f(a) : B\)}
% \end{prooftree}

% This correspondence reveals a deep connection:
% logical implication and function types share the same structural rules,
% differing only in focus — whether on truth (\(A \rightarrow B\ \text{true}\))
% or on typing (\(\lambda x.\,b : A \rightarrow B\)).
% \end{example}

% \subsection*{References}
% Per Martin-Löf, \textit{Intuitionistic Type Theory}, 1983–1984. \\
% P. Dybjer, \textit{Intuitionistic Type Theory}, \textit{Stanford Encyclopedia of Philosophy}, 2016. \\
% Frank Pfenning, \textit{Logical Frameworks and Natural Deduction}, lecture notes. \\
% Additional formal presentations and lecture notes on type theory and natural deduction, CMU (various).


\section{Curry Howard isomorphism}
We have been preparing for this argument, and the reader will have surely
noticed a strong similarity when defining logical connectives
using deduction rules; they are remarkably similar to types
constructed using type judgments. For instance, function
types can be seen as implications.
This is not a coincidence, but rather a fundamental theorem
first proven by Haskell Curry and William Howard.
It forms the core of modern type theory and establishes
a deep connection between logic, computation, and mathematics.
The isomorphism states:
\begin{align*}
  \text{Propositions}        & \leftrightarrow \text{Types}              \\
  \text{Proofs}              & \leftrightarrow \text{Programs}           \\
  \text{Proof Normalization} & \leftrightarrow \text{Program Evaluation}
\end{align*}
\noindent\textbf{Implication} ($P \Rightarrow Q$) corresponds to the \textbf{function type} ($P \to Q$).
A proof of an implication is a function that transforms any proof
of the premise into a proof of the conclusion.
\noindent\textbf{Conjunction} ($P \land Q$) corresponds
to the \textbf{product type} ($P \times Q$).
A proof of a conjunction consists of a pair containing proofs of both conjuncts.
\noindent\textbf{Disjunction} ($P \lor Q$) corresponds
to the \textbf{sum type} ($P + Q$).
A proof of a disjunction is either a proof of the
first disjunct or a proof of the second disjunct.
Lean uses inference rules and type
judgments as well as computing connectives using each related type.
For instance, $A \land B$ can be represented as \lstinline[language=lean]|And(A, B)| or \lstinline[language=lean]|A ∧ B|.
Its introduction rule is constructed by
\lstinline[language=lean]|And.intro _ _| or simply
\lstinline[language=lean]|⟨_, _⟩| (underscores are placeholders).
The pair $A \land B$ can then be consumed using elimination
rules \lstinline[language=lean]|And.left| and \lstinline[language=lean]|And.right|.

\begin{example}\label{ex:conj_intro_2}
  Let's look at a simple Lean example:
  \begin{lstlisting}[language=lean]
    example (ha : a) (hb : b) : (a ∧ b) := And.intro ha hb
  \end{lstlisting}
  This means: given a proof of $a$ ( \lstinline[language=lean]|ha|)
  and a proof of $b$  ( \lstinline[language=lean]|hb|) ,
  we can form a proof of $(a \land b)$.
  \lstinline[language=lean]|And.intro| is implemented as:
  \begin{lstlisting}[language=lean]
    And.intro : p -> q -> (p ∧ q)
  \end{lstlisting}
  It says: if you give me a proof of $p$ and a proof of $q$,
  then I return a proof of $p \land q$.
  We therefore conclude the proof by directly giving
  \lstinline[language=lean]|And.intro ha hb|.
  Here is another way of writing the same statement:
  \begin{lstlisting}[language=lean]
    example (ha : a) (hb : b) : And(a, b) := ⟨ha, hb⟩
  \end{lstlisting}
\end{example}
For a more concrete example, let's look at how
proof normalization using a system of inference rules
corresponds to computation in Lean.
To reduce complexity of a \textbf{proof tree} in natural deduction,
one follows a
\textbf{top-down} approach,
unfolding each component to be proved step by step.
\begin{example}[Associativity of Conjunction]
  We prove that $(A \land B) \land C$ implies $A \land (B \land C)$.
  First, from the assumption $(A \land B) \land C$, we can derive $A$:
  \begin{prooftree}
    \AxiomC{$(A \land B) \land C$}
    \RightLabel{$\land E_1$}
    \UnaryInfC{$A \land B$}
    \RightLabel{$\land E_1$}
    \UnaryInfC{$A$}
  \end{prooftree}
  Second, we can derive $B \land C$:
  \begin{prooftree}
    \AxiomC{$(A \land B) \land C$}
    \RightLabel{$\land E_1$}
    \UnaryInfC{$A \land B$}
    \RightLabel{$\land E_2$}
    \UnaryInfC{$B$}
    \AxiomC{$(A \land B) \land C$}
    \RightLabel{$\land E_2$}
    \UnaryInfC{$C$}
    \RightLabel{$\land I$}
    \BinaryInfC{$B \land C$}
  \end{prooftree}
  Finally, combining these derivations we obtain $A \land (B \land C)$:
  \begin{prooftree}
    \AxiomC{$(A \land B) \land C \vdash A$}
    \AxiomC{$(A \land B) \land C \vdash B \land C$}
    \RightLabel{$\land I$}
    \BinaryInfC{$A \land (B \land C)$}
  \end{prooftree}
\end{example}
\begin{example}[Lean Implementation]
  Let us now implement the same proof in Lean.
  \newpage
  \begin{lstlisting}[language=lean]
theorem and_associative (a b c : Prop) : (a ∧ b) ∧ c → a ∧ (b ∧ c) :=
  fun h : (a ∧ b) ∧ c →
  -- First, from the assumption (a ∧ b) ∧ c, we can derive a:
  have hab : a ∧ b := h.left
  have ha : a := hab.left 
  -- Second, we can derive b ∧ c (here we only extract b and c and combine them in the next step)
  have hc : c := h.right
  have hb : b := hab.right
  -- Finally, combining these derivations we obtain a ∧ (b ∧ c)
  show a ∧ (b ∧ c) from ⟨ha, ⟨hb, hc⟩⟩
\end{lstlisting}
  We introduce the \lstinline[language=lean]|theorem| with the name
  \lstinline[language=lean]|and_associative|.
  The type signature \lstinline[language=lean]|(a ∧ b) ∧ c → a ∧ (b ∧ c)|
  represents our logical implication.
  Here, we construct the implication proof using a
  function (following the Curry Howard isomorphism) with the \lstinline[language=lean]|fun| keyword.
  The \lstinline[language=lean]|have| keyword introduces local
  lemmas within our proof scope, allowing us to break down complex
  reasoning into manageable intermediate steps,
  mirroring our natural deduction proof from before.
  Just before the keyword \lstinline[language=lean]|show|,
  the info view displays the following
  context and goal:
  \begin{lstlisting}[language=lean]
  a b c : Prop
  h : (a ∧ b) ∧ c
  hab : a ∧ b
  ha : a
  hc : c
  hb : b
  ⊢ a ∧ b ∧ c
\end{lstlisting}
  Finally, the \lstinline[language=lean]|show| keyword explicitly states what
  we are proving and verifies that our provided term has the correct type.
  In this case, \lstinline[language=lean]|show a ∧ (b ∧ c) from ⟨ha, ⟨hb, hc⟩⟩|
  asserts that we are constructing a proof of \lstinline[language=lean]|a ∧ (b ∧ c)|
  using the term \lstinline[language=lean]|⟨ha, ⟨hb, hc⟩⟩|.
  The \lstinline[language=lean]|show| keyword serves two
  purposes:
  it makes the proof more readable by explicitly
  documenting what is being proved at this step,
  and it performs a type check to ensure the provided
  proof term matches the stated
  goal up to \textbf{definitional equality}.
  Two types are definitionally equal in Lean when they
  are identical after computation
  and unfolding of definitions; in other words, when Lean's type checker
  can mechanically verify they are the same without requiring additional proof steps.
  Here, the goal \lstinline[language=lean]|⊢ a ∧ b ∧ c| is definitionally
  equal to \lstinline[language=lean]|a ∧ (b ∧ c)| due to how conjunction
  associates, so \lstinline[language=lean]|show| accepts this statement.
  If we had tried to use \lstinline[language=lean]|show| with a type that
  was only \textbf{propositionally} equal (requiring a proof to establish equality)
  but not definitionally equal, Lean would reject it.
\end{example}
\section{Predicate logic and dependency}
To capture more complex mathematical ideas, we extend our system from
propositional logic to \textbf{predicate logic}.
A \textbf{predicate} is a statement or proposition that depends on a variable.
In propositional logic we represent a proposition simply by $P$.
In predicate logic, this is generalized: a predicate is written as $P(a)$,
where $a$ is a variable. Notice that a predicate is just a function.
This extension allows us to introduce \textbf{quantifiers}:
$\forall$ (``for all'') and $\exists$ (``there exists'').
These quantifiers express that a given formula holds either for every object
or for at least one object, respectively.
In Lean if \lstinline[language=lean]|α| is any type, we can represent a
predicate \lstinline[language=lean]|P| on \lstinline[language=lean]|α| as
an object of type \lstinline[language=lean]|α → Prop|.
Thus given an \lstinline[language=lean]|x : α| (an element
with type \lstinline[language=lean]|α| )
\lstinline[language=lean]|P(x) : Prop| would be representative of a proposition
holding for \lstinline[language=lean]|x|.
% When introducing variables into a formal language we must keep in mind that the specific choice 
% of a variable name can be substituted without
% changing the meaning of the predicate or statement. This should feel familiar from mathematics,
% where the meaning of an expression does not depend on the names we assign to variables.
% Some variables are \textbf{bound} (constrained), while others remain \textbf{free} 
% (arbitrary, in programming often called "dummy" variables). 
% When substituting variables, it is important to ensure that this distinction is preserved.
% This phenomenon, called \textbf{variable capture}, parallels familiar mathematical practice: 
% if $f(x) \coloneqq \int_1^2 \frac{dt}{x-t}$, then $f(t)$ equals $\int_1^2 \frac{ds}{t-s}$, 
% not the ill-defined $\int_1^2 \frac{dt}{t-t}$. The same principle applies to predicate logic. For example, consider
% [[
% $\exists y.\,(y > x)$.
% ]]
% This states that for a given $x$ there exists a $y$ such that $y > x$. 
% If we naively substitute $y+1$ for $x$, we would obtain
% [[
% $\exists y.\,(y > y+1)$,
% ]]
% where the $y$ in $y+1$ has been \textbf{captured} by the quantifier $\exists y$. 
% This transforms the original statement from "there exists some $y$ greater than the free variable $x$" into the always-false statement 
% "there exists some $y$ greater than itself plus one."
% To avoid the probelm, in the above example, we would first rename the bound variable to something fresh 
% say $z$, obtaining $\exists z.\,(z > x)$, and then safely substitute to get $\exists z.\,(z > y+1)$.
% \begin{notation}
%   We use the notation $\phi[t/x]$ for \textbf{substitution}, meaning all occurrences of the free 
%   variable $x$ in formula (or expression) $\phi$ are replaced by term $t$.
% \end{notation}
% We can now present the inference rules for quantifiers.
% \paragraph{Universal Quantification ($\forall$)}
% \begin{itemize}
%     \item Introduction
%     \begin{prooftree}
%     \AxiomC{$A$}
%     \RightLabel{$\forall I$}
%     \UnaryInfC{$\forall x.\,A$}
%     \end{prooftree}
%     The variable $x$ must be arbitrary in the derivation of $A$. 
%     This rule captures statements like 
%     $\forall x \in \mathbb{N}$, $x$ has a successor, 
%     but would not apply to $\forall x \in \mathbb{N}$, $x$ is prime 
%     (since we cannot derive this for an arbitrary natural number).
%     \item Elimination
%     \begin{prooftree}
%     \AxiomC{$\forall x.\,A$}
%     \RightLabel{$\forall E$}
%     \UnaryInfC{$A[t/x]$}
%     \end{prooftree}
%     The conclusion $A[t/x]$ represents the substitution of term $t$ for variable $x$ in formula $A$. 
%     From a proof of $\forall x.\,A(x)$ we can infer $A(t)$ for any term $t$.
% \end{itemize}
% \paragraph{Existential Quantification ($\exists$)}
% \begin{itemize}
%     \item Introduction
%     \begin{prooftree}
%     \AxiomC{$A[t/x]$}
%     \RightLabel{$\exists I$}
%     \UnaryInfC{$\exists x.\,A$}
%     \end{prooftree}
%     The substitution premise means that if we can find a specific term $t$ for which $A(t)$ holds, 
%     then we can introduce the existential quantifier. 
%     The introduction rule requires a witness $t$ for which the predicate holds.
%     \item Elimination
%     \begin{prooftree}
%     \AxiomC{$\exists x.\,A$}
%     \AxiomC{$[A] \vdash B$}
%     \RightLabel{$\exists E$}
%     \BinaryInfC{$B$}
%     \end{prooftree}
%     To eliminate an existential quantifier, we assume $A$ holds for some witness 
%     and derive $B$ without making any assumptions about the specific witness.
% \end{itemize}

We can give an informal reading of the quantifiers as infinite logical operations:
\begin{align*}
  \forall x.\,P(x) & \equiv P(a) \land P(b) \land P(c) \land \ldots \\
  \exists x.\,P(x) & \equiv P(a) \lor P(b) \lor P(c) \lor \ldots
\end{align*}
The expression $\forall x.\, P(x)$ can be understood as a generalized form of conjunction.
It expresses that $P$ holds for all possible values of $x$.
Similarly, $\exists x.\, P(x)$ is a generalized disjunction, expressing that $P$ holds
for at least one value of $x$.
Under the Curry-Howard isomorphism, universal quantifiers correspond to
\textbf{dependent function types} (also called Pi types, written $\Pi$),
while existential quantifiers correspond to
\textbf{dependent pair types} (also called Sigma types, written $\Sigma$).
These are constructs from dependent type theory, which provides a way to interpret
predicates or, more generally, types depending on some data or variable.
% This generalizes concepts such as predicates and functions.
This time we are not going to involve deduction rules or type judgments.
Instead, we will extend the isomorphism
to quantifiers directly
by presenting the Lean syntax.
\begin{example}[Quantifiers in Lean]
  Lean expresses quantifiers as follows:
  \begin{lstlisting}[language=lean]
  ∀ (x : X), P x
  Forall (x : X), P x
  -- Equivalently, using Pi types
  Π (x : X), P x
  \end{lstlisting}
  \begin{lstlisting}[language=lean]
  ∃ x : α, p
  Exists (λ x : α => p)
  -- Equivalently, using Sigma types
  Σ x : α, p
  \end{lstlisting}
\end{example}
\begin{example}[Universal introduction in Lean]
  The \textbf{universal introduction rule} allows us to prove $\forall x, P(x)$
  by proving $P(x)$ for an \textbf{arbitrary} $x$.
  In Lean, this corresponds to lambda abstraction (constructing a function):
  \newpage
  \begin{lstlisting}[language=lean]
  example : ∀ n : Nat, n ≥ 0 :=
    fun n => Nat.zero_le n
  \end{lstlisting}
\end{example}
\begin{example}[Universal elimination in Lean]
  The \textbf{universal elimination rule} allows us to instantiate
  a universally quantified statement with a specific value.
  In Lean, this is simply function application:
  \begin{lstlisting}[language=lean]
  example (h : ∀ n : Nat, n ≥ 0) : 5 ≥ 0 :=
    h 5
  \end{lstlisting}
\end{example}
\begin{example}[Existential introduction in Lean]
  When introducing an \textbf{existential} proof,
  we need a \textbf{pair} consisting
  of a witness and a proof that this witness
  satisfies the statement.
  \begin{lstlisting}[language=lean]
  example (x : Nat) (h : x > 0) : ∃ y, y < x :=
    ⟨0, h⟩
  \end{lstlisting}
  Notice that \lstinline[language=lean]|⟨0, h⟩| is a product type holding
  data (the witness~0) and a proof that it satisfies the property.
\end{example}
\begin{example}[Existential elimination in Lean]
  The \textbf{existential elimination rule}
  (\lstinline[language=lean]|Exists.elim|) allows us to prove a proposition $Q$
  from $\exists x, P(x)$ by showing that $Q$ follows from $P(w)$
  for an \textbf{arbitrary} value $w$.
  The existential quantifier can be interpreted as an infinite disjunction,
  so existential elimination naturally corresponds to a \textbf{proof by cases}
  (with a single case).
  In Lean, this is done using \textbf{pattern matching}
  with \lstinline[language=lean]|cases|:
  \newpage
  \begin{lstlisting}[language=lean]
  example (h : ∃ n : Nat, n > 0) : ∃ n : Nat, n > 0 := by
    cases h with
    | intro witness proof => ⟨witness, proof⟩
  \end{lstlisting}
\end{example}

\section{Constructive Mathematics}

Mathematicians have traditionally worked within \textbf{classical logic},
using \textbf{sets} as the primary means of structuring mathematical objects.
In contrast, \textbf{type theory} does not take sets as its primitive notion,
nor is it built by first applying logic and then adding structure.
Instead, logic is internal to type theory and is based on \textbf{constructive}
(or \textbf{intuitionistic}) logic, introduced by Brouwer and formalized by
Heyting (see, e.g., \cite{girard1989proofs}).

A major point of departure from classical logic is that, in constructive logic,
statements cannot simply be classified as true or false;
their truth depends on whether a proof exists.
There are many conjectures, such as the Riemann Hypothesis,
for which we do not yet know whether a proof or disproof exists,
so we cannot say whether they are true or false.
Consequently, constructive logic does not universally accept principles such
as the \textbf{axiom of choice} or the \textbf{law of excluded middle}
(every proposition is either true or false) as axioms.
As a consequence, proof by contradiction does not work in this setting
without additional justification.

Constructive logic emphasizes that a statement is only
considered true if we can explicitly construct
a proof or provide a \textbf{witness} for it.
This is what makes constructive mathematics inherently \textbf{computable}.

We already touched on this concept in the previous section.
In particular, we presented the logical connectives via the
Brouwer--Heyting--Kolmogorov (BHK) interpretation.
Following this interpretation, negation is not a primitive type
but is instead constructed as a function $\text{Prop} \to \text{False}$.
We also emphasized that, constructively,
a proof of existence consists of a pair:
a witness together with a proof that the stated property holds for that witness.
\begin{example}[Constructive existence proof]
  We give a constructive proof in Lean that there exist natural numbers
  $a$ and $b$ such that $a + b = 7$:
  \begin{lstlisting}[language=lean]
  example : ∃ a b : Nat, a + b = 7 := by
    use 3, 4
  \end{lstlisting}
  The \lstinline[language=lean]|use| tactic (from Mathlib) provides
  explicit witnesses: $a = 3$ and $b = 4$.
  Lean then automatically evaluates the expression and verifies that
  $3 + 4 = 7$.
  This example is simple enough for Lean to complete the proof automatically.
\end{example}

In classical mathematics, one might attempt a proof by contradiction.
However, this approach is not directly accepted in constructive mathematics,
as it doesn't provide explicit witnesses for the claimed objects.
Nonetheless, while constructive at its core, Lean allows users to
invoke classical principles, such as contraposition or proof by contradiction,
through tactics like \lstinline[language=lean]|exfalso| or by importing
\lstinline[language=lean]|Classical|.

\begin{example}[Reasoning from false]
  Here is an example of deriving any proposition from a contradiction:
  \begin{lstlisting}[language=lean]
  example (p : Prop) (h : False) : p := by
    exfalso
    exact h
  \end{lstlisting}
  This example takes a proposition $p$ to prove and a false hypothesis $h$.
  The \lstinline[language=lean]|exfalso| tactic transforms the goal into
  $\vdash \mathsf{False}$, meaning we now need to derive a contradiction.
  Since we already have a false hypothesis $h$,
  we can provide it using the \lstinline[language=lean]|exact| tactic.
  This principle is known as \textit{ex falso quodlibet}
  (from falsehood, anything follows).
\end{example}

% \paragraph{Key Principles of Constructive Mathematics}
% Some fundamental principles that distinguish constructive from classical mathematics include:
% \begin{itemize}
%   \item The law of excluded middle $P \lor \neg P$ is not universally accepted.
%   \item $\forall (x: \mathbb{N}). (\text{isPrime}(x) \lor \neg \text{isPrime}(x))$
%         is constructively valid because primality is a decidable property.
%   \item $\forall (x: \mathbb{N}). (\text{halts}(x) \lor \neg \text{halts}(x))$
%         is not decidable (where $\text{halts}$ determines if a program terminates).
%   \item Double negation elimination $\neg \neg P \to P$ is not constructively valid in general,
%         though $P \to \neg \neg P$ always holds.
% \end{itemize}