\chapter{Logic and Proposition as Types}

\section{First Order Logic}
Logic is the study of reasoning, branching into various systems.
We refer to \textbf{classical logic} as the one that underpins much
of traditional mathematics.
It's the logic of truthtables.
We first introduce \textbf{propositional logic}, which is the simplest
form of classical logic.
Later we will extend this to \textbf{predicate (or first-order) logic}, which includes
\textbf{predicates} and \textbf{quantifiers}.
In this setting, a \textbf{proposition} is a statement that is either true or false,
and a \textbf{proof} is a logical argument that establishes the truth of a
proposition.
Propositions can be combined with logical \textbf{connectives} such as ``and'' ($\wedge$),
``or'' ($\vee$), ``not'' ($\neg$),``false'' ($\bot$), ,``true'' ($\top$)
``implies'' ($\Rightarrow$),  and ``if and only if'' ($\Leftrightarrow$).
These connectives allow the creation of complex or compound propositions.
Here how connectives are defined in Lean:
\begin{example}[LogicaL connectives in Lean]\mbox{}
  \begin{lstlisting}[language=lean]
    #check And (a b : Prop) : Prop
    #check Or (a b : Prop) : Prop
  \end{lstlisting}
  \lstinline[language=lean]|Prop| is the proposition type mentioned before.
\end{example}
Logic is often formalized through a framework known as the \textbf{natural deduction system},
developed by Gentzen in the 1930s (\cite{wadler2015propositions}).
This approach brings logic closer to a computable, algorithmic system.
It specifies rules for deriving
\textbf{conclusions} from \textbf{premises} (assumptions from other propositions),
called \textbf{inference rules}.
\begin{example}[Deductive style rule]
  Here is an hypothetical example of inference rule (\cite{nordstrom1990programming}, page 35).
  \begin{prooftree}
    \AxiomC{$P_1$}
    \AxiomC{$P_2$}
    \AxiomC{$\cdots$}
    \AxiomC{$P_n$}
    \QuaternaryInfC{$C$}
  \end{prooftree}
  Where the $P_1, P_2, \ldots, P_n$, above the line, are hypothetical
  premises and, the hypothetical conclusion $C$ is below the line.
\end{example}
The inference rules needed are, \textbf{introduction rules} wich specify
how to form compound propositions from simpler ones, and
\textbf{elimination rules} needed to derive information about them.
Let's look at how we can define some connectives, first using
natural deduction (from \cite{thompson1999types}, Section 1.1).
\paragraph{Conjunction ($\land$)}
\paragraph{Introduction Rule}
\begin{prooftree}
  \AxiomC{$A$}
  \AxiomC{$B$}
  \RightLabel{$\land$-Intro}
  \BinaryInfC{$A \land B$}
\end{prooftree}
\paragraph{Elimination Rule}
\mbox{}\\[0.5em]
\noindent
\begin{minipage}[t]{0.48\textwidth}
  \vspace{0pt}
  \begin{prooftree}
    \AxiomC{$A \land B$}
    \RightLabel{$\land$-Elim$_1$}
    \UnaryInfC{$A$}
  \end{prooftree}
\end{minipage}\hfill
\begin{minipage}[t]{0.48\textwidth}
  \vspace{0pt}
  \begin{prooftree}
    \AxiomC{$A \land B$}
    \RightLabel{$\land$-Elim$_2$}
    \UnaryInfC{$B$}
  \end{prooftree}
\end{minipage}
\paragraph{Disjunction ($\lor$)}
\paragraph{Introduction Rule}
\mbox{}\\[0.5em]
\noindent
\begin{minipage}[t]{0.48\textwidth}
  \vspace{0pt}
  \begin{prooftree}
    \AxiomC{$A$}
    \RightLabel{$\lor$-Intro$_1$}
    \UnaryInfC{$A \lor B$}
  \end{prooftree}
\end{minipage}\hfill
\begin{minipage}[t]{0.48\textwidth}
  \vspace{0pt}
  \begin{prooftree}
    \AxiomC{$B$}
    \RightLabel{$\lor$-Intro$_2$}
    \UnaryInfC{$A \lor B$}
  \end{prooftree}
\end{minipage}
\paragraph{ Elimination (Proof by cases)}
\begin{prooftree}
  \AxiomC{$A \lor B$}
  \AxiomC{$[A] \vdash C$}
  \AxiomC{$[B] \vdash C$}
  \RightLabel{$\lor$-Elim}
  \TrinaryInfC{$C$}
\end{prooftree}
\paragraph{Implication ($\to$)}
\paragraph{Introduction Rule}
\begin{prooftree}
  \AxiomC{$[A] \vdash B$}
  \RightLabel{$\to$-Intro}
  \UnaryInfC{$A \to B$}
\end{prooftree}
\paragraph{ Elimination (Modus Ponens)}
\begin{prooftree}
  \AxiomC{$A \to B$}
  \AxiomC{$A$}
  \RightLabel{$\to$-Elim}
  \BinaryInfC{$B$}
\end{prooftree}
\begin{notation}
  We use $A \vdash B$ (called turnstile) to designate a
  deduction of $B$ from $A$.
  It is used in judgments and type theory with
  the meaning of ``entails that''.
  The square brackets around a premise $[A]$ mean that the premise $A$ is meant to
  be \textbf{discharged} at the conclusion. The classical example is the
  introduction rule for the implication connective.
  To prove an implication $A \to B$, we assume $A$
  (shown as $[A]$), derive $B$ under this assumption, and then discharge the
  assumption $A$ to conclude that $A \to B$ holds without the assumption.
\end{notation}
\section{Primitive Types}
Type theory employs this porocedure too,
by referring to deduction
rules as \textbf{judments}.
A type judgment has the form $\Gamma \vdash t : T$,
meaning: under \textbf{context} $\Gamma$ (a list of typed variables),
the term $t$ has type $T$.
Using formal inference rules in the type judgment
system, such as \textbf{introduction} and \textbf{elimination} rules,
we can construct new compound types from existing ones.
\begin{example}[Judgment style rule]
  \mbox{}
  \begin{prooftree}
    \AxiomC{$\Gamma \vdash$}
    \AxiomC{$p_1:P_1$}
    \AxiomC{$p_2:P_2$}
    \AxiomC{$\cdots$}
    \AxiomC{$P_n$}
    \QuinaryInfC{$C$}
  \end{prooftree}
\end{example}
Technically, there are two more inference rules that we will not consider in this setting:
\textbf{formation rules}, used to declare that a type is well-defined, and
\textbf{computation rules}, which specify how a term will be evaluated.
Moreover, without going too deep into the jargon,
one specific judgment is
$\Gamma \vdash A \equiv B\ \text{type}$, which means ``types $A$ and $B$ are
\textbf{judgmentally (or definitionally) equal} in context $\Gamma$.''
Similarly for terms, $\Gamma \vdash t_1 \equiv t_2 : A$ means ``terms $t_1$ and $t_2$ are
judgmentally equal of type $A$ in context $\Gamma$.''
\paragraph{Brief explanation of equality in type theory}

In Lean, the operator \lstinline[language=lean]|:=|
stands for definitional equality and is used by the kernel to verify proof equality.
A mathematician proving a theorem applies a series of \textbf{reduction rules} to
simplify the proof.
Similarly, one can think of this
computational reduction process in formal verification.
However, a computer cannot simply employ the same informal
approach to equality that a mathematician might use intuitively.
A rigorous explanation of definitional equality
goes beyond the scope of this thesis.
To state it simply: \textbf{two terms are definitionally equal when they
  reduce to the same normal for}. A \textbf{normal for} represents the most
reduced state of a term, obtained by systematically applying a
sequence of reduction rules until no further reductions are possible.
In contrast, \textbf{propositional equality} requires additional logical
bridges and propositions to establish equivalence.
Because it is grounded in logical propositions rather
than pure computation, propositional equality is not
directly computable by the type checker and must be explicitly proved.

Let's now construct new types from given types $A$ and $B$.
\paragraph{Product Type}
As a fundamental example, $A \times B$
denotes the type of pairs $(a, b)$ where $a : A$ and $b : B$,
called the \textbf{product type}.
\paragraph{Introduction Rule (pairing)}
\begin{prooftree}
  \AxiomC{$a : A$}
  \AxiomC{$b : B$}
  \BinaryInfC{$(a, b) : A \times B$}
\end{prooftree}
In Lean:
\begin{lstlisting}[language=lean]
Prod.mk a b : Prod A B   -- or A × B
(a, b) : A × B           
⟨a, b⟩ : A × B           
\end{lstlisting}
\paragraph{Elimination Rules (projections)}\mbox{}\\[0.5em]
\noindent
\begin{minipage}[t]{0.48\textwidth}
  \vspace{0pt}
  \begin{prooftree}
    \AxiomC{$p : A \times B$}
    \UnaryInfC{$\mathsf{fst}(p) : A$}
  \end{prooftree}
\end{minipage}\hfill
\begin{minipage}[t]{0.48\textwidth}
  \vspace{0pt}
  \begin{prooftree}
    \AxiomC{$p : A \times B$}
    \UnaryInfC{$\mathsf{snd}(p) : B$}
  \end{prooftree}
\end{minipage}

\noindent In Lean:
\begin{lstlisting}[language=lean]
  p.1 : A       -- or Prod.fst p
  p.2 : B       -- or Prod.snd p
\end{lstlisting}
\paragraph{Sum Type}
The \textbf{sum type} $A + B$ (also called a coproduct or disjoint union) consists of values that are
either of type $A$ (tagged with $\mathsf{inl}$) or
of type $B$ (tagged with $\mathsf{inr}$).
\paragraph{Introduction Rules (injections)}
\mbox{}\\[0.5em]
\noindent
\begin{minipage}[t]{0.48\textwidth}
  \vspace{0pt}
  \begin{prooftree}
    \AxiomC{$a : A$}
    \UnaryInfC{$\mathsf{inl}(a) : A + B$}
  \end{prooftree}
\end{minipage}\hfill
\begin{minipage}[t]{0.48\textwidth}
  \vspace{0pt}
  \begin{prooftree}
    \AxiomC{$b : B$}
    \UnaryInfC{$\mathsf{inr}(b) : A + B$}
  \end{prooftree}
\end{minipage}

\noindent In Lean:
\begin{lstlisting}[language=lean]
Sum.inl a : Sum A B   -- or A ⊕ B
Sum.inr b : Sum A B
\end{lstlisting}
\paragraph{Elimination Rule (case analysis)}
\begin{prooftree}
  \AxiomC{$p : A + B$}
  \AxiomC{$\begin{array}{c}  f : (A \implies C) \end{array}$}
  \AxiomC{$\begin{array}{c}  g : (B \implies C) \end{array}$}
  \TrinaryInfC{$\mathsf{cases}(p, f, g) : C$}
\end{prooftree}
\newpage
In Lean:
\begin{lstlisting}[language=lean]
example (p : Sum A B) (f : A → C) (g : B → C) : C := by
  cases p with
  | inl x => f x
  | inr y => g y
\end{lstlisting}
\paragraph{Function Types}
The type of the form $A \to B$, used in the sum elimination rule
represents functions from $A$ to $B$.
\paragraph{Introduction Rule (function application or lambda abstraction)}
\begin{prooftree}
  \AxiomC{$\begin{array}{c} x : A  \vdash  \Phi : B \end{array}$}
  % \UnaryInfC{$\lambda x.\Phi : A \to B$}
  \UnaryInfC{$f  : A \to B$}
\end{prooftree}
Where $f$ is a function that maps any element $x : A$ to an element $\Phi : B$.
In Lean, lambda abstraction is written using \lstinline[language=lean]|fun| or \lstinline[language=lean]|λ|:
\begin{lstlisting}[language=lean]
def identityFun (A : Type) : A → A := fun x => x
\end{lstlisting}
\paragraph{Elimination Rule (application)}
\begin{prooftree}
  \AxiomC{$f : A \to B$}
  \AxiomC{$a : A$}
  \BinaryInfC{$f(a) : B$}
\end{prooftree}
In Lean, function application is written using juxtaposition:
\begin{lstlisting}[language=lean]
example (f : A → B) (a : A) : B := f a
\end{lstlisting}
Functions are a primitive concept in type theory. We can \textbf{apply} a function
$f : A \to B$ to an element $a : A$ to obtain an element of $B$, denoted $f(a)$.
In type theory, it is common to omit the parentheses and write the application
simply as $f \, a$.

\section{The Curry Howard Isomorphism}
We have been preparing for this argument, and the reader will have surely
noticed a strong similarity when defining logical connectives
using deduction rules; they are remarkably similar to types
constructed using type judgments. For instance, function
types can be seen as implications.
This is not a coincidence, but rather a fundamental theorem
first proven by Haskell Curry and William Howard.
It forms the core of type theory and establishes
a deep connection between logic, computation, and mathematics.
\textbf{Implication} ($P \Rightarrow Q$) corresponds to the \textbf{function type} ($P \to Q$).
A proof of an implication is a function that transforms any proof
of the premise into a proof of the conclusion.
\noindent\textbf{Conjunction} ($P \land Q$) corresponds
to the \textbf{product type} ($P \times Q$).
A proof of a conjunction consists of a pair containing proofs of both conjuncts.
\noindent\textbf{Disjunction} ($P \lor Q$) corresponds
to the \textbf{sum type} ($P + Q$).
A proof of a disjunction is either a proof of the
first disjunct or a proof of the second disjunct.
Same goes for the rest of the connectives.
Lean uses inference rules and type
judgments as well as computing connectives using each related type.
For instance, $A \land B$ can be represented as \lstinline[language=lean]|And(A, B)| or \lstinline[language=lean]|A ∧ B|.
Its introduction rule is constructed by
\lstinline[language=lean]|And.intro _ _| or simply
\lstinline[language=lean]|⟨_, _⟩| (underscores are placeholders).
The pair $A \land B$ can then be consumed using elimination
rules \lstinline[language=lean]|And.left| and \lstinline[language=lean]|And.right|.

\begin{example}\label{ex:conj_intro_2}
  Let's look at a simple Lean example:
  \begin{lstlisting}[language=lean]
    example {a b : Prop} (ha : a) (hb : b) : (a ∧ b) := And.intro ha hb
  \end{lstlisting}
  Using brackets \lstinline[language=lean]|{ }|, we let Lean infer
  that $a$ and $b$ are propositions (\lstinline[language=lean]|Prop|).
  The example means that given a proof of $a$ (\lstinline[language=lean]|ha|)
  and a proof of $b$  (\lstinline[language=lean]|hb|) ,
  we can form a proof of $(a \land b)$.
  \lstinline[language=lean]|And.intro| is implemented as:
  \begin{lstlisting}[language=lean]
    And.intro : p -> q -> (p ∧ q)
  \end{lstlisting}
  It says: if you give me a proof of $p$ and a proof of $q$,
  then I return a proof of $p \land q$.
  We therefore conclude the proof by directly giving
  \lstinline[language=lean]|And.intro ha hb|.
  Here is another way of writing the same statement:
  \begin{lstlisting}[language=lean]
    example (ha : a) (hb : b) : And(a, b) := ⟨ha, hb⟩
  \end{lstlisting}
\end{example}
\noindent
For a more concrete example, let's look at how
proof normalization using a system of inference rules
corresponds to computation in Lean.
To reduce complexity of a \textbf{proof tree} in natural deduction,
one, tipically, follows a
\textbf{top-down} approach,
unfolding each component to be proved step by step.
\begin{example}[Associativity of Conjunction]
  We prove that $(A \land B) \land C$ implies $A \land (B \land C)$.
  First, from the assumption $(A \land B) \land C$, we can derive $A$:
  \begin{prooftree}
    \AxiomC{$(A \land B) \land C$}
    \RightLabel{$\land E_1$}
    \UnaryInfC{$A \land B$}
    \RightLabel{$\land E_1$}
    \UnaryInfC{$A$}
  \end{prooftree}
  Second, we can derive $B \land C$:
  \begin{prooftree}
    \AxiomC{$(A \land B) \land C$}
    \RightLabel{$\land E_1$}
    \UnaryInfC{$A \land B$}
    \RightLabel{$\land E_2$}
    \UnaryInfC{$B$}
    \AxiomC{$(A \land B) \land C$}
    \RightLabel{$\land E_2$}
    \UnaryInfC{$C$}
    \RightLabel{$\land I$}
    \BinaryInfC{$B \land C$}
  \end{prooftree}
  Finally, combining these derivations we obtain $A \land (B \land C)$:
  \begin{prooftree}
    \AxiomC{$(A \land B) \land C \vdash A$}
    \AxiomC{$(A \land B) \land C \vdash B \land C$}
    \RightLabel{$\land I$}
    \BinaryInfC{$A \land (B \land C)$}
  \end{prooftree}
\end{example}
\newpage
\begin{example}[Lean Implementation]
  Let us now implement the same proof in Lean.
  \begin{lstlisting}[language=lean]
theorem and_associative {a b c : Prop} : (a ∧ b) ∧ c → a ∧ (b ∧ c) :=
  fun h : (a ∧ b) ∧ c →
  -- First, from the assumption (a ∧ b) ∧ c, we can derive a:
  have hab : a ∧ b := h.left
  have ha : a := hab.left 
  -- Second, we can derive b ∧ c (here we only extract b and c and combine them in the next step)
  have hc : c := h.right
  have hb : b := hab.right
  -- Finally, combining these derivations we obtain a ∧ (b ∧ c)
  show a ∧ (b ∧ c) from ⟨ha, ⟨hb, hc⟩⟩
\end{lstlisting}
  We introduce the \lstinline[language=lean]|theorem| with the name
  \lstinline[language=lean]|and_associative|.
  The type signature \lstinline[language=lean]|(a ∧ b) ∧ c → a ∧ (b ∧ c)|
  represents our logical implication.
  Here, we construct the implication proof using a
  function (following the Curry Howard isomorphism) with the \lstinline[language=lean]|fun| keyword.
  The \lstinline[language=lean]|have| keyword introduces local
  lemmas within our proof scope, allowing us to break down complex
  reasoning into manageable intermediate steps,
  mirroring our natural deduction proof from before.
  Just before the keyword \lstinline[language=lean]|show|,
  the info view displays the following
  context and goal:
  \begin{lstlisting}[language=lean]
  a b c : Prop
  h : (a ∧ b) ∧ c
  hab : a ∧ b
  ha : a
  hc : c
  hb : b
  ⊢ a ∧ b ∧ c
\end{lstlisting}
  Resembling type judgments, the goal is juxtaposed after the turnstile ($\vdash$).
  What comes before it is the current context.
  Finally, \lstinline[language=lean]|show a ∧ (b ∧ c) from ⟨ha, ⟨hb, hc⟩⟩|
  asserts that we are constructing a proof of \lstinline[language=lean]|a ∧ (b ∧ c)|
  using the term \lstinline[language=lean]|⟨ha, ⟨hb, hc⟩⟩|.
  The \lstinline[language=lean]|show| keyword makes the proof
  more readable
  and ensures that the provided
  proof term matches the stated
  goal up to definitional equality.
  As mentioned already, two types (or terms) are definitionally equal in Lean when they
  are identical after computation
  and unfolding of definitions; in other words, when Lean's type checker
  can mechanically verify they are the same without requiring additional proof steps.
  Here, the goal \lstinline[language=lean]|⊢ a ∧ b ∧ c| is definitionally
  equal to \lstinline[language=lean]|a ∧ (b ∧ c)| due to how conjunction
  associates, so \lstinline[language=lean]|show| accepts this statement.
  If we had tried to use \lstinline[language=lean]|show| with a type that
  was only propositionally equal
  but not definitionally equal, Lean would reject it.
\end{example}
\section{Predicate Logic and Dependency}
To capture more complex mathematical ideas, we extend our system from
propositional logic to \textbf{predicate logic}.
A \textbf{predicate} is a statement or proposition that depends on a variable.
In propositional logic we represent a proposition simply by $P$.
In predicate logic, this is generalized.
A predicate is written as $P(a)$,
where $a$ is a variable. Notice that a predicate is just a function.
This extension allows us to introduce \textbf{quantifiers}:
$\forall$ (``for all'') and $\exists$ (``there exists'').
These quantifiers express that a given formula holds either for every object
or for at least one object, respectively.
In Lean if \lstinline[language=lean]|α| is any type, we can represent a
predicate \lstinline[language=lean]|P| on \lstinline[language=lean]|α| as
an object of type \lstinline[language=lean]|α → Prop|.
Thus given an \lstinline[language=lean]|x : α| (an element
with type \lstinline[language=lean]|α| )
\lstinline[language=lean]|P(x) : Prop| would be representative of a proposition
holding for \lstinline[language=lean]|x|.
We can give an informal reading of the quantifiers as infinite logical operations:
\begin{align*}
  \forall x.\,P(x) & \equiv P(a) \land P(b) \land P(c) \land \ldots \\
  \exists x.\,P(x) & \equiv P(a) \lor P(b) \lor P(c) \lor \ldots
\end{align*}
The dot symbol following the quantifier, as in $\forall x.,$, binds
every occurrence of the variable $x$ in the expression $P(x)$.
The expression $\forall x.\, P(x)$ can be understood as a generalized form of conjunction.
It expresses that $P$ holds for all possible values of $x$.
Similarly, $\exists x.\, P(x)$ is a generalized disjunction, expressing that $P$ holds
for at least one value of $x$.
Under the Curry-Howard isomorphism, universal quantifiers correspond to
\textbf{dependent function types} (also called Pi types, written $\Pi$),
while existential quantifiers correspond to
\textbf{dependent pair types} (also called Sigma types, written $\Sigma$).
These are constructs from dependent type theory, which provides a way to interpret
predicates or, more generally, types depending on some data or variable.
Technically the correspondence is not that immediate and actually Lean implements,
\lstinline[language=lean]|Exists| and \lstinline[language=lean]|Forall|
using as inductive types (this follows also for the previously defined connectives).
This time we are not going to involve deduction rules or type judgments.
Instead, we will extend the isomorphism
to quantifiers directly
by presenting the Lean syntax.
\begin{example}[Quantifiers in Lean]
  Lean expresses quantifiers as follows:
  \begin{lstlisting}[language=lean]
variable (X : Type) (P : X → Prop)
 (∀ (x : X), P x) -- ∀ corresponds to Pi type Π
 (∃ (x : X), P x) -- ∃ corresponds to Sigma type Σ
  \end{lstlisting}
\end{example}
\newpage
\begin{example}[Universal introduction in Lean]
  The \textbf{universal introduction rule} allows us to prove $\forall x, P(x)$
  by proving $P(x)$ for an \textbf{arbitrary} $x$.
  In Lean, this corresponds to constructing a function:
  \begin{lstlisting}[language=lean]
  example : ∀ n : Nat, n ≥ 0 :=
    fun n => Nat.zero_le n 
  \end{lstlisting}
  From the \lstinline[language=lean]|Nat| module in Lean, we use
  \lstinline[language=lean]|zero_le|, a built-in theorem that already
  proves the statement.
\end{example}
\begin{example}[Universal elimination in Lean]
  The \textbf{universal elimination rule} allows us to instantiate
  a universally quantified statement with a specific value.
  In Lean, this is simply function application:
  \begin{lstlisting}[language=lean]
  example (h : ∀ n : Nat, n ≥ 0) : 5 ≥ 0 :=
    h 5
  \end{lstlisting}
\end{example}
\begin{example}[Existential introduction in Lean]
  When introducing an \textbf{existential} proof,
  we need a \textbf{pair} consisting
  of a witness and a proof that this witness
  satisfies the statement.
  \begin{lstlisting}[language=lean]
  example (x : Nat) (h : x > 0) : ∃ y, y < x :=
    ⟨0, h⟩
  \end{lstlisting}
  Notice that \lstinline[language=lean]|⟨0, h⟩| is a product type holding
  data (the witness) and a proof that it satisfies the property.
\end{example}
\begin{example}[Existential elimination in Lean]
  The \textbf{existential elimination rule}
  (\lstinline[language=lean]|Exists.elim|) allows us to prove a proposition $Q$
  from $\exists x, P(x)$ by showing that $Q$ follows from $P(w)$
  for an \textbf{arbitrary} value $w$.
  The existential quantifier can be interpreted as an infinite disjunction,
  so existential elimination naturally corresponds to a \textbf{proof by cases}
  (with a single case).
  In Lean, this is done using \textbf{pattern matching}
  with \lstinline[language=lean]|cases|:
  \begin{lstlisting}[language=lean]
  example (h : ∃ n : Nat, n > 0) : ∃ n : Nat, n > 0 := by
    cases h with
    | intro witness proof => ⟨witness, proof⟩
  \end{lstlisting}
\end{example}

\section{Constructive Mathematics}

Mathematicians have traditionally worked within \textbf{classical logic},
using \textbf{sets} as the primary means of structuring mathematical objects.
In contrast, \textbf{type theory} does not take sets as its primitive notion,
nor is it built by first applying logic and then adding structure.
Instead, logic is internal to type theory and is based on \textbf{constructive}
(or \textbf{intuitionistic}) logic, introduced by Brouwer and formalized by
Heyting (see, e.g., \cite{girard1989proofs}, Ch 1, page 6).
A major point of departure from classical logic is that, in constructive logic,
statements cannot simply be classified as true or false;
their truth depends on whether a proof exists.
There are many conjectures, such as the Riemann Hypothesis,
for which we do not yet know whether a proof or disproof exists,
so we cannot say whether they are true or false.
Consequently, constructive logic does not universally accept principles such
as the \textbf{axiom of choice} or the \textbf{law of excluded middle}
(every proposition is either true or false) as axioms.
As a consequence, proof by contradiction does not work in this setting
without additional justification.
Constructive logic emphasizes that a statement is only
considered true if we can explicitly provide a \textbf{witness} for it.
This is what makes constructive mathematics inherently \textbf{computable}.
% We already touched on this concept in the previous section.
% In particular, we presented the logical connectives via the
% Brouwer--Heyting--Kolmogorov (BHK) interpretation.
We also emphasized that, constructively,
a proof of existence consists of a pair:
a witness together with a proof that the stated property holds for that witness.
\begin{example}[Constructive existence proof]
  We give a \textbf{constructive proof} in Lean that there exist natural numbers
  $a$ and $b$ such that $a + b = 7$:
  \begin{lstlisting}[language=lean]
example : ∃ a b : Nat, a + b = 7 := ⟨3, 4, rfl⟩
\end{lstlisting}
  To prove an existential statement, we provide \textbf{witnesses}
  (concrete values $a = 3$ and $b = 4$) and a \textbf{proof}
  that the predicate holds ($3 + 4 = 7$).
\end{example}
In classical mathematics, one might attempt a proof by contradiction.
However, this approach is not directly accepted in constructive mathematics,
as it doesn't provide explicit witnesses for the claimed objects.
Nonetheless, while constructive at its core, Lean allows users to
invoke classical principles, such as contraposition or proof by contradiction,
through \textbf{tactics} ((to be explained later))
like \lstinline[language=lean]|exfalso|.
\begin{example}[Reasoning from false]
  Here is an example of deriving any proposition from a contradiction:
  \begin{lstlisting}[language=lean]
  example (p : Prop) (h : False) : p := by
    exfalso
    exact h
  \end{lstlisting}
  This example takes a proposition $p$ to prove and a false hypothesis $h$.
  The \lstinline[language=lean]|exfalso| tactic transforms the goal into
  $\vdash \mathsf{False}$, meaning we now need to derive a contradiction.
  Since we already have a false hypothesis $h$,
  we can provide it using the \lstinline[language=lean]|exact| tactic.
\end{example}