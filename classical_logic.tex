\section{Logic and Proposition as Types}
\subsection{First Order Logic}
Logic is the study of reasoning, branching into various systems.
We refer to \textbf{classical logic} as the one that underpins much
of traditional mathematics.
It's the logic of the ancient Greeks (not fair) and truthtables, and it remains
used nowadays for pedagogical reasons.
We first introduce \textbf{propositional logic}, which is the simplest
form of classical logic.
Later we will extend this to \textbf{predicate (or first-order) logic}, which includes
\textbf{predicates} and \textbf{quantifiers}.
In this setting, a \textbf{proposition} is a statement that is either true or false,
and a \textbf{proof} is a logical argument that establishes the truth of a
proposition.
Propositions can be combined with logical \textbf{connectives} such as ``and'' ($\wedge$),
``or'' ($\vee$), ``not'' ($\neg$),``false'' ($\bot$), ,``true'' ($\top$) ``implies'' ($\Rightarrow$),  and ``if and only if'' ($\Leftrightarrow$).
These connectives allow the creation of complex or compound propositions.
\newpage
Here how connectives are defined in Lean:
\begin{example}[LogicaL connectives in Lean]\mbox{}
  \begin{lstlisting}[language=lean]
    #check And (a b : Prop) : Prop
    #check Or (a b : Prop) : Prop
    #check True : Prop
    #check False : Prop
    #check Not (a : Prop) : Prop
    #check Iff (a b : Prop) : Prop
  \end{lstlisting}
  \lstinline[language=lean]|Prop| stands for proposition, and it is an
  essential component of Lean’s type system.
  For now, we can think of it as a special type whose
  inhabitants are proofs; somewhat
  paradoxically, a type of types.
\end{example}
Logic is often formalized through a framework known as the \textbf{natural deduction system},
developed by Gentzen in the 1930s (\cite{wadler2015propositions}).
This approach brings logic closer to a computable, algorithmic system.
It specifies rules for deriving
\textbf{conclusions} from \textbf{premises} (assumptions from other propositions),
called \textbf{inference rules}.
\begin{example}[Deductive style rule]
  Here is an hypothetical example of inference rule.
  \begin{prooftree}
    \AxiomC{$P_1$}
    \AxiomC{$P_2$}
    \AxiomC{$\cdots$}
    \AxiomC{$P_n$}
    \QuaternaryInfC{$C$}
  \end{prooftree}
  Where the $P_1, P_2, \ldots, P_n$, above the line, are hypothetical premises and, the hypothetical conclusion $C$ is below the line.
\end{example}
The inference rules needed are:
\begin{itemize}
  \item \textbf{Introduction rules} specify how to form compound propositions from simpler ones, and
  \item \textbf{Elimination rules} specify how to use compound propositions to derive information about their components.
\end{itemize}
Let's look at how we can define some connectives first using natural deduction.
\paragraph{Conjunction ($\land$)}
\paragraph{Introduction Rule}
\begin{prooftree}
  \AxiomC{$A$}
  \AxiomC{$B$}
  \RightLabel{$\land$-Intro}
  \BinaryInfC{$A \land B$}
\end{prooftree}
\paragraph{Elimination Rule}
\mbox{}\\[0.5em]
\noindent
\begin{minipage}[t]{0.48\textwidth}
  \vspace{0pt}
  \begin{prooftree}
    \AxiomC{$A \land B$}
    \RightLabel{$\land$-Elim$_1$}
    \UnaryInfC{$A$}
  \end{prooftree}
\end{minipage}\hfill
\begin{minipage}[t]{0.48\textwidth}
  \vspace{0pt}
  \begin{prooftree}
    \AxiomC{$A \land B$}
    \RightLabel{$\land$-Elim$_2$}
    \UnaryInfC{$B$}
  \end{prooftree}
\end{minipage}
\paragraph{Disjunction ($\lor$)}
\paragraph{Introduction Rule}
\mbox{}\\[0.5em]
\noindent
\begin{minipage}[t]{0.48\textwidth}
  \vspace{0pt}
  \begin{prooftree}
    \AxiomC{$A$}
    \RightLabel{$\lor$-Intro$_1$}
    \UnaryInfC{$A \lor B$}
  \end{prooftree}
\end{minipage}\hfill
\begin{minipage}[t]{0.48\textwidth}
  \vspace{0pt}
  \begin{prooftree}
    \AxiomC{$B$}
    \RightLabel{$\lor$-Intro$_2$}
    \UnaryInfC{$A \lor B$}
  \end{prooftree}
\end{minipage}
\paragraph{ Elimination (Proof by cases)}
\begin{prooftree}
  \AxiomC{$A \lor B$}
  \AxiomC{$[A] \vdash C$}
  \AxiomC{$[B] \vdash C$}
  \RightLabel{$\lor$-Elim}
  \TrinaryInfC{$C$}
\end{prooftree}
\paragraph{Implication ($\to$)}
\paragraph{Introduction Rule}
\begin{prooftree}
  \AxiomC{$[A] \vdash B$}
  \RightLabel{$\to$-Intro}
  \UnaryInfC{$A \to B$}
\end{prooftree}
\paragraph{ Elimination (Modus Ponens)}
\begin{prooftree}
  \AxiomC{$A \to B$}
  \AxiomC{$A$}
  \RightLabel{$\to$-Elim}
  \BinaryInfC{$B$}
\end{prooftree}
\begin{notation}
  We use $A \vdash B$ (called turnstile) to designate a
  deduction of $B$ from $A$.
  It is employed in Gentzen’s \textbf{sequent calculus}
  (\cite{girard1989proofs})
  and moslty used in type theory.
  The square brackets around a premise $[A]$ mean that the premise $A$ is meant to
  be \textbf{discharged} at the conclusion. The classical example is the
  introduction rule for the implication connective.
  To prove an implication $A \to B$, we assume $A$
  (shown as $[A]$), derive $B$ under this assumption, and then discharge the
  assumption $A$ to conclude that $A \to B$ holds without the assumption.
  The turnstile is predominantly used in judgments and type theory with
  the meaning of ``entails that''.
\end{notation}
\subsection{Primitive Types}
Type theory employs this porocedure too,
by referring to deduction
rules as \textbf{judments}.
A type judgment has the form $\Gamma \vdash t : T$,
meaning: under \textbf{context} $\Gamma$ (a list of typed variables),
the term $t$ has type $T$.
Using formal inference rules in the type judgment
system, such as \textbf{introduction} and \textbf{elimination} rules
(this time used for introducing and cosuming terms of a type),
we can construct new compound types from existing ones.
\begin{example}[Judgment style rule]
  \mbox{}
  \begin{prooftree}
    \AxiomC{$\Gamma \vdash$}
    \AxiomC{$p_1:P_1$}
    \AxiomC{$p_2:P_2$}
    \AxiomC{$\cdots$}
    \AxiomC{$P_n$}
    \QuinaryInfC{$C$}
  \end{prooftree}
\end{example}
Technically, there are two more inference rules that we will not consider in this setting:
\textbf{formation rules}, used to declare that a type is well-formed, and
\textbf{computation rules}, which specify how a term will be evaluated.
Moreover, without going too deep into the jargon,
one specific judgment is
$\Gamma \vdash A \equiv B\ \text{type}$, which means ``types $A$ and $B$ are
\textbf{judgmentally (or definitionally) equal} in context $\Gamma$.''
Similarly for terms, $\Gamma \vdash t_1 \equiv t_2 : A$ means ``terms $t_1$ and $t_2$ are
judgmentally equal of type $A$ in context $\Gamma$.''
In Lean, the operator \lstinline[language=lean]|:=|
stands for definitional equality and is used by the kernel to verify proof equality.

Let's now construct new types from given types $A$ and $B$.
\paragraph{Product Type}
As a fundamental example, $A \times B$
denotes the type of pairs $(a, b)$ where $a : A$ and $b : B$,
called the \textbf{product type}.
\paragraph{Introduction Rule (pairing)}
\begin{prooftree}
  \AxiomC{$a : A$}
  \AxiomC{$b : B$}
  \BinaryInfC{$(a, b) : A \times B$}
\end{prooftree}

\noindent In Lean:
\begin{lstlisting}[language=lean]
Prod.mk a b : Prod A B   -- or A × B
(a, b) : A × B            -- tuple notation
⟨a, b⟩ : A × B            -- angle bracket notation
\end{lstlisting}
\paragraph{Elimination Rules (projections)}\mbox{}\\[0.5em]
\noindent
\begin{minipage}[t]{0.48\textwidth}
  \vspace{0pt}
  \begin{prooftree}
    \AxiomC{$p : A \times B$}
    \UnaryInfC{$\mathsf{fst}(p) : A$}
  \end{prooftree}
\end{minipage}\hfill
\begin{minipage}[t]{0.48\textwidth}
  \vspace{0pt}
  \begin{prooftree}
    \AxiomC{$p : A \times B$}
    \UnaryInfC{$\mathsf{snd}(p) : B$}
  \end{prooftree}
\end{minipage}

\noindent In Lean:
\begin{lstlisting}[language=lean]
  p.1 : A       -- or Prod.fst p
  p.2 : B       -- or Prod.snd p
\end{lstlisting}
\paragraph{Sum Type}
The \textbf{sum type} $A + B$ (also called a coproduct or disjoint union) consists of values that are
either of type $A$ (tagged with $\mathsf{inl}$) or
of type $B$ (tagged with $\mathsf{inr}$).
\paragraph{Introduction Rules (injections)}
\mbox{}\\[0.5em]
\noindent
\begin{minipage}[t]{0.48\textwidth}
  \vspace{0pt}
  \begin{prooftree}
    \AxiomC{$a : A$}
    \UnaryInfC{$\mathsf{inl}(a) : A + B$}
  \end{prooftree}
\end{minipage}\hfill
\begin{minipage}[t]{0.48\textwidth}
  \vspace{0pt}
  \begin{prooftree}
    \AxiomC{$b : B$}
    \UnaryInfC{$\mathsf{inr}(b) : A + B$}
  \end{prooftree}
\end{minipage}

\noindent In Lean:
\begin{lstlisting}[language=lean]
Sum.inl a : Sum A B   -- or A ⊕ B
Sum.inr b : Sum A B
\end{lstlisting}
\paragraph{Elimination Rule (case analysis)}
\begin{prooftree}
  \AxiomC{$p : A + B$}
  \AxiomC{$\begin{array}{c}  f : (A \implies C) \end{array}$}
  \AxiomC{$\begin{array}{c}  g : (B \implies C) \end{array}$}
  \TrinaryInfC{$\mathsf{case}(p, x.f, y.g) : C$}
\end{prooftree}
In Lean, we can use the \lstinline[language=lean]|cases|:
\begin{lstlisting}[language=lean]
example (p : Sum A B) (f : A → C) (g : B → C) : C := by
  cases p with
  | inl x => f x
  | inr y => g y
\end{lstlisting}
\paragraph{Function Types}
we can construct the function type $A \to B$,
representing functions from $A$ to $B$.
\paragraph{Introduction Rule (lambda abstraction)}
\begin{prooftree}
  \AxiomC{$\begin{array}{c} x : A  \vdash  \Phi : B \end{array}$}
  \UnaryInfC{$\lambda x.\Phi : A \to B$}
\end{prooftree}
In Lean, lambda abstraction is written using \lstinline[language=lean]|fun| or \lstinline[language=lean]|λ|:
\begin{lstlisting}[language=lean]
fun (x : A) => Phi : A → B
-- or using λ notation
λ (x : A) => Phi : A → B
-- Example: identity function
def id : A → A := fun x => x
-- or
def id : A → A := λ x => x
\end{lstlisting}
\paragraph{Elimination Rule (application)}
\begin{prooftree}
  \AxiomC{$f : A \to B$}
  \AxiomC{$a : A$}
  \BinaryInfC{$f(a) : B$}
\end{prooftree}
In Lean, function application is written using juxtaposition:
\begin{lstlisting}[language=lean]
example (f : A → B) (a : A) : B := f a
\end{lstlisting}
Functions are a primitive concept in type theory,
and we provide a brief introduction here.
We can \textbf{apply} a function $f : A \to B$
to an element $a : A$ to obtain an element of $B$,
denoted $f(a)$. In type theory, it is common to omit
the parentheses and write the application simply
as $f\, a$.
There are two equivalent ways to construct function
types: either by direct definition or by using
$\lambda$-abstraction.
Introducing a function by definition means that
we introduce a function by giving it a name (let's say, $f$) and saying we define $f : A \to B$ by giving an equation
\begin{equation} \label{lambda_definition}
  f(x) \coloneqq \Phi
\end{equation}
where $x$ is a variable and $\Phi$ is an expression
which may use $x$. In order for this to be valid,
we have to check that $\Phi : B$ assuming $x : A$.
Now we can compute $f(a)$ by replacing the variable $x$ in $\Phi$ with $a$. As an example,
consider the function $f : \mathbb{N} \to \mathbb{N}$ which
is defined by $f(x) \coloneqq x + x$. Then $f(2)$ is \textbf{definitionally equal} to $2 + 2$.
If we don't want to introduce a name for the
function, we can use \textbf{$\lambda$-abstraction}.
Given an expression $\Phi$ of type $B$ which
may use $x : A$, as above, we write $\lambda(x : A). \Phi$ to
indicate the same function defined by
(\ref{lambda_definition}). Thus, we have
$$ (\lambda(x : A). \Phi) : A \to B. $$
% \begin{example}
%   The previously defined function  has the typing
%   judgment
%   $$ (\lambda(x : \mathbb{N}). x + x) : \mathbb{N} \to \mathbb{N}. $$
%   As another example, for any types $A$ and $B$
%   and any element $y : B$, we have a
%   \textbf{constant function}
%   $$ (\lambda(x : A). y) : A \to B. $$
%   The \textbf{identity function} on any
%   type $A$ is given by
%   $$ (\lambda(x : A). x) : A \to A. $$
% \end{example}
By convention, the ``scope'' of the variable
binding ``$\lambda x.$''
is the entire rest of the expression,
unless delimited with parentheses.
Thus, for instance, $\lambda x. x + x$
should be parsed as $\lambda x.(x + x)$,
not as $(\lambda x. x) + x$ .
Now a $\lambda$-abstraction is a function,
so we can apply it to an argument $a : A$.
We then have the following computation
rule ($\beta$-reduction), which is a
\textbf{definitional equality}:
$$ (\lambda x. \Phi)(a) \equiv \Phi' $$
where $\Phi'$ is the expression $\Phi$ in
which all occurrences of $x$ have been
replaced by $a$.
Continuing the above example, we have
$(\lambda x. x + x)(2) \equiv 2 + 2. $
% Note that from any function $f : A \to B$,
% we can construct a lambda abstraction
% function $\lambda x. f(x)$.
% Since this is by definition ``the function
% that applies $f$ to its argument'' we consider
% it to be definitionally
% equal to $f$ (\textbf{$\eta$-conversion}):
% $$ f \equiv (\lambda x. f(x)). $$
% This equality is the uniqueness principle
% for function types, because it shows that $f$
% is uniquely determined by its values.
% The introduction of functions by definitions
% with explicit parameters can be reduced to
% simple definitions by using $\lambda$-abstraction:
% i.e., we can read a definition of $f : A \to B$ by
% $$ f(x) \coloneqq \Phi
% $$
% as
% $$ f \coloneqq \lambda x. \Phi. $$
When performing calculations involving variables, we must carefully preserve the \textbf{binding structure} of expressions during substitution. Consider the function $f : \mathbb{N} \to (\mathbb{N} \to \mathbb{N})$ defined as:
$$ f(x) \coloneqq \lambda y. x + y $$
Suppose we have assumed $y : \mathbb{N}$ somewhere in our context. What is $f(y)$?
A naive approach would replace $x$ with $y$ directly in the expression $\lambda y. x + y$, yielding $\lambda y. y + y$. However, this substitution is \textbf{semantically incorrect} because it causes \textbf{variable capture}: the free variable $y$ (referring to our assumption) becomes bound by the $\lambda$-abstraction, fundamentally altering the expression's meaning.
The correct approach uses \textbf{$\alpha$-conversion} (variable renaming).
Since bound variables have only local scope, we can consistently rename them while preserving binding structure.
The expression $\lambda y. x + y$ is judgmentally equal to $\lambda z. x + z$ for any fresh variable $z$. Therefore:
$$ f(y) \equiv \lambda z. y + z $$
This phenomenon parallels familiar mathematical practice: if $f(x) \coloneqq \int_1^2 \frac{dt}{x-t}$,
then $f(t)$ equals $\int_1^2 \frac{ds}{t-s}$, not the ill-defined $\int_1^2 \frac{dt}{t-t}$.
Lambda abstractions bind dummy variables exactly as integrals do.
For functions of multiple variables, we employ \textbf{currying} (named after mathematician Haskell Curry). Instead of using product types, we represent a two-argument function as a function returning another function.
A function taking inputs $a : A$ and $b : B$ to produce output in $C$ has type:
$$ f : A \to (B \to C) \equiv A \to B \to C $$
where the arrow associates to the right by convention.
Given $a : A$ and $b : B$, we apply $f$ sequentially: first to $a$, then the result to $b$, obtaining $f(a)(b) : C$.
To simplify notation and avoid excessive parentheses, we adopt several conventions. We write $f(a)(b)$ as $f(a, b)$ for abbreviated application. Without parentheses entirely, $f \, a \, b$ means $(f \, a) \, b$ following left-associative application. For multi-parameter definitions, we write $f(x, y) \coloneqq \Phi$ where $\Phi : C$ under assumptions $x : A$ and $y : B$.
Using $\lambda$-abstraction, such definitions correspond to:
$$ f \coloneqq \lambda x. \lambda y. \Phi $$
Alternative notation using map symbols:
$$ f \coloneqq x \mapsto y \mapsto \Phi $$
This currying approach extends naturally to functions of three or more arguments, allowing us to represent any multi-argument function as a sequence of single-argument functions.
\begin{example}\mbox{}
  \begin{lstlisting}[language=lean]
def add : Nat -> (Nat -> Nat) := fun x => (fun y => x + y)
#eval add 3 4   -- Output: 7
\end{lstlisting}
  Theoretically, lambda evaluation proceeds in steps:
  \begin{align*}
    \text{add } 3\, 4 & \equiv (\text{add } 3)\, 4                         \\
                      & \equiv ((\lambda x.\, \lambda y.\, x + y)\, 3)\, 4 \\
                      & \equiv ((\lambda y.\, x + y)[x := 3])\, 4          \\
                      & \equiv (\lambda y.\, 3 + y)\, 4                    \\
                      & \equiv (3 + y)[y := 4]                             \\
                      & \equiv 3 + 4                                       \\
                      & \equiv 7
  \end{align*}
\end{example}
% \paragraph{Computation Rules}
% \[
%   \mathsf{match}\ \mathsf{inl}(a) \ \mathsf{with} \ \dots \equiv f(a)
%   \qquad
%   \mathsf{match}\ \mathsf{inr}(b) \ \mathsf{with} \ \dots \equiv g(b)
% \]

% \section{Judgments and Propositions}

% Logic is often formalized through a framework that distinguishes clearly between
% \textbf{judgments} and \textbf{propositions}, following Martin-Löf's foundational approach
% (\cite{martin-lof-1983}, \cite{plato:intuitionistic-type-theory}).
% A \textbf{judgment} represents something we may know — an object of knowledge that becomes
% evident once we have a proof of it.

% The most fundamental form of judgment in logic is ``\textit{A is true}'', where \( A \) is a
% proposition. This is formally written as \( A\ \text{true} \).  
% When we derive such judgments under assumptions, we write
% $$
% \Gamma \vdash A\ \text{true},
% $$
% where \( \Gamma \) represents our hypothetical assumptions
% (\cite{pfenning-natded}).

% \begin{example}[Judgment-Based Inference Rules]
% All logical reasoning can be expressed through \textbf{introduction} and
% \textbf{elimination} rules for judgments.
% For instance, conjunction is characterized as follows.

% \textbf{Introduction:} how to establish the judgment \( A \land B\ \text{true} \):

% \begin{prooftree}
%   \AxiomC{\(A\ \text{true}\)}
%   \AxiomC{\(B\ \text{true}\)}
%   \RightLabel{\(\land\text{-I}\)}
%   \BinaryInfC{\(A \land B\ \text{true}\)}
% \end{prooftree}

% \textbf{Elimination:} how to use the judgment \( A \land B\ \text{true} \):

% \begin{minipage}[t]{0.45\textwidth}
% \begin{prooftree}
%   \AxiomC{\(A \land B\ \text{true}\)}
%   \RightLabel{\(\land\text{-E}_1\)}
%   \UnaryInfC{\(A\ \text{true}\)}
% \end{prooftree}
% \end{minipage}\hfill
% \begin{minipage}[t]{0.45\textwidth}
% \begin{prooftree}
%   \AxiomC{\(A \land B\ \text{true}\)}
%   \RightLabel{\(\land\text{-E}_2\)}
%   \UnaryInfC{\(B\ \text{true}\)}
% \end{prooftree}
% \end{minipage}
% \end{example}

% \begin{notation}[The Turnstile Symbol]
% The symbol \( \vdash \) (the \textit{turnstile}) separates assumptions from conclusions
% in judgments.  
% $$
% \Gamma \vdash J
% $$
% means that the judgment \( J \) follows from the assumptions \( \Gamma \), or equivalently,
% that \( J \) is evident given evidence for \( \Gamma \).

% When assumptions are \textit{discharged} during reasoning, we indicate this with square
% brackets \([A]\).
% For example, to establish an implication \( A \rightarrow B \), we assume \( A \)
% (written \([A]\)), derive \( B \) under this assumption, then discharge \( A \):

% \begin{prooftree}
%   \AxiomC{\([A\ \text{true}]^{u}\)}
%   \AxiomC{\(\vdots\)}
%   \AxiomC{\(B\ \text{true}\)}
%   \RightLabel{\(\rightarrow\text{-I}^{u}\)}
%   \TrinaryInfC{\(A \rightarrow B\ \text{true}\)}
% \end{prooftree}
% \end{notation}

% This judgment-based framework extends naturally to \textbf{type theory}, where additional
% judgment forms are introduced.  
% While logic focuses on the judgment \( A\ \text{true} \),
% type theory introduces several fundamental forms 
% (\cite{plato:intuitionistic-type-theory}):

% \begin{align}
%   &\Gamma \vdash A\ \text{type}
%     && \text{(\(A\) is a well-formed type)} \notag \\
%   &\Gamma \vdash t : A
%     && \text{(\(t\) is a term of type \(A\))} \notag \\
%   &\Gamma \vdash A \equiv B\ \text{type}
%     && \text{(types \(A\) and \(B\) are judgmentally equal)} \notag \\
%   &\Gamma \vdash t_1 \equiv t_2 : A
%     && \text{(terms \(t_1\) and \(t_2\) are judgmentally equal)} \notag \\
% \end{align}

% The \textbf{context} \( \Gamma \) represents a list of assumptions about variables and
% their types:
% $$
% \Gamma = x_1 : A_1,\, x_2 : A_2,\, \ldots,\, x_n : A_n.
% $$

% \begin{example}[Unified Introduction/Elimination Pattern]
% Both logical connectives and type constructors follow the same
% \textit{introduction/elimination} pattern.
% For function types (corresponding to implication):

% \textbf{Formation:}
% \begin{prooftree}
%   \AxiomC{\(\Gamma \vdash A\ \text{type}\)}
%   \AxiomC{\(\Gamma \vdash B\ \text{type}\)}
%   \BinaryInfC{\(\Gamma \vdash A \rightarrow B\ \text{type}\)}
% \end{prooftree}

% \textbf{Introduction:}
% \begin{prooftree}
%   \AxiomC{\(\Gamma, x : A \vdash b : B\)}
%   \RightLabel{\(\rightarrow\text{-I}\)}
%   \UnaryInfC{\(\Gamma \vdash \lambda x : A.\, b : A \rightarrow B\)}
% \end{prooftree}

% \textbf{Elimination:}
% \begin{prooftree}
%   \AxiomC{\(\Gamma \vdash f : A \rightarrow B\)}
%   \AxiomC{\(\Gamma \vdash a : A\)}
%   \RightLabel{\(\rightarrow\text{-E}\)}
%   \BinaryInfC{\(\Gamma \vdash f(a) : B\)}
% \end{prooftree}

% This correspondence reveals a deep connection:
% logical implication and function types share the same structural rules,
% differing only in focus — whether on truth (\(A \rightarrow B\ \text{true}\))
% or on typing (\(\lambda x.\,b : A \rightarrow B\)).
% \end{example}

% \subsection*{References}
% Per Martin-Löf, \textit{Intuitionistic Type Theory}, 1983–1984. \\
% P. Dybjer, \textit{Intuitionistic Type Theory}, \textit{Stanford Encyclopedia of Philosophy}, 2016. \\
% Frank Pfenning, \textit{Logical Frameworks and Natural Deduction}, lecture notes. \\
% Additional formal presentations and lecture notes on type theory and natural deduction, CMU (various).


\subsection{Curry Howard isomorphism}
We have been preparing for this argument, and the reader will have surely
noticed a strong similarity when defining logical connectives
using deduction rules; they are remarkably similar to types
constructed using type judgments. For instance, function
types can be seen as implications.
This is not a coincidence, but rather a fundamental theorem
first proven by Haskell Curry and William Howard.
It forms the core of modern type theory and establishes
a deep connection between logic, computation, and mathematics.
The isomorphism states:
\begin{align}
  \text{Propositions}        & \leftrightarrow \text{Types}              \\
  \text{Proofs}              & \leftrightarrow \text{Programs}           \\
  \text{Proof Normalization} & \leftrightarrow \text{Program Evaluation}
\end{align}
\noindent\textbf{Implication} ($P \Rightarrow Q$) corresponds to the \textbf{function type} ($P \to Q$).
A proof of an implication is a function that transforms any proof
of the premise into a proof of the conclusion.
\noindent\textbf{Conjunction} ($P \land Q$) corresponds
to the \textbf{product type} ($P \times Q$).
A proof of a conjunction consists of a pair containing proofs of both conjuncts.
\noindent\textbf{Disjunction} ($P \lor Q$) corresponds
to the \textbf{sum type} ($P + Q$).
A proof of a disjunction is either a proof of the
first disjunct or a proof of the second disjunct.
Lean uses inference rules and type
judgments as well as computing connectives using each related type.
For instance, $A \land B$ can be represented as \lstinline[language=lean]|And(A, B)| or \lstinline[language=lean]|A ∧ B|.
Its introduction rule is constructed by
\lstinline[language=lean]|And.intro _ _| or simply
\lstinline[language=lean]|⟨_, _⟩| (underscores are placeholders for assumptions or ``propositional functions'').
The pair $A \land B$ can then be consumed using elimination
rules \lstinline[language=lean]|And.left| and \lstinline[language=lean]|And.right|.

\begin{example}\label{ex:conj_intro_2}
  Let's look at our first Lean example:
  \begin{lstlisting}[language=lean]
    example (ha : a) (hb : b) : (a ∧ b) := And.intro ha hb
  \end{lstlisting}
  Lean aims to resemble the language used in mathematics.
  For instance, when defining a function or expression, one can use keywords such as
  \lstinline[language=lean]|theorem| or \lstinline[language=lean]|def|.
  Here, I used \lstinline[language=lean]|example|, which is handy
  for defining anonymous expressions
  for demonstration purposes.
  After that comes the statement to be proved:
  \begin{lstlisting}[language=lean]
    (ha : a) (hb : b) : (a ∧ b) 
  \end{lstlisting}
  This means: given a proof of $a$ and a proof of $b$, we can form a proof of $(a \land b)$.
  The operator \lstinline[language=lean]|:=| assigns a value (or returns an expression) for the statement, which
  ``has to be a proof of it.''
  \lstinline[language=lean]|And.intro| is implemented as:
  \newpage
  \begin{lstlisting}[language=lean]
    And.intro : p -> q -> (p ∧ q)
  \end{lstlisting}
  It says: if you give me a proof of $p$ and a proof of $q$,
  then I return a proof of $p \land q$.
  We therefore conclude the proof by directly giving
  \lstinline[language=lean]|And.intro ha hb|.
  Here is another way of writing the same statement:
  \newpage
  \begin{lstlisting}[language=lean]
    example (ha : a) (hb : b) : And(a, b) := ⟨ha, hb⟩
  \end{lstlisting}
\end{example}is system of inference rules allows us to construct proofs in an
algorithmic and systematical way, organized in what is called a \textbf{proof tree}.
To reduce complexity, we follow a
\textbf{top-down}
% approach (see \cite{thompson1999types} and
% \cite{nordstrom1990programming})
.
This methodology forms the basis of \textbf{proof assistants} like Lean,
Coq, and Agda, which help
verify the correctness of mathematical proofs by checking each step
against these rules.
We will see later that Lean, in fact, provides an info view of the proof tree
which helps us
understand and visualize
the proof structure.


Let's examine a concrete example of a proof.
\begin{example}[Associativity of Conjunction]
  We prove that $(A \land B) \land C$ implies $A \land (B \land C)$.
  First, from the assumption $(A \land B) \land C$, we can derive $A$:
  \begin{prooftree}
    \AxiomC{$(A \land B) \land C$}
    \RightLabel{$\land E_1$}
    \UnaryInfC{$A \land B$}
    \RightLabel{$\land E_1$}
    \UnaryInfC{$A$}
  \end{prooftree}
  Second, we can derive $B \land C$:
  \begin{prooftree}
    \AxiomC{$(A \land B) \land C$}
    \RightLabel{$\land E_1$}
    \UnaryInfC{$A \land B$}
    \RightLabel{$\land E_2$}
    \UnaryInfC{$B$}
    \AxiomC{$(A \land B) \land C$}
    \RightLabel{$\land E_2$}
    \UnaryInfC{$C$}
    \RightLabel{$\land I$}
    \BinaryInfC{$B \land C$}
  \end{prooftree}
  Finally, combining these derivations we obtain $A \land (B \land C)$:
  \begin{prooftree}
    \AxiomC{$(A \land B) \land C \vdash A$}
    \AxiomC{$(A \land B) \land C \vdash B \land C$}
    \RightLabel{$\land I$}
    \BinaryInfC{$A \land (B \land C)$}
  \end{prooftree}
\end{example}
\begin{example}[Lean Implementation]
  Let us now implement the same proof in Lean.
  \newpage
  \begin{lstlisting}[language=lean]
theorem and_associative (a b c : Prop) : (a ∧ b) ∧ c \to a ∧ (b ∧ c) :=
  fun h : (a ∧ b) ∧ c =>
  -- First, from the assumption (a ∧ b) ∧ c, we can derive a:
  have hab : a ∧ b := h.left -- extracts (derive) a proof of (a ∧ b) from the assumption
  have ha : a := hab.left -- extracts a from (a ∧ b)
  -- Second, we can derive b ∧ c (here we only extract b and c and combine them in the next step)
  have hc : c := h.right
  have hb : b := hab.right
  -- Finally, combining these derivations we obtain a ∧ (b ∧ c)
  show a ∧ (b ∧ c) from ⟨ha, ⟨hb, hc⟩⟩
\end{lstlisting}
  We introduce the \lstinline[language=lean]|theorem| with the name
  \lstinline[language=lean]|and_associative|.
  The type signature \lstinline[language=lean]|(a ∧ b) ∧ c → a ∧ (b ∧ c)|
  represents our logical implication.
  Here, we construct tehe proof term using a function with the \lstinline[language=lean]|fun| keyword.
  Why a function? We have already encountered the Curry-Howard correspondence in Lean
  previously, though without explicitly stating it.
  According to this correspondence, a proof of an implication can be
  understood as a function that takes a hypothesis as input and produces
  the desired conclusion as output. We will revisit this concept in more
  detail later.
  The \lstinline[language=lean]|have| keyword introduces local
  lemmas within our proof scope, allowing us to break down complex
  reasoning into manageable intermediate steps, mirroring our natural deduction proof from before.
  Just before the keyword \lstinline[language=lean]|show|, the info view displays the following
  context and goal:
  \begin{lstlisting}[language=lean]
  a b c : Prop
  h : (a ∧ b) ∧ c
  hab : a ∧ b
  ha : a
  hc : c
  hb : b
\vdash a ∧ b ∧ c
\end{lstlisting}
  Finally, the \lstinline[language=lean]|show| keyword explicitly states what
  we are proving and verifies that our provided term has the correct type.
  In this case, \lstinline[language=lean]|show a ∧ (b ∧ c) from ⟨ha, ⟨hb, hc⟩⟩|
  asserts that we are constructing a proof of \lstinline[language=lean]|a ∧ (b ∧ c)|
  using the term \lstinline[language=lean]|⟨ha, ⟨hb, hc⟩⟩|.
  The \lstinline[language=lean]|show| keyword serves two purposes:
  it makes the proof more readable by explicitly documenting what is being proved at this step,
  and it performs a type check to ensure the provided proof term matches the stated
  goal up to \textbf{definitional equality}.
  Two types are definitionally equal in Lean when they are identical after computation
  and unfolding of definitions—in other words, when Lean's type checker
  can mechanically verify they are the same without requiring additional proof steps.
  Here, the goal \lstinline[language=lean]|⊢ a ∧ b ∧ c| is definitionally
  equal to \lstinline[language=lean]|a ∧ (b ∧ c)| due to how conjunction
  associates, so \lstinline[language=lean]|show| accepts this statement.
  If we had tried to use \lstinline[language=lean]|show| with a type that
  was only \textbf{propositionally} equal (requiring a proof to establish equality)
  but not definitionally equal, Lean would reject it.
\end{example}
\subsection{Predicate logic and dependency}
To capture more complex mathematical ideas, we extend our system from
propositional logic to \textbf{predicate logic}.
A \textbf{predicate} is a statement or proposition that depends on a variable.
In propositional logic we represent a proposition simply by $P$.
In predicate logic, this is generalized: a predicate is written as $P(a)$,
where $a$ is a variable. Notice that a predicate is just a function.
This extension allows us to introduce \textbf{quantifiers}:
$\forall$ (``for all'') and $\exists$ (``there exists'').
These quantifiers express that a given formula holds either for every object
or for at least one object, respectively.
In Lean if \lstinline[language=lean]|α| is any type, we can represent a
predicate \lstinline[language=lean]|P| on \lstinline[language=lean]|α| as
an object of type \lstinline[language=lean]|α → Prop|.
Thus given an \lstinline[language=lean]|x : α| (an element
with type \lstinline[language=lean]|α| )
\lstinline[language=lean]|P(x) : Prop| would be representative of a proposition
holding for \lstinline[language=lean]|x|.
% When introducing variables into a formal language we must keep in mind that the specific choice 
% of a variable name can be substituted without
% changing the meaning of the predicate or statement. This should feel familiar from mathematics,
% where the meaning of an expression does not depend on the names we assign to variables.
% Some variables are \textbf{bound} (constrained), while others remain \textbf{free} 
% (arbitrary, in programming often called "dummy" variables). 
% When substituting variables, it is important to ensure that this distinction is preserved.
% This phenomenon, called \textbf{variable capture}, parallels familiar mathematical practice: 
% if $f(x) \coloneqq \int_1^2 \frac{dt}{x-t}$, then $f(t)$ equals $\int_1^2 \frac{ds}{t-s}$, 
% not the ill-defined $\int_1^2 \frac{dt}{t-t}$. The same principle applies to predicate logic. For example, consider
% [[
% $\exists y.\,(y > x)$.
% ]]
% This states that for a given $x$ there exists a $y$ such that $y > x$. 
% If we naively substitute $y+1$ for $x$, we would obtain
% [[
% $\exists y.\,(y > y+1)$,
% ]]
% where the $y$ in $y+1$ has been \textbf{captured} by the quantifier $\exists y$. 
% This transforms the original statement from "there exists some $y$ greater than the free variable $x$" into the always-false statement 
% "there exists some $y$ greater than itself plus one."
% To avoid the probelm, in the above example, we would first rename the bound variable to something fresh 
% say $z$, obtaining $\exists z.\,(z > x)$, and then safely substitute to get $\exists z.\,(z > y+1)$.
% \begin{notation}
%   We use the notation $\phi[t/x]$ for \textbf{substitution}, meaning all occurrences of the free 
%   variable $x$ in formula (or expression) $\phi$ are replaced by term $t$.
% \end{notation}
% We can now present the inference rules for quantifiers.
% \paragraph{Universal Quantification ($\forall$)}
% \begin{itemize}
%     \item Introduction
%     \begin{prooftree}
%     \AxiomC{$A$}
%     \RightLabel{$\forall I$}
%     \UnaryInfC{$\forall x.\,A$}
%     \end{prooftree}
%     The variable $x$ must be arbitrary in the derivation of $A$. 
%     This rule captures statements like 
%     $\forall x \in \mathbb{N}$, $x$ has a successor, 
%     but would not apply to $\forall x \in \mathbb{N}$, $x$ is prime 
%     (since we cannot derive this for an arbitrary natural number).
%     \item Elimination
%     \begin{prooftree}
%     \AxiomC{$\forall x.\,A$}
%     \RightLabel{$\forall E$}
%     \UnaryInfC{$A[t/x]$}
%     \end{prooftree}
%     The conclusion $A[t/x]$ represents the substitution of term $t$ for variable $x$ in formula $A$. 
%     From a proof of $\forall x.\,A(x)$ we can infer $A(t)$ for any term $t$.
% \end{itemize}
% \paragraph{Existential Quantification ($\exists$)}
% \begin{itemize}
%     \item Introduction
%     \begin{prooftree}
%     \AxiomC{$A[t/x]$}
%     \RightLabel{$\exists I$}
%     \UnaryInfC{$\exists x.\,A$}
%     \end{prooftree}
%     The substitution premise means that if we can find a specific term $t$ for which $A(t)$ holds, 
%     then we can introduce the existential quantifier. 
%     The introduction rule requires a witness $t$ for which the predicate holds.
%     \item Elimination
%     \begin{prooftree}
%     \AxiomC{$\exists x.\,A$}
%     \AxiomC{$[A] \vdash B$}
%     \RightLabel{$\exists E$}
%     \BinaryInfC{$B$}
%     \end{prooftree}
%     To eliminate an existential quantifier, we assume $A$ holds for some witness 
%     and derive $B$ without making any assumptions about the specific witness.
% \end{itemize}

This time we are not going to involve deduction rules or type judgments.
Instead, we will extend the Curry--Howard isomorphism to quantifiers directly
by presenting the Lean syntax.
We can give an informal reading of the quantifiers as infinite logical operations:
\begin{align*}
  \forall x.\,P(x) & \equiv P(a) \land P(b) \land P(c) \land \ldots \\
  \exists x.\,P(x) & \equiv P(a) \lor P(b) \lor P(c) \lor \ldots
\end{align*}
The expression $\forall x.\, P(x)$ can be understood as a generalized form of conjunction.
It expresses that $P$ holds for all possible values of $x$.
Similarly, $\exists x.\, P(x)$ is a generalized disjunction, expressing that $P$ holds
for at least one value of $x$.

Under the Curry-Howard isomorphism, universal quantifiers correspond to
\textbf{dependent function types} (also called Pi types, written $\Pi$),
while existential quantifiers correspond to
\textbf{dependent pair types} (also called Sigma types, written $\Sigma$).
These are constructs from dependent type theory, which provides a way to interpret
predicates or, more generally, types depending on some data or variable.
This generalizes concepts such as predicates and functions.
\begin{example}[Quantifiers in Lean]
  Lean expresses quantifiers as follows:
  \begin{lstlisting}[language=lean]
  ∀ (x : X), P x
  Forall (x : X), P x
  -- Equivalently, using Pi types
  Π (x : X), P x
  \end{lstlisting}
  \begin{lstlisting}[language=lean]
  ∃ x : α, p
  Exists (λ x : α => p)
  -- Equivalently, using Sigma types
  Σ x : α, p
  \end{lstlisting}
\end{example}
\begin{example}[Universal introduction in Lean]
  The \textbf{universal introduction rule} allows us to prove $\forall x, P(x)$
  by proving $P(x)$ for an \textbf{arbitrary} $x$.
  In Lean, this corresponds to lambda abstraction (constructing a function):
  \newpage
  \begin{lstlisting}[language=lean]
  example : ∀ n : Nat, n ≥ 0 :=
    fun n => Nat.zero_le n
  \end{lstlisting}
\end{example}
\begin{example}[Universal elimination in Lean]
  The \textbf{universal elimination rule} allows us to instantiate
  a universally quantified statement with a specific value.
  In Lean, this is simply function application:
  \begin{lstlisting}[language=lean]
  example (h : ∀ n : Nat, n ≥ 0) : 5 ≥ 0 :=
    h 5
  \end{lstlisting}
\end{example}
\begin{example}[Existential introduction in Lean]
  When introducing an \textbf{existential} proof,
  we need a \textbf{pair} consisting
  of a witness and a proof that this witness
  satisfies the statement.
  \begin{lstlisting}[language=lean]
  example (x : Nat) (h : x > 0) : ∃ y, y < x :=
    ⟨0, h⟩
  \end{lstlisting}
  Notice that \lstinline[language=lean]|⟨0, h⟩| is a product type holding
  data (the witness~0) and a proof that it satisfies the property.
\end{example}
\begin{example}[Existential elimination in Lean]
  The \textbf{existential elimination rule}
  (\lstinline[language=lean]|Exists.elim|) allows us to prove a proposition $Q$
  from $\exists x, P(x)$ by showing that $Q$ follows from $P(w)$
  for an \textbf{arbitrary} value $w$.
  The existential quantifier can be interpreted as an infinite disjunction,
  so existential elimination naturally corresponds to a \textbf{proof by cases}.
  In Lean, this is done using pattern matching with the \lstinline[language=lean]|cases| tactic:
  \newpage
  \begin{lstlisting}[language=lean]
  example (h : ∃ n : Nat, n > 0) : ∃ n : Nat, n > 0 := by
    cases h with
    | intro witness proof => exact ⟨witness, proof⟩
  \end{lstlisting}
\end{example}
\section{Describing and use properties}
Functions are primitive objects in type theory.
For example, it is interesting to note that a relation can be expressed as a function:
\lstinline[language=lean]|R : α → α → Prop|.
Similarly, when defining a predicate (\lstinline[language=lean]|P : α → Prop|) we must first declare
\lstinline[language=lean]|α : Type| to be some arbitrary type.
This is what is called \textbf{polymorphism}, more specifically \textbf{parametrical polymorphism}.
A canonical example is the identity function, written as
\lstinline[language=lean]|α → α|, where
\lstinline[language=lean]|α| is a type variable.
It has the same type for
both its domain and codomain, this means it can be
applied to booleans (returning a boolean), numbers (returning a number),
functions (returning a function), and so on.
In the same spirit, we can define a transitivity property of a relation as follows:
\begin{lstlisting}[language=lean]
def Transitive (α : Type) (R : α → α → Prop) : Prop :=
  ∀ x y z, R x y → R y z → R x z
\end{lstlisting}
To use \lstinline[language=lean]|Transitive|, we must provide both the type
\lstinline[language=lean]|α| and the relation itself.
For example, here is a proof of transitivity for the less-than relation on
$\mathbb{N}$ ( in Lean \lstinline[language=lean]|Nat| or \lstinline[language=lean]|ℕ|):
\begin{lstlisting}[language=lean]
theorem le_trans_proof : Transitive Nat (· ≤ · : Nat → Nat → Prop) :=
  fun x y z h1 h2 => Nat.le_trans h1 h2 -- this lemma is provided by Lean 
\end{lstlisting}
Looking at this code, we immediately notice that explicitly
passing the type argument \lstinline[language=lean]|Nat| is somewhat repetitive.
Lean allows us to omit it by letting the type inference mechanism fill it in automatically.
This is achieved by using \textbf{implicit arguments} with curly brackets:
\begin{lstlisting}[language=lean]
def Transitive {α : Type} (R : α → α → Prop) : Prop :=
  ∀ x y z, R x y → R y z → R x z
theorem le_trans_proof : Transitive (· ≤ · : Nat → Nat → Prop) :=
  fun x y z h1 h2 => Nat.le_trans h1 h2 
\end{lstlisting}
Lean's type inference system is quite powerful: in many cases, types can be completely
inferred without explicit annotations. For instance, (NEED TO EXPLAIN TYPE INFERENECE).
Let us now revisit the transitivity proof, but this time for the less-than-equal relation on
the rational numbers (\lstinline[language=lean]|Rat| or \lstinline[language=lean]|ℚ|) instead.
\newpage
\begin{lstlisting}[language=lean]
import Mathlib

theorem rat_le_trans : Transitive (· ≤ · :   Rat → Rat → Prop) :=
  fun _ _ _ h1 h2 => Rat.le_trans h1 h2
\end{lstlisting}
Here, \lstinline[language=lean]|Rat| denotes the rational numbers in Lean,
and \lstinline[language=lean]|Rat.le_trans| is the transitivity lemma
for \lstinline[language=lean]|≤| on rational numbers, provided by Mathlib.
We import Mathlib to access \lstinline[language=lean]|Rat|
and \lstinline[language=lean]|le_trans|.
Mathlib is the community‑driven mathematical
library for Lean, containing a large body of formalized mathematics
and ongoing development.
It is the defacto standard library for both programming and proving
in Lean \cite{mathlib2020}, we will dig into it as we go along.
Notice that we used a function to discharge the universal
quantifiers required by transitivity. The underscores indicate
unnamed variables that we do not use later. If we had named
them, say \lstinline|x y z|, then:
\lstinline[language=lean]|h1| would be a proof of \lstinline[language=lean]|x ≤ y|,
\lstinline[language=lean]|h2| would be a proof of \lstinline[language=lean]|y ≤ z|,
and \lstinline[language=lean]|Rat.le_trans h1 h2| produces a proof of \lstinline[language=lean]|x ≤ z|.
The \lstinline[language=lean]|Transitive| definition is imported from Mathlib and similarly
defined as before.
\begin{example}
  The code can be made more readable using tactic mode.
  In this mode, you use tactics—commands provided by Lean or defined by users—to
  carry out proof steps succinctly, avoid code repetition,
  and automate common patterns.
  This often yields shorter, clearer proofs than writing the full term by hand.
  \begin{lstlisting}[language=lean]
import Mathlib

theorem rat_le_trans : Transitive (· ≤ · : Rat → Rat → Prop) := by
intro x y z hxy hyz
exact Rat.le_trans hxy hyz
\end{lstlisting}
  This proof performs the same steps but is much easier to read.
  Using \lstinline[language=lean]|by| we enter Lean's tactic mode,
  which (together with the info view)
  shows the current goal and context.
  Move your cursor just before \lstinline[language=lean]|by|
  and observe how the info view changes.
  The goal is initially displayed as \lstinline[language=lean]|⊢ Transitive fun x1 x2 ↦ x1 ≤ x2|.
  The tactic \lstinline[language=lean]|intro| is mainly used to introduce
  variables and hypotheses corresponding to universal quantifiers
  and assumptions into the context (essentially deconstructing universal quantifiers and implications).
  Now position your cursor just before \lstinline[language=lean]|exact|
  and observe the info view again.
  The goal is now \lstinline[language=lean]|⊢ x ≤ z|, with the context
  showing the variables and hypotheses introduced by the previous tactic.
  The \lstinline[language=lean]|exact| tactic closes the goal
  by supplying the term \lstinline[language=lean]|Rat.le_trans hxy hyz| that exactly matches the goal
  (the specification of \lstinline[language=lean]|Transitive|).
  You can hover over each tactic to see its definition and documentation.
\end{example}
\subsection{Exploring Mathlib (The Rat structure)}
In these examples we cheated and have used predefined lemmas such as
\lstinline[language=lean]|Nat.le_trans| and
\lstinline[language=lean]|Rat.le_trans|, just to simplify the presentation.
We can now dig into the implementation of these lemmas.
Let's look at the source code of \lstinline[language=lean]|Rat.le_trans|.
The Mathlib 4 documentation website is at
\url{https://leanprover-community.github.io/mathlib4_docs}, and
the documentation for
\lstinline[language=lean]|Rat.le_trans| is at
\url{https://leanprover-community.github.io/mathlib4_docs/Mathlib/Algebra/Order/Ring/Unbundled/Rat.html#Rat.le_trans}.
Click the "source" link there to jump to the implementation in the Mathlib repository. In editors like
VS Code you can also jump directly to the definition (Ctrl+click; Cmd+click on macOS).
Another way to check source code is by using \lstinline[language=lean]|#print Rat.le_trans|.
\newpage
\begin{lstlisting}[language=lean]
variable (a b c : Rat)
protected lemma le_trans (hab : a ≤ b) (hbc : b ≤ c) : a ≤ c := by
  rw [Rat.le_iff_sub_nonneg] at hab hbc
  have := Rat.add_nonneg hab hbc
  simp_rw [sub_eq_add_neg, add_left_comm (b + -a) c (-b), add_comm (b + -a) (-b), add_left_comm (-b) b (-a), add_comm (-b) (-a), add_neg_cancel_comm_assoc, ← sub_eq_add_neg] at this
  rwa [Rat.le_iff_sub_nonneg]
\end{lstlisting}
The proof uses several tactics and lemmas from Mathlib.
The \lstinline[language=lean]|rw| or \lstinline[language=lean]|rewrite| tactic
is very common and sintactically similar to
the mathematical practice of rewriting an expression using an equality.
In this case, with \lstinline[language=lean]|at|, we use it to rewrite the
hypotheses \lstinline[language=lean]|hab|
and \lstinline[language=lean]|hbc|
using the another Mathlib's lemma \lstinline[language=lean]|Rat.le_iff_sub_nonneg|,
which states that for any two rational numbers \lstinline[language=lean]|x| and
\lstinline[language=lean]|y|, \lstinline[language=lean]|x ≤ y|
is equivalent to \lstinline[language=lean]|0 ≤ y - x|.
Thus we now have the hypotheses tranformerd to :
\begin{lstlisting}[language=lean]
  hab : 0 ≤ b - a
  hbc : 0 ≤ c - b
\end{lstlisting}
The \lstinline[language=lean]|have| tactic introduces an intermediate result.
If you omit a name, Lean assigns it the default name \lstinline[language=lean]|this|.
In our situation, from \lstinline[language=lean]|hab : a ≤ b| and \lstinline[language=lean]|hbc : b ≤ c|
we can derive that \lstinline[language=lean]|b - a| and \lstinline[language=lean]|c - b|
are nonnegative, hence their sum is nonnegative:
\begin{lstlisting}[language=lean]
  this : 0 ≤ b - a + (c - b)
\end{lstlisting}
The most involved step uses \lstinline[language=lean]|simp_rw| to
simplify the expression via a sequence of other existing Mathlib's lemmas.
The tactic \lstinline[language=lean]|simp_rw| is a variant of \lstinline[language=lean]|simp|:
it performs rewriting using the simp set (and any lemmas you provide), applying the rules
in order and in the given direction. Lemmas that \lstinline[language=lean]|simp| can use
are typically marked with the \lstinline[language=lean]|@[simp]| attribute.
This is particularly useful for simplifying algebraic expressions and equations.
After these simplifications we obtain:
\begin{lstlisting}[language=lean]
  this : 0 ≤ c - a
\end{lstlisting}
Clearly, the proof relies mostly on \lstinline[language=lean]|Rat.add_nonneg|.
Its source code is fairly involved and uses advanced features
that are beyond our current scope. Nevertheless, it highlights
an important aspect of formal mathematics in Mathlib.
Mathlib defines \lstinline[language=lean]|Rat| as an instance of
a linear ordered field, implemented via a normalized fraction
representation: a pair of integers (numerator and denominator)
with positive denominator and coprime numerator and denominator \cite{mathlibdoc}.
To achieve this, it uses a \textbf{structure}. In Lean, a structure is a dependent record
(or product type) type  used to group together related fields or properties as a single data type.
Unlike ordinary records, the type of later fields may depend on the values of earlier ones.
Defining a structure automatically introduces a constructor (usually mk) and projection
functions that retrieve (deconstruct) the values of its fields.
Structures may also include proofs expressing properties that the fields must satisfy.
\newpage
\begin{lstlisting}[language=lean]
  structure Rat where
    /-- Constructs a rational number from components.
    We rename the constructor to `mk'` to avoid a clash with the smart constructor. -/
    mk' ::
    /-- The numerator of the rational number is an integer. -/
    num : Int
    /-- The denominator of the rational number is a natural number. -/
    den : Nat := 1
    /-- The denominator is nonzero. -/
    den_nz : den ≠ 0 := by decide
    /-- The numerator and denominator are coprime: it is in "reduced form". -/
    reduced : num.natAbs.Coprime den := by decide
\end{lstlisting}
In order to work with rational numbers in Mathlib, we use the
\lstinline[language=lean]|Rat.mk'| constructor to create a rational number from
its numerator and denominator, if omitted the default would be \lstinline[language=lean]|Rat.mk|.
The fields \lstinline[language=lean]|den_nz| and \lstinline[language=lean]|reduced| are proofs that
the denominator is nonzero and that the numerator and denominator are coprime, respectively.
These proofs are automatically generated by Lean's \lstinline[language=lean]|decide| tactic, which can
solve certain decidable propositions (to be discussed in the next section).
\begin{example}
  Here is how we can define and manipulate rational numbers in Lean.
  \begin{lstlisting}[language=lean]
    def half : Rat := Rat.mk' 1 2
    def third : Rat := Rat.mk' 1 3
  \end{lstlisting}
\end{example}
When working with rational numbers, or more generally with structures, we must provide the
required proofs as arguments to the constructor (or Lean must be able to ensure them).
For instance \lstinline[language=lean]|Rat.mk' 1 0| or \lstinline[language=lean]|Rat.mk' 2 6|
would be rejected.
In the case of rationals, Mathlib unfolds the definition through
\lstinline[language=lean]|Rat.numDenCasesOn|. This principle states that, to prove a property of an
arbitrary rational number, it suffices to consider numbers of the form \lstinline[language=lean]|n /. d|
in canonical (normalized) form, with \lstinline[language=lean]|d > 0| and \lstinline[language=lean]|gcd n d = 1|.
This reduction allows mathlib to transform proofs about \lstinline[language=lean]|ℚ|
into proofs about \lstinline[language=lean]|ℤ| and \lstinline[language=lean]|ℕ|,
and then lift the result back to rationals.
\begin{example}
  We present a simplified implementation of addition
  non-negativity for rationals (\lstinline[language=lean]|Rat.add_nonneg| ),
  maintaining a similar approach: projecting everything to the natural numbers and
  integers first. To illustrate the proof technique clearly, we avoid using existing lemmas
  from the Rat module in Mathlib.
  Mathlib is indeed organized into modules by mathematical
  domain (e.g., Nat, Int, Rat).
  % Lemmas are typically namespaced (e.g., Rataddnonneg) 
  % and often marked protected to prevent namespace pollution. 

  We start by defining helper lemmas needed in the main proof.
  Given a natural number
  (which in this case represents the denominator of a rational number) that is not
  equal to zero, we prove it must be positive. This follows directly by
  applying the Mathlib lemma \lstinline[language=lean]|Nat.pos_of_ne_zero|:
  \newpage
  \begin{lstlisting}[language=lean]
  import Mathlib

  lemma nat_ne_zero_pos (den : ℕ) (h_den_nz : den ≠ 0) : 0 < den :=
    Nat.pos_of_ne_zero h_den_nz
\end{lstlisting}
  The naming convention follows Mathlib
  best practices aiming to be descriptive by indicating
  types and properties involved.

  The following lemma is slightly more involved.
  It states that if a rational number (num / den)
  is non-negative, then its numerator must also be non-negative:
  \begin{lstlisting}[language=lean]
lemma rat_num_nonneg {num : ℤ} {den : ℕ} (hden_pos : 0 < den)
    (h : (0 : ℚ) ≤ num / den) : 0 ≤ num := by
  contrapose! h
  have hden_pos_to_rat : (0 : ℚ) < den := Nat.cast_pos.mpr hden_pos
  have hnum_neg_to_rat : num < (0 : ℚ) := Int.cast_lt.mpr h
  exact div_neg_of_neg_of_pos hnum_neg_to_rat hden_pos_to_rat
\end{lstlisting}
  The lemma requires the denominator to be positive as well as the non-negativity of the rational number,
  expressed as \lstinline[language=lean]|num / den| where the types of
  \lstinline[language=lean]|num| and \lstinline[language=lean]|den| are inferred.
  First, notice the type annotation \lstinline[language=lean]|(0 : ℚ)|.
  This explicit type annotation on zero forces the entire equation to be casted
  into rational numbers.
  Without this annotation, Lean would infer \lstinline[language=lean]|0|
  as a natural number by default. However, since the main theorem we are proving concerns rational numbers,
  we must ensure all comparisons occur in \lstinline[language=lean]|ℚ|.
  The tactic \lstinline[language=lean]|contrapose!| does what you might expect: it proves a statement by contraposition. According to the documentation:
  \begin{itemize}
    \item \lstinline[language=lean]|contrapose| turns a goal \lstinline[language=lean]|P → Q| into \lstinline[language=lean]|¬ Q → ¬ P|
    \item \lstinline[language=lean]|contrapose!| turns a goal \lstinline[language=lean]|P → Q| into \lstinline[language=lean]|¬ Q → ¬ P| and pushes negations inside \lstinline[language=lean]|P| and \lstinline[language=lean]|Q| using \lstinline[language=lean]|push_neg|
    \item \lstinline[language=lean]|contrapose h| first reverts the local assumption \lstinline[language=lean]|h|, then uses \lstinline[language=lean]|contrapose| and \lstinline[language=lean]|intro h|
    \item \lstinline[language=lean]|contrapose! h| first reverts the local assumption \lstinline[language=lean]|h|, then uses \lstinline[language=lean]|contrapose!| and \lstinline[language=lean]|intro h|
  \end{itemize}
  In our case, \lstinline[language=lean]|contrapose! h| transforms the goal from
  proving \lstinline[language=lean]|0 ≤ num| to assuming \lstinline[language=lean]|num < 0|
  and proving \lstinline[language=lean]|num / den < 0|.
  We then introduce two local hypotheses. The first, \lstinline[language=lean]|hden_pos_to_rat|,
  proves that the denominator
  is positive when cast to rationals, using \lstinline[language=lean]|Nat.cast_pos|.
  The suffix \lstinline[language=lean]|.mpr| selects the ``modus ponens reverse''
  direction of the biconditional (the \lstinline[language=lean]|←|
  direction of the \lstinline[language=lean]|↔|).
  Next, we introduce \lstinline[language=lean]|hnum_neg_to_rat|,
  which expresses that the numerator is negative when cast to rationals, using \lstinline[language=lean]|Int.cast_lt| with \lstinline[language=lean]|.mpr| again.
  Finally, we apply \lstinline[language=lean]|div_neg_of_neg_of_pos|,
  which states that dividing a negative number by a positive number yields a
  negative result, thus completing the proof by contraposition.
  Note that we are allowing ourselves to use existing lemmas from Mathlib,
  such as \lstinline[language=lean]|div_neg_of_neg_of_pos| from the \lstinline[language=lean]|Field| module,
  but not from the \lstinline[language=lean]|Rat| module,
  to keep the presentation clear and focused on the main proof techniques.
  \newpage
  Now we can prove the main result:
  \begin{lstlisting}[language=lean]
lemma rat_add_nonneg (a b : Rat) : 0 ≤ a → 0 ≤ b → 0 ≤ a + b := by

  intro ha hb
  cases a with | div a_num a_den a_den_nz a_cop =>
  cases b with | div b_num b_den b_den_nz b_cop =>
  -- Goal: ⊢ 0 ≤ ↑a_num / ↑a_den + ↑b_num / ↑b_den
  rw[div_add_div] -- applies the addition formula requiring two new goals 
  · sorry 
  · sorry 
  · sorry
\end{lstlisting}
  \newpage
  We first introduce the two hypotheses \lstinline[language=lean]|ha| and
  \lstinline[language=lean]|hb| into the context using \lstinline[language=lean]|intro|.
  As mentioned earlier, a structure can be viewed as a product type or a record type with
  a single constructor. The tactic \lstinline[language=lean]|cases a with|
  exposes the fields of \lstinline[language=lean]|Rat|: the
  numerator ()\lstinline[language=lean]|a_num|), denominator
  (\lstinline[language=lean]|a_den|), the proof that the denominator is non-zero
  (\lstinline[language=lean]|a_den_nz|), and the coprimality condition
  (\lstinline[language=lean]|a_cop|). Notice how the goal transforms
  the rationals \lstinline[language=lean]|a| and \lstinline[language=lean]|b| into:
  \begin{lstlisting}[language=lean]
⊢ 0 ≤ ↑a_num / ↑a_den + ↑b_num / ↑b_den
\end{lstlisting}
  where \lstinline[language=lean]|↑| denotes type coercion from
  \lstinline[language=lean]|ℤ| or \lstinline[language=lean]|ℕ| to
  \lstinline[language=lean]|ℚ|.
  Now we rewrite the goal using \lstinline[language=lean]|rw [div_add_div]|,
  a theoprem from the \lstinline[language=lean]|Field| module,
  which applies the addition formula for division.
  Let us briefly examine the source code of this theorem:
  \begin{lstlisting}[language=lean]
variable [Semifield K] {a b d : K}

theorem div_add_div (a : K) (c : K) (hb : b ≠ 0) (hd : d ≠ 0) :
    a / b + c / d = (a * d + b * c) / (b * d) := ...
\end{lstlisting}
  The type \lstinline[language=lean]|K| here is assumed to be a
  \lstinline[language=lean]|Semifield|. The \lstinline[language=lean]|variable|
  keyword is a way to declare parameters that are potentially used across
  multiple theorems or definitions. We will explore Lean's powerful algebraic
  hierarchy and the meaning of the square brackets \lstinline[language=lean]|[ ]|
  in a later section.
  Using this rewrite is particularly time-saving, since otherwise one would have to
  establish the well-definedness of rational addition in terms of the underlying
  structure (a non-trivial task).
  This theorem requires proofs \lstinline[language=lean]|(hb : b ≠ 0)| and
  \lstinline[language=lean]|(hd : d ≠ 0)|, generating two additional side goals.
  We handle each goal separately using the focusing bullet \lstinline[language=lean]|·|.
  The first bullet addresses the main goal (proving the sum is non-negative),
  while the subsequent bullets discharge the non-zero denominator conditions.
  I have omitted the actual proofs, here, using \lstinline[language=lean]|sorry|,
  which we haven't mentioned before. \lstinline[language=lean]|sorry| is a useful
  feature of Lean that tells the system to accept an incomplete proof for the time being,
  allowing you to continue development without proving every detail immediately.
  We can now tackle the remaining goals:
  \newpage
  \begin{lstlisting}[language=lean]
· -- Goal: ⊢ 0 ≤ (↑a_num * ↑b_den + ↑a_den * ↑b_num) / (↑a_den * ↑b_den)
  have hnum_nonneg : (0 : ℚ) ≤ a_num * b_den + a_den * b_num := by
    have ha_num_nonneg := by
      have ha_den_pos := nat_ne_zero_pos a_den a_den_nz
      exact rat_num_nonneg ha_den_pos ha
    have hb_num_nonneg := by
      have hb_den_pos := nat_ne_zero_pos b_den b_den_nz
      exact rat_num_nonneg hb_den_pos hb
    apply add_nonneg -- works for any OrderedAddCommMonoid
    · apply mul_nonneg -- works for any OrderedSemiring
      · exact Int.cast_nonneg.mpr ha_num_nonneg
      · exact Nat.cast_nonneg b_den
    · apply mul_nonneg
      · exact Nat.cast_nonneg a_den
      · exact Int.cast_nonneg.mpr hb_num_nonneg

  have hden_nonneg : (0 : ℚ) ≤ a_den * b_den := by
    rw [← Nat.cast_mul]
    exact Nat.cast_nonneg (a_den * b_den)
  exact div_nonneg hnum_nonneg hden_nonneg

· exact Nat.cast_ne_zero.mpr a_den_nz -- Goal: ⊢ ↑a_den ≠ 0
· exact Nat.cast_ne_zero.mpr b_den_nz -- Goal: ⊢ ↑b_den ≠ 0
\end{lstlisting}

  We introduce two key hypotheses, \lstinline[language=lean]|hnum_nonneg|
  and \lstinline[language=lean]|hden_nonneg|, which will be required by
  \lstinline[language=lean]|div_nonneg| from the
  \lstinline[language=lean]|GroupWithZero| module.
  This lemma provides us with a term that directly validates our statement.
  Note that \lstinline[language=lean]|div_nonneg| is a generalized lemma that applies
  not only to rational numbers but to all ordered groups with zero that are
  also partially ordered.
  The hypothesis \lstinline[language=lean]|hnum_nonneg| proves that the numerator
  is non-negative by working with the coerced expressions in
  \lstinline[language=lean]|ℚ|. It uses \lstinline[language=lean]|add_nonneg| and
  \lstinline[language=lean]|mul_nonneg|, which are general theorems that work for
  any ordered additive commutative monoid and ordered semiring, respectively.
  The actual reasoning is done using integer-related theorems
  (via \lstinline[language=lean]|Int.cast_nonneg|) for the numerators and natural
  number theorems (via \lstinline[language=lean]|Nat.cast_nonneg|) for the denominators.
  The hypothesis \lstinline[language=lean]|hden_nonneg| proves that the
  denominator is non-negative by working entirely with natural numbers.
  We use the rewrite \lstinline[language=lean]|rw [← Nat.cast_mul]|,
  which moves the coercion (in this case from \lstinline[language=lean]|ℕ|
  to \lstinline[language=lean]|ℚ|) inside the multiplication:
  \lstinline[language=lean]|↑(m * n) = ↑m * ↑n|. The \lstinline[language=lean]|←|
  symbol means that we want the transformation from right to
  left (i.e., we apply the equality in reverse to move the cast inward).
  Type casts and coercions require these kinds of rewrite rules, not only
  for multiplication but also for addition and other operations, and
  similarly for \lstinline[language=lean]|ℤ| or other numerical types.
  These lemmas, such as \lstinline[language=lean]|Nat.cast_mul|,
  \lstinline[language=lean]|Int.cast_add|, etc., ensure that algebraic operations
  commute with type coercions.
\end{example}
\subsection{Coercions and Type Casting}
We extensively used type casting and coercions in this proof, which requires some
explanation \cite{lewis_madelaine_simplifying_casts_coercions_2020}.
Lean's type system lacks subtyping, means that types
like \lstinline[language=lean]|ℕ|, \lstinline[language=lean]|ℤ|, and \lstinline[language=lean]|ℚ|
are distinct and do not have a subtype relationship. In order to translate between these types,
we need to use explicit type casts or rely on automatic coercions. For example,
natural numbers (\lstinline[language=lean]|ℕ|) can be coerced to integers (\lstinline[language=lean]|ℤ|),
and integers can be coerced to rational numbers (\lstinline[language=lean]|ℚ|).
Casting and coercion are related but distinct concepts:
\begin{itemize}
  \item \textbf{Casting} refers to the explicit conversion of a value from one type to another,
        typically using functions like \lstinline[language=lean]|Int.cast| or \lstinline[language=lean]|Nat.cast|.
        These functions have accompanying lemmas that preserve properties across type conversions,
        such as \lstinline[language=lean]|Int.cast_lt| and \lstinline[language=lean]|Nat.cast_pos|.
  \item \textbf{Coercion}, on the other hand, is a more general mechanism that allows
        Lean to automatically convert between types when needed.
        More generally, in expressions like \lstinline[language=lean]|x + y| where \lstinline[language=lean]|x|
        and \lstinline[language=lean]|y| are of different types,
        Lean will automatically coerce them to a common type. For example, if \lstinline[language=lean]|x : ℕ|
        and \lstinline[language=lean]|y : ℤ|, then \lstinline[language=lean]|x|
        will be coerced to \lstinline[language=lean]|ℤ|.
\end{itemize}
The notation \lstinline[language=lean]|↑| denotes an explicit coercion
(in between cast and coercion).
To illustrate the expected behavior of coercion simplification, consider
the expression \lstinline[language=lean]|↑m + ↑n < (10 : ℤ)|,
where \lstinline[language=lean]|m, n : ℕ| are cast to \lstinline[language=lean]|ℤ|.
The expected normal form is \lstinline[language=lean]|m + n < (10 : ℕ)|,
since \lstinline[language=lean]|+|,
<,
and the numeral \lstinline[language=lean]|10| are polymorphic
(i.e., they can work with any numerical type such as \lstinline[language=lean]|ℤ|
or \lstinline[language=lean]|ℕ|). The simplification should proceed as follows:
\begin{enumerate}
  \item Replace the numeral on the right with the cast of a natural number:
        \lstinline[language=lean]|↑m + ↑n < ↑(10 : ℕ)|
  \item Factor \lstinline[language=lean]|↑| to the outside on the left:
        \lstinline[language=lean]|↑(m + n) < ↑(10 : ℕ)|
  \item Eliminate both casts to obtain an inequality over \lstinline[language=lean]|ℕ|:
        \lstinline[language=lean]|m + n < (10 : ℕ)|
\end{enumerate}
Lean provides tactics like \lstinline[language=lean]|norm_cast|
to simplify expressions involving such coercions.
The \lstinline[language=lean]|norm_cast| tactic normalizes casts
by pushing them outward and eliminating redundant coercions, often simplifying
proofs significantly by reducing goals to their ``native'' types.

\subsection{Type Classes and Algebraic Hierarchy in Lean}

In our proof of \lstinline[language=lean]|rat_add_nonneg|,
we used many generalized lemmas from Mathlib, such as \lstinline[language=lean]|add_nonneg|,
\lstinline[language=lean]|mul_nonneg|, and \lstinline[language=lean]|div_nonneg|,
which apply to a wide range of types beyond just rational numbers.
Similarly, in our earlier work with natural numbers, we used \lstinline[language=lean]|Nat.le_trans|,
a theorem specifically for natural numbers that is part of Lean's core library
(\lstinline[language=lean]|lean/Init/Prelude.lean|). Mathlib is built on top of this base library.
However, the transitivity property holds not only for naturals but also for integers,
reals, and, in fact, for any partially ordered set.
Rather than duplicating this theorem for each type, Mathlib provides a general
lemma \lstinline[language=lean]|le_trans| that works for any type \lstinline[language=lean]|α|
endowed with a partial ordering.
This is achieved through \textbf{type classes}, Lean's mechanism for defining and working with
abstract algebraic structures in an ad hoc polymorphic manner.
Type classes provide a powerful and flexible way to specify properties and operations that can be
shared across different types, thereby enabling polymorphism and code reuse. Ad hoc polymorphism
arises when a function or operator is defined over several distinct types, with behavior that varies
depending on the type.
A standard example \cite{wadler_blott_ad_hoc_polymorphism_1988} is overloaded multiplication:
the same symbol \lstinline[language=lean]|*| denotes multiplication of integers
(e.g., \lstinline[language=lean]|3 * 3|) and of floating-point numbers
(e.g., \lstinline[language=lean]|3.14 * 3.14|).
By contrast, parametric polymorphism occurs when a function is defined over a range of types
but acts uniformly on each of them. For instance, the \lstinline[language=lean]|List.length|
function applies in the same way to a list of integers and to a list of floating-point numbers.

Under the hood, a type class is a structure.
An important aspect of structures, and hence type classes, is that they support hierarchy
and composition through inheritance. For example, mathematically, a monoid is a semigroup
with an identity element, and a group is a monoid with inverses. In Lean, we can express this
by defining a \lstinline[language=lean]|Monoid| structure that extends the
\lstinline[language=lean]|Semigroup| structure, and a
\lstinline[language=lean]|Group| structure that extends the
\lstinline[language=lean]|Monoid| structure using the
\lstinline[language=lean]|extends| keyword:
\begin{lstlisting}[language=lean]
-- A semigroup has an associative binary operation
structure Semigroup (α : Type*) where
  mul : α → α → α
  mul_assoc : ∀ a b c : α, mul (mul a b) c = mul a (mul b c)

-- A monoid extends semigroup with an identity element  
structure Monoid (α : Type*) extends Semigroup α where
  one : α
  one_mul : ∀ a : α, mul one a = a
  mul_one : ∀ a : α, mul a one = a

-- A group extends monoid with inverses
structure Group (α : Type*) extends Monoid α where
  inv : α → α
  mul_left_inv : ∀ a : α, mul (inv a) a = one
\end{lstlisting}
The symbol \lstinline[language=lean]|*| in \lstinline[language=lean]|(α : Type*)|
indicates a universe variable (we will discuss universes later). Sometimes,
to avoid inconsistencies between types (such as Girard's paradox),
universes must be specified explicitly. This is an example of universe polymorphism.
Thus we have now seen all the polymorphism flavors in Lean: parametric, ad hoc, and universe polymorphism.

Type classes are defined using the \lstinline[language=lean]|class| keyword,
which is syntactic sugar for defining a structure. Thus, the previous example
can be rewritten using type classes:
\newpage
\begin{lstlisting}[language=lean]
-- A semigroup has an associative binary operation
class Semigroup (α : Type*) where
  mul : α → α → α
  mul_assoc : ∀ a b c : α, mul (mul a b) c = mul a (mul b c)

-- A monoid extends semigroup with an identity element  
class Monoid (α : Type*) extends Semigroup α where
  one : α
  one_mul : ∀ a : α, mul one a = a
  mul_one : ∀ a : α, mul a one = a

-- A group extends monoid with inverses
class Group (α : Type*) extends Monoid α where
  inv : α → α
  mul_left_inv : ∀ a : α, mul (inv a) a = one
\end{lstlisting}
The main difference is that type classes support \textbf{instance resolution}.
We use the keyword \lstinline[language=lean]|instance| to declare that a particular type is an
instance of a type class, which inherits the properties and operations defined in the type class.
Instances can be automatically inferred by Lean's type inference system,
allowing for concise and expressive code.
For example, we can declare that \lstinline[language=lean]|ℤ| is a group under addition:
\begin{lstlisting}[language=lean]
instance : Group ℤ where
  mul := Int.add
  one := 0
  inv := Int.neg
  mul_assoc := Int.add_assoc
  one_mul := Int.zero_add
  mul_one := Int.add_zero
  mul_left_inv := Int.neg_add_cancel
\end{lstlisting}
Now, any theorem proven for an arbitrary \lstinline[language=lean]|Group α|
automatically applies to \lstinline[language=lean]|ℤ| without any additional work.
This mechanism is particularly useful for defining and working with order structures
like preorders and partial orders. Mathematically, a preorder consists
of a set \lstinline[language=lean]|P| and a binary relation \lstinline[language=lean]|≤|
on \lstinline[language=lean]|P| that is reflexive and transitive \cite{mathinlean}.
\newpage
\begin{lstlisting}[language=lean, caption=Preorder Type Class in Lean]
-- A preorder is a reflexive, transitive relation `≤` with `<` defined in terms of `≤`
class Preorder (α : Type*) extends LE α, LT α where
  le_refl : ∀ a : α, a ≤ a
  le_trans : ∀ a b c : α, a ≤ b → b ≤ c → a ≤ c
  lt := fun a b => a ≤ b ∧ ¬ b ≤ a
  lt_iff_le_not_ge : ∀ a b : α, a < b ↔ a ≤ b ∧ ¬ b ≤ a := by intros; rfl

instance [Preorder α] : Lean.Grind.Preorder α where
  le_refl := Preorder.le_refl
  le_trans := Preorder.le_trans _ _ _
  lt_iff_le_not_le := Preorder.lt_iff_le_not_ge _ _
\end{lstlisting}

The \lstinline[language=lean]|class Preorder| declares a type class over a type
\lstinline[language=lean]|α|, bundling the \lstinline[language=lean]|≤|
and \lstinline[language=lean]|<| relations
(inherited via \lstinline[language=lean]|extends LE α, LT α|)
with the preorder axioms: reflexivity (\lstinline[language=lean]|le_refl|)
and transitivity (\lstinline[language=lean]|le_trans|).
The field \lstinline[language=lean]|lt| provides a default definition of strict
inequality in terms of \lstinline[language=lean]|≤|,
and the theorem \lstinline[language=lean]|lt_iff_le_not_ge| characterizes this relationship,
proved automatically via reflexivity (\lstinline[language=lean]|by intros; rfl|).
The \lstinline[language=lean]|instance| declaration connects the \lstinline[language=lean]|Preorder|
class to Lean's \lstinline[language=lean]|Grind| tactic automation, which allows automatic
reasoning with preorder properties during proof search.
Returning to our rational number proof, this explains why lemmas
like \lstinline[language=lean]|add_nonneg| and \lstinline[language=lean]|mul_nonneg|
work seamlessly: \lstinline[language=lean]|ℚ| is an instance of
\lstinline[language=lean]|OrderedSemiring|, which extends
\lstinline[language=lean]|Preorder| and other algebraic structures,
automatically providing all their theorems.
(Cocnlude the discussion of less or equal is trnaitive)

An important aspect of type classes is that they can be parameterized by other type classes,
enabling the construction of complex hierarchies of algebraic structures.
Topological spaces in Mathlib are represented using type classes and they can be derived metric spaces and normed spaces.
For example, the \lstinline[language=lean]|TopologicalSpace| type class defines
the structure of a topological space, and it can be extended to define metric spaces
and normed spaces, which are topological spaces with additional structure.
This hierarchical organization allows for the reuse of definitions and theorems across different
mathematical domains, facilitating a modular and scalable approach to formalization.
(Little Intro on THe TOPOLOGY CLass,  Continuos CLosure ecc )

Topology in Lean is built upon the concept of open sets, defined
using the \lstinline[language=lean]|IsOpen| predicate. A topological space is a type
equipped with a collection of open sets satisfying the standard axioms.
In Lean, this is represented using the \lstinline[language=lean]|TopologicalSpace| type class:
\newpage
\begin{lstlisting}[language=lean]
class TopologicalSpace (α : Type*) where
  IsOpen : Set α → Prop
  isOpen_univ : IsOpen univ
  isOpen_inter : ∀ s t, IsOpen s → IsOpen t → IsOpen (s ∩ t)
  isOpen_sUnion : ∀ s, (∀ t ∈ s, IsOpen t) → IsOpen (sUnion s)
\end{lstlisting}

Key topological concepts used in our formalization include:

\begin{description}
  \item[Connectedness:] A space $X$ is connected if it cannot be written as the union of
        two disjoint non-empty open sets. Equivalently, every continuous function from $X$
        to a discrete space is constant. In Lean, this is formalized
        as \lstinline[language=lean]|IsConnected : Set α → Prop|.
  \item[Path-connectedness:] A space $X$ is path-connected if for any two points
        $x, y \in X$, there exists a continuous path $\gamma : [0,1] \to X$
        with $\gamma(0) = x$ and $\gamma(1) = y$. In Lean, the unit interval $[0,1]$
        is represented as the type \lstinline[language=lean]|unitInterval|,
        and path-connectedness is formalized as \lstinline[language=lean]|IsPathConnected : Set α → Prop|.
  \item[Continuous functions:] A function $f : X \to Y$ between topological spaces
        is continuous if the preimage of every open set is open. Lean provides
        \lstinline[language=lean]|Continuous f : Prop|, as well as \lstinline[language=lean]|ContinuousAt|
        and \lstinline[language=lean]|ContinuousOn| for local versions.
  \item[Closure:] The closure of a set $S$ is the smallest closed set containing $S$,
        equivalently the set of all limit points of $S$. In Lean, this is
        \lstinline[language=lean]|closure : Set α → Set α|.
\end{description}


(Maybe include part of the REal construcion / classical noncomputable ecc ecc /)


\section{Formalizing the topologist's sine curve}

As part of my thesis work, with the help and revision from Prof David Loeffler,
I have formalized a well-known counterexample in topology: the \textbf{topologist’s sine curve}.
This classic example illustrates a space that is \textbf{connected} but not \textbf{path-connected}.
It demonstrates the use of type classes, structures, and several tactics in Lean that were discussed earlier.
In my presentation, I will provide a high-level overview along with some specific examples,
of my original code, wich is way longer than the final result.
My original proof follows Conrad's paper (\cite{Conrad_connnotpathconn}), with a few modifications
and some differences from the final
formalization \href{https://leanprover-community.github.io/mathlib4_docs/Counterexamples/TopologistsSineCurve.html}{\textbf{Counterexamples – Topologist's Sine Curve}}.
The topologist's sine curve is defined as the graph of $y = \sin(1/x)$
for $x \in (0, \infty)$,
together with the origin $(0, 0)$.
We define three sets in $\mathbb{R}^2$:
\begin{itemize}
  \item $S$: the oscillating curve $\{(x, \sin(1/x)) : x > 0\}$
  \item $Z$: the singleton set $\{(0, 0)\}$
  \item $T$: their union $S \cup Z$
\end{itemize}
In Lean, this is expressed as follows:
\begin{lstlisting}[language=lean]
  open Real Set
  def pos_real := Ioi (0 : ℝ)
  noncomputable def sine_curve := fun x ↦ (x, sin (x⁻¹))
  def S : Set (ℝ × ℝ) := sine_curve '' pos_real
  def Z : Set (ℝ × ℝ) := { (0, 0) }
  def T : Set (ℝ × ℝ) := S ∪ Z
\end{lstlisting}
We open the \lstinline[language=lean]|Real| and \lstinline[language=lean]|Set| namespaces
to avoid prefixing real number and set operations with \lstinline[language=lean]|Real.|
and \lstinline[language=lean]|Set.|, respectively.
We define the interval $(0, \infty)$ as \lstinline[language=lean]|pos_real|,
using the predefined notation \lstinline[language=lean]|Ioi 0|, from \lstinline[language=lean]|Set|.
The function \lstinline[language=lean]|sine_curve| maps a positive real number
to a point on the topologist's sine curve in $\mathbb{R}^2$.
Here, \lstinline[language=lean]|''| denotes the image of a set under a function.
It's noncomputable because it involves the sine function,
which is not computable in Lean's core logic.
The sets \lstinline[language=lean]|S|, \lstinline[language=lean]|Z|,
and \lstinline[language=lean]|T|
are defined using set operations.
,
and \lstinline[language=lean]|{ (0, 0) }| denotes the singleton
set containing the point $(0, 0)$.
\lstinline[language=lean]|Set| is the type of sets, defined as predicates
(i.e., functions from a type to \lstinline[language=lean]|Prop|).
The sets are subsets of the product space $\mathbb{R}^2$,
represented as \lstinline[language=lean]|ℝ × ℝ|.
The sin function \lstinline[language=lean]|sin| is defined in the
\lstinline[language=lean]|Real|.

The goal is to prove that $T$ is connected but not path-connected.
Let's start with connectedness.
\subsection{$T$ is connected}
First of all one can directlly see that  $S$ is connected, since it is the
image of the set ($(0, \infty)$) under the continuous map
$x \mapsto (x, \sin(1/x))$ and a interval in $\mathbb{R}$ is connected.
Moreover, the closure of $S$ is connected, and every set in between a connected
set and its closure are connected.
Since $T$ is contained in the closure of  $S$, $T$ is connected.
This is how a mathematician would argue informally, using known facts.
However, in a formal proof, one must justify each step.
For instance, justifying that $S$ is connected
requires proving that the map
$x \mapsto (x, \sin(1/x))$ is continuous on $(0, \infty)$
and that $(0, \infty)$ is connected.

As we have seen, even showing that a rational number is non-negative
requires several steps and the use of various lemmas from Mathlib.
Similarly, proving that a set is connected can involve multiple steps
for the
newer programmer.


We can use the structure \lstinline[language=lean]|IsConnected|
to set up the statement and see if we can argue similarly in Lean.
\begin{lstlisting}[language=lean]
lemma S_is_conn : IsConnected S := by sorry
\end{lstlisting}
In the file where \lstinline[language=lean]|IsConnected| is defined,
\texttt{Topology/Connected/Basic.lean}, we see that it requires $S$ to be nonempty and preconnected.
You can verify this by unfolding \lstinline[language=lean]|IsConnected| in the goal.
\newpage
\begin{lstlisting}[language=lean]
lemma S_is_conn : IsConnected S := by
  unfold IsConnected 
  ⊢ S.Nonempty ∧ IsPreconnected S
  sorry
\end{lstlisting}
Following the definition of \lstinline[language=lean]|IsPreconnected|, we see that it captures
the usual definition of preconnectedness: that $S$ cannot be
partitioned into two nonempty disjoint open sets. This trivially requires
nonemptiness to make sense.
The \lstinline[language=lean]|unfold| tactic helps to expand definitions; one can use it to expand the definition of $S$ or
\lstinline[language=lean]|pos_real| defined before, as well as other Mathlib expressions.
Reflecting our argument, we can check if Mathlib includes the fact
that every interval
is connected and that connectedness is preserved
under continuous maps.
Indeed, in \texttt{Topology/Connected/Interval.lean}, we find the theorem
\lstinline[language=lean]|isConnected_Ioi.image|, stating that the image of an
interval of the form $(a, \infty)$
under a continuous map is connected.

\begin{lstlisting}[language=lean]
lemma S_is_conn : IsConnected S := by
  apply isConnected_Ioi.image
  -- ⊢ ContinuousOn sine_curve (Ioi 0) 
  sorry
\end{lstlisting}
The \lstinline[language=lean]|apply| tactic applies the theorem similar to
\lstinline[language=lean]|exact|, the latter tries
to close the goal with \lstinline[language=lean]|rfl|.
The theorem \lstinline[language=lean]|isConnected_Ioi.image| requires proving the continuity of the map
on the interval $(0, \infty)$, which is expressed as
\lstinline[language=lean]|ContinuousOn sine_curve (Ioi 0)|.
The predicate \lstinline[language=lean]|ContinuousOn f S|
expresses that a function $f$ is continuous on a set $S$, which is what we need to prove now.
The function $x \mapsto (x, \sin(1/x))$ is continuous on $(0, \infty)$ as the
product of two functions continuous on the given domain: the identity map $x \mapsto x$
and the map $x \mapsto \sin(1/x)$.
\newpage
Here is the full proof in Lean:
\begin{lstlisting}[language=lean]
lemma inv_is_continuous_on_pos_real : ContinuousOn (fun x : ℝ => x⁻¹) (pos_real) := by
  apply ContinuousOn.inv₀
  · exact continuous_id.continuousOn
  · intro x hx; exact ne_of_gt hx

lemma sin_comp_inv_is_continuous_on_pos_real : ContinuousOn
 (sine_curve) (pos_real) := by
  apply ContinuousOn.prodMk continuous_id.continuousOn
  apply continuous_sin.comp_continuousOn
  exact inv_is_continuous_on_pos_real
\end{lstlisting}
Starting from the bottom lemma, \lstinline[language=lean]|ContinuousOn.prodMk| states that the product of two functions continuous on a set is continuous on that set,
requiring a proof of the continuity of each component.
The first component is the identity map, which is continuous on any set. Mathlib provides
\lstinline[language=lean]|continuous_id.continuousOn| for this purpose.
The second component is the composition of the sine function with the inverse function.
The sine function is continuous everywhere, and for this we can use
\lstinline[language=lean]|continuous_sin|.
The method \lstinline[language=lean]|comp_continuousOn| is accessible from the fact that \lstinline[language=lean]|continuous_sin| gives
an instance of a continuous map and is generalized in the \lstinline[language=lean]|ContinuousOn| module.
The theorem \lstinline[language=lean]|Continuous.comp_continuousOn| states that the composition of a continuous function with a function
that is continuous on a set is continuous on that set, and requires proof of the continuity
on the set of the inner function.
We separate the proof that the inverse function is continuous on the positive reals
into the auxiliary lemma \lstinline[language=lean]|inv_is_continuous_on_pos_real|.
The theorem \lstinline[language=lean]|continuousOn_inv₀| states that if a function
is continuous and non-zero on a set, then its inverse is continuous on that set.
The continuity of the identity map is proved as before.
The second argument requires proving that $x \neq 0$ for all $x$ in $(0, \infty)$.
\begin{lstlisting}[language=lean]
  · intro x hx
    exact ne_of_gt hx
\end{lstlisting}
The hypothesis \lstinline[language=lean]|hx| states that $x$ is in $(0, \infty)$,
which implies that $x > 0$.
The theorem \lstinline[language=lean]|ne_of_gt| states that if a real number is greater than zero,
then it is non-zero, which completes the proof.
Thus the final proof goes as follows:
\begin{lstlisting}[language=lean]
lemma S_is_conn : IsConnected S := by
  apply isConnected_Ioi.image 
  · exact sin_comp_inv_is_continuous_on_pos_real
\end{lstlisting}

When writing a proof, one starts by working out the informal argument on paper.
Then one tries to translate it into Lean, step by step, looking for theorems in Mathlib.
Afterwards, one can try to optimize the proof by removing unnecessary steps or refactoring it.
Proving properties like continuity and connectedness is very common,
and there are obviously ways to achieve this with less work.
Let's showcase a refactoring of the entire proof.
First, the auxiliary lemmas
can be reduced to one-liners.
\newpage
\begin{lstlisting}[language=lean]
lemma inv_is_continuous_on_pos_real : ContinuousOn (fun x : ℝ => x⁻¹) (pos_real) :=
  ContinuousOn.inv₀ (continuous_id.continuousOn) (fun _ hx =>  ne_of_gt hx)
  
lemma sin_comp_inv_is_continuous_on_pos_real : ContinuousOn
 (sine_curve) (pos_real) :=
 ContinuousOn.prodMk continuous_id.continuousOn <|
  Real.continuous_sin.comp_continuousOn <| (inv_is_continuous_on_pos_real)
\end{lstlisting}
We removed the \lstinline[language=lean]|by| keyword since we can provide a \textbf{term}
that directly proves the statement.
In \lstinline[language=lean]|inv_is_continuous_on_pos_real|, we directly apply
\lstinline[language=lean]|ContinuousOn.inv₀| with the two required arguments.
Notice that we can use a lambda function \lstinline[language=lean]|fun _ hx =>  ne_of_gt hx|
to prove that $x \neq 0$ for all $x$ in $(0, \infty)$
(recall the propositions-as-types correspondence).
In the next lemma, we use
the \lstinline[language=lean]|<\|| reverse application operator,
which allows us to avoid parentheses by changing the order of application.
This means that \lstinline[language=lean]|f <| g <| h| is
equivalent to \lstinline[language=lean]|f (g h)|.
% In our case,
% \lstinline[language=lean]|Real.continuous_sin.comp_continuousOn <\||
% \lstinline[language=lean]|(inv_is_continuous_on_pos_real)|
% is equivalent to
% \lstinline[language=lean]|Real.continuous_sin.comp_continuousOn (inv_is_continuous_on_pos_real)|.
We can inline these two lemmas into the main proof to get a final one-liner:
\begin{lstlisting}[language=lean]
lemma S_is_conn : IsConnected S :=
  isConnected_Ioi.image sine_curve <| continuous_id.continuousOn.prodMk <|
    continuous_sin.comp_continuousOn <|
    ContinuousOn.inv₀ continuous_id.continuousOn (fun _ hx => ne_of_gt hx)
\end{lstlisting}
Notice again the use of the \textbf{pipe} operator.
Reading from left to right, we are building up the proof by successive applications:
\begin{itemize}
  \item We start with \lstinline[language=lean]|isConnected_Ioi.image sine_curve|, which states that the image of $(0, \infty)$ under \lstinline[language=lean]|sine_curve| is connected if we can prove the function is continuous.
  \item We then apply \lstinline[language=lean]|continuous_id.continuousOn.prodMk|, which constructs the product of two continuous functions.
  \item Next, \lstinline[language=lean]|continuous_sin.comp_continuousOn| provides the continuity of the sine composition.
  \item Finally, \lstinline[language=lean]|ContinuousOn.inv₀ continuous_id.continuousOn (fun _ hx => ne_of_gt hx)| proves the continuity of the inverse function on positive reals.
\end{itemize}
The entire chain can be read as building the continuity proof from the innermost function (the inverse) outward to the complete sine curve function, which is then used to prove that $S$ is connected.
% The expression \lstinline[language=lean]|continuousOn_inv₀.mono fun _ hx ↦ hx.ne'|
% applies the theorem and provides the required arguments.
% The \lstinline[language=lean]|mono| method allows us to weaken the domain of
% continuity from \lstinline[language=lean]|x : x = 0 |
% to \lstinline[language=lean]|pos_real|,
% which is a subset of the former.
% The lambda function \lstinline[language=lean]|fun _ hx ↦ hx.ne'| proves that $x \neq 0$ for all $x$ in $(0, \infty)$.
% This is a common pattern in Lean, where we often need to prove that certain
% conditions hold for all elements of a set.


% \begin{lstlisting}[language=lean]
% lemma sine_curve_is_continuous_on_pos_real_one_liner : ContinuousOn (sine_curve) (pos_real) :=
%  continuous_id.continuousOn.prodMk <| Real.continuous_sin.comp_continuousOn
%    <| continuousOn_inv₀.mono fun _ hx ↦ hx.ne'
% \end{lstlisting}
Since the intersection of $Z$ and $S$ is empty, we cannot
directly conclude that $T$ is connected from the connectedness of its components alone.
However, we can use the fact that every subset between a connected set and its closure is connected.

\begin{theorem}
  Let $C$ be a connected topological space, and denote $\overline{C}$ as its closure.
  It follows that every subset $C \subseteq S \subseteq \overline{C}$ is connected.
\end{theorem}

In Mathlib, this theorem is available as
\lstinline[language=lean]|IsConnected.subset_closure|.
We can set up the statement and progress from there.
\begin{lstlisting}[language=lean]
theorem T_is_conn : IsConnected T := by
  apply IsConnected.subset_closure
  · exact S_is_conn -- ⊢ IsConnected ?s
  · tauto_set -- ⊢ S ⊆ T
  · sorry -- ⊢ T ⊆ closure S
\end{lstlisting}

The theorem requires three goals:
\begin{enumerate}
  \item That $S$ is connected, which was already proved in \lstinline[language=lean]|S_is_conn|.
  \item That $S \subseteq T$, which is a trivial set operation.
        The tactic \lstinline[language=lean]|tauto_set| handles this kind of set tautologies.
  \item That $T \subseteq \overline{S}$ (the closure of $S$), which requires proof.
\end{enumerate}
Let's continue with the final point.
\begin{lstlisting}[language=lean]
lemma T_sub_cls_S : T ⊆ closure S := by
  intro x hx
  cases hx with
  | inl hxS => exact subset_closure hxS
  | inr hxZ =>
      sorry
\end{lstlisting}
Proving that one set is contained in another can be done naively in a pointwise manner.
We introduce an element $x \in \mathbb{R}^2$ together with the proof that $x \in T$.
Since $T$ is a union, we use \lstinline[language=lean]|cases| to separate the two cases.
When $x \in S$, the goal is trivially solved by \lstinline[language=lean]|exact subset_closure hxS|.
The case where $x \in Z$, requires more work.

Now a trick. One of the most painful issue in fomralizing math in Lean is the
use of existing theorems. One can use several ways to look for the exact theorem.
Let's try using the \lstinline[language=lean]|apply?|
tactic to see what the infoview suggests:
\begin{lstlisting}[language=lean]
lemma T_sub_cls_S : T ⊆ closure S := by
  intro x hx
  cases hx with
  | inl hxS => exact subset_closure hxS
  | inr hxZ =>
      apply?
      sorry
\end{lstlisting}
Depending on the previous work in the file, Lean can already unify the goal with available theorems and suggest the next step.
Similar tactics include:
\begin{itemize}
  \item \lstinline[language=lean]|exact?| for finding an exact match to close the goal
  \item \lstinline[language=lean]|rw?| for suggesting rewrites and definitionally equal replacements
  \item \lstinline[language=lean]|simp?| for suggesting simplifications
\end{itemize}
Another useful tool is Loogle (similar to Haskell's Hoogle),
which helps you find theorems by their type signature or name patterns.
You can access it at \url{https://loogle.lean-lang.org/} or
use it directly in VS Code.
The best approach is, anyway, to think first about how you would tackle the problem on
a piece of paper, as mentioned earlier.
Since we are working with a topology on $\mathbb{R}$,
we know that
this is a metrizable topology, therefore it is induced by the metric
space structure on $\mathbb{R}$.
Thus, we can expand our toolkit by working with Lean's
\lstinline[language=lean]|MetricSpace| module,
which provides specialized tools for reasoning about
metric spaces, such as balls, distances, and
metric-specific characterizations of continuity and convergence.
We know that the closure of a set contains all its limit points.
To show that the point $(0, 0)$ is contained in the closure of $S$,
we need to show that it is a limit point of $S$.
Thus, one can define a sequence in $S$ tending to $(0, 0)$, and we are done.
At this point, \lstinline[language=lean]|apply?| suggests several ways to proceed, involving new symbols such as
\lstinline[language=lean]|𝓝| (neighborhoods) and
\lstinline[language=lean]|ᶠ| (eventually/frequently). For example:
\begin{lstlisting}
Try this: refine Frequently.mem_closure ?_
Remaining subgoals:
  ⊢ ∃ᶠ (x : ℝ × ℝ) in 𝓝 x, x ∈ S
\end{lstlisting}
or the more familiar metric space approach:
\begin{lstlisting}
Try this: refine Metric.mem_closure_iff.mpr ?_
Remaining subgoals:
  ⊢ ∀ ε > 0, ∃ b ∈ S, dist x b < ε
\end{lstlisting}
While we could work with metric space properties directly using the
familiar $\varepsilon$-$\delta$ formulation,
we instead introduce and explain filters. This approach may seem more
abstract initially, but it provides
a more general and powerful framework that works uniformly
across all topological spaces, not just metric spaces.
Moreover, once understood, filters often make proofs more
concise and elegant by allowing us to reason
at a higher level of abstraction.
\subsubsection{Limits and Convergence with Filters}
As seen before, continuity is defined in the \lstinline[language=lean]|Topology| type class.
One can create their own epsilon-delta definition given a notion of distance.
Similarly, defining the convergence of a sequence in $\mathbb{R}$ can be done manually as follows:
\begin{lstlisting}[language=lean]
def ConvergesTo (ε : ℝ > 0)(s : ℕ → ℝ) (a : ℝ) :=
  ∀ ε > 0, ∃ N, ∀ n ≥ N, |s n - a| < ε
\end{lstlisting}
Where \lstinline[language=lean]|s| is a sequence and \lstinline[language=lean]|a| is the limit point.
However, there are many other types of limits to consider, like lmits of a function at a point,
limits at infinity (from above or below), one-sided limits (from the left or right) and so on.
Defining each of these separately would require a huge amount of work to include in Mathlib,
with significant duplication of theorems and proofs. Moreover, many fundamental theorems
(like the characterization of continuity via limits) would need to be reproved for each type of limit.
Fortunately, Bourbaki solved this issue by introducing the notion of \textbf{filters} to unify
all concepts of limits, convergence, neighborhoods and terms like eventually or
frequently often into a single framework.
Mathlib adopts this notion to achieve an elegant solution that fully
covers the entire landscape of limit-related concepts.
Intuitively, a filter represents a notion of "sufficiently large" subsets.
More fomrally, a filter $F$ on a type $X$ is a collection of subsets of $X$ satisfying three axioms:
\begin{enumerate}
  \item \textbf{Non-emptiness:} $X \in F$ (the whole space is in the filter)
  \item \textbf{Upward closure:} If $U \in F$ and $U \subseteq V$, then $V \in F$
        (supersets of "large" sets are "large")
  \item \textbf{Intersection closure:} If $U, V \in F$, then $U \cap V \in F$
        (finite intersections of "large" sets are "large")
\end{enumerate}
We are going to use some of the following concetps:
\begin{itemize}
  \item \textbf{At top filter} \lstinline[language=lean]|atTop : Filter ℕ|: Contains sets that
        include all sufficiently large natural numbers.
        Formally, $U \in$ \lstinline[language=lean]|atTop| if and only if there
        exists $N$ such that $\{n \mid n \geq N\} \subseteq U$.
        This captures the idea of "$n \to \infty$."
  \item \textbf{Neighborhood filter} \lstinline[language=lean]|𝓝 x|: In a topological space,
        this filter contains all neighborhoods of the point $x$.
        A set is in \lstinline[language=lean]|𝓝 x| if it contains an
        open set containing $x$. This captures the idea of "near $x$."
        % \item \textbf{At bot filter} \lstinline[language=lean]|atBot|: The dual of \lstinline[language=lean]|atTop|, containing sets that include all sufficiently small elements (for $n \to -\infty$).
        % \item \textbf{Within filter} \lstinline[language=lean]|𝓝[S] x|: The neighborhood filter of $x$ restricted to a set $S$, useful for one-sided limits and limits on subspaces.
  \item \lstinline[language=lean]|∀ᶠ x in f, p x| (\lstinline[language=lean]|f.Eventually p|):
        "Eventually in filter $f$, property $p$ holds."
        This means there exists some set $U \in f$ such that $p$ holds for all $x \in U$.
        For example, \lstinline[language=lean]|∀ᶠ n in atTop, n > 100| means
        "for all sufficiently large $n$, we have $n > 100$."
  \item \lstinline[language=lean]|∃ᶠ x in f, p x| (\lstinline[language=lean]|f.Frequently p|):
        "Frequently in filter $f$, property $p$ holds." This means for every set $U \in f$, there exists
        some $x \in U$ where $p$ holds. This captures the idea that $p$ holds "infinitely often" or
        "arbitrarily close."
        For example, \lstinline[language=lean]|∃ᶠ n in atTop, Even n| means
        "there are arbitrarily large even numbers."
  \item \lstinline[language=lean]|Tendsto f l₁ l₂|: "Function $f$ tends from filter
        $l_1$ to filter $l_2$." This is used for convergence.
        % Formally, this means that for every set $U \in l_2$, the
        % preimage $f^{-1}(U) \in l_1$.
\end{itemize}
\begin{example}
  Here some examples of the use of filter in Lean.
  \begin{itemize}
    \item Sequence convergence: \lstinline[language=lean]|Tendsto s atTop (𝓝 a)| means $s_n \to a$ as $n \to \infty$
    \item Function limits: \lstinline[language=lean]|Tendsto f (𝓝 x) (𝓝 y)| means $f(x') \to y$ as $x' \to x$
    \item Continuity: A function is continuous at $x$ iff \lstinline[language=lean]|Tendsto f (𝓝 x) (𝓝 (f x))|
          (does this remember the continuity in terms of open sets?)
          % \item Limits at infinity: \lstinline[language=lean]|Tendsto f atTop (𝓝 y)| means $f(x) \to y$ as $x \to \infty$
          % \item One-sided limits: \lstinline[language=lean]|Tendsto f (𝓝[>] x) (𝓝 y)| means $f$ approaches $y$ from the right of $x$
  \end{itemize}
\end{example}
Using filters, we can prove that $T \subseteq \overline{S}$ by
showing that the origin is a
limit point of $S$.
We construct a sequence $f : \mathbb{N} \to \mathbb{R}^2$ in $S$
converging to $(0,0)$ using the
\lstinline[language=lean]|Tendsto| framework:
\newpage
\begin{lstlisting}[language=lean]
lemma T_sub_cls_S : T ⊆ closure S := by
  intro x hx
  cases hx with
  | inl hxS => exact subset_closure hxS
  | inr hxZ =>
      rw [hxZ]
      -- Define sequence: f(n) = (1/(nπ), 0)
      let f : ℕ → ℝ × ℝ := fun n => ((n * Real.pi)⁻¹, 0)
      -- Show f converges to (0, 0)
      have hf : Tendsto f atTop (𝓝 (0, 0)) := by
        refine Tendsto.prodMk_nhds ?_ tendsto_const_nhds
        exact tendsto_inv_atTop_zero.comp
          (Tendsto.atTop_mul_const' Real.pi_pos tendsto_natCast_atTop_atTop)
      -- Show f eventually takes values in S
      have hf' : ∀ᶠ n in atTop, f n ∈ S := by
        filter_upwards [eventually_gt_atTop 0] with n hn
        exact ⟨(n * Real.pi)⁻¹,
          inv_pos.mpr (mul_pos (Nat.cast_pos.mpr hn) Real.pi_pos),
          by simp [f, sine_curve, inv_inv, Real.sin_nat_mul_pi]⟩
      -- Apply sequential characterization of closure
      exact mem_closure_of_tendsto hf hf'
\end{lstlisting}
The proof is already reduced as much as possible.
Let's break down what's happening in without getting into details.
Using \lstinline[language=lean]|let|, we define
$f(n) = \left(\frac{1}{n\pi}, 0\right)$,
which we will show converges to $(0,0)$
and stays in $S$.

\begin{enumerate}

  \item \textbf{Convergence proof} (\lstinline[language=lean]|hf|):
        We show \lstinline[language=lean]|Tendsto f atTop (𝓝 (0, 0))|.
        \begin{itemize}
          \item We use \lstinline[language=lean]|Tendsto.prodMk_nhds| to split the product:
                we need to show the first coordinate tends to $0$ and the
                second is constantly $0$.
          \item For the first coordinate, we compose
                \lstinline[language=lean]|tendsto_inv_atTop_zero|
                (which states $\frac{1}{x} \to 0$ as $x \to \infty$) with the
                fact that $n\pi \to \infty$.
          \item The second constant coordinate is handled by
                \lstinline[language=lean]|tendsto_const_nhds|.
        \end{itemize}
  \item \textbf{Membership proof} (\lstinline[language=lean]|hf'|):
        We show \lstinline[language=lean]|∀ᶠ n in atTop, f n ∈ S|, meaning $f(n) \in S$
        for all sufficiently large $n$.
        \begin{itemize}
          \item We use \lstinline[language=lean]|filter_upwards|, which allows us to combine
                hypotheses about properties that hold eventually to prove another property holds eventually.
                Here, we combine it with \lstinline[language=lean]|eventually_gt_atTop 0|,
                which states that eventually $n > 0$.
          \item For such $n$, we show $f(n) = \left(\frac{1}{n\pi}, 0\right)$ is in $S$ by noting that
                the second term is:
                $$
                  \sin\left(\frac{1}{\left(\frac{1}{n\pi}\right)}\right) = \sin(n\pi) = 0.
                $$
        \end{itemize}

\end{enumerate}
Finally, \lstinline[language=lean]|mem_closure_of_tendsto|
combines these facts:
if a sequence eventually stays in $S$ and converges to $x$,
then $x$ is in the closure of $S$.
% (ALTERNATIVE PROOF)

% \begin{proof}
%   To show that $S$ lies in the closure of $S^+$, we have to express each $p \in S$ as a limit of a
%   sequence of points in $S^+$. If $p \in S^+$, we use the constant sequence $\{p, p, \ldots\}$. If $p = (0, y)$ with
%   $|y| \leq 1$, we argue as follows. Certainly $y = \sin(\theta)$ for some $\theta \in [-\pi, \pi]$, whence $y = \sin(\theta + 2n\pi)$
%   for all positive integers $n$. Thus, for $x_n = 1/(\theta + 2n\pi) > 0$ we have $\sin(1/x_n) = y$ for all $n$. Since
%   $x_n \to 0$ as $n \to \infty$, we have $(x_n, \sin(1/x_n)) = (x_n, y) \to (0, y)$.
% \end{proof}
\subsubsection{Finalising the first part of the proof}
If you are a one-liner enthusiast like me, you don't mind trying to combine
bits and pieces to get a clean final result.
We can simplify the final theorem as follows initially:
\begin{lstlisting}[language=lean]
theorem T_is_conn : IsConnected T := 
  IsConnected.subset_closure S_is_conn (by tauto_set) T_sub_cls_S
\end{lstlisting}
The second argument is still in tactic mode with \lstinline[language=lean]|by tauto_set|,
but it looks clean and we can keep it as is.
With a bit of courage, we can also inline the proof of \lstinline[language=lean]|S_is_conn|
(while \lstinline[language=lean]|T_sub_cls_S| is way too long to inline)
to get a more self-contained one-liner:
\begin{lstlisting}[language=lean]
theorem T_is_conn : IsConnected T :=
  IsConnected.subset_closure (isConnected_Ioi.image sine_curve <|
    continuous_id.continuousOn.prodMk <|
    Real.continuous_sin.comp_continuousOn <|
    ContinuousOn.inv₀ continuous_id.continuousOn
    (fun _ hx => ne_of_gt hx)) (by tauto_set) T_sub_cls_S
\end{lstlisting}
Making these amendments is not only for the sake of shortening the proof.
Lean will, obviously, compile the proof faster by not
entering tactic mode or using multiple tactics.
Tactics internally hide many operations they automatically perform to close the goal.
Moreover, if we directly provide
a term for the proof, Lean will infer and unify everything by definitional
equality; remember the very first example we did.
By providing explicit proof terms, we give Lean less work to do, making the
proof more transparent and efficient.
This practice of "golfing" is essential in a huge library such as Mathlib
community that needs to balance performance and maintainability.
From now on the rest of th code will be presented in it's reduced form.
Here is the link to the entire first part of the proof:
[\href{https://live.lean-lang.org//#codez=JYWwDg9gTgLgBAWQIYwBYBtgCMBQEwCmAdnACr4ToQDmAnnAGLDowFRwDKB8A8lACZscOQQDM4kAM4B9KASTo4ALgC8nbgDoAkhGBwAFAAZlcQLiEAShxEIRAMYRwAVxhIs6AnDFxJwIgWm2jlAAbh6qcKKOJAAecIBlhAbRADRwAEry6Bo+JPrRgN4EAJ3mll4cJlzw+qZwAOtm5spq+tn+gSEEDQDknRIQMnIKIgTiAFrl3AbVdRaNcADeBoYphg0AvkPipOOVU/WzZYBURHAjwu4gIEhk0pKOWAHoMmVKZHCAYURwtlQ3cpyzWLQ4OBwXwwKAQOCxVDRQEfJCSAiSOBQuAAd2AaBhAB9gURFFCyioAHxwAjRJC2eA3LDwmD3PpBDz4rE49hQsZEmFAoFQFFwADabIAupyue54OJnoBUQjggCTCMy1eXhSIkEhEgz6EgAKjSGQ0YGA5kKy0sXK5qCQoSRErIxH4khg4PEKHIYAMgFYNwC7O4tjQ1wv8Raa5KJfB49WD+AgANbSIioO1wAD80jgrCIdodARs9pjcckAa5pPJ8FT6Yg0l8wWkzvw0gAXmwIBp7OB86b9EwWGwNKRbfbG9WwNIQI50JmiPaeukFHrgNIpCnexmiCgAMJw2kDqswF0m01Ii2M0Q9Z6AACJAAW4cBIvjgA5S4hIgAgiX5+gF7rnBztQaSOMAopACRE+QIUIiBgRwFHQWhpGoDdt3wOBDEFVF0VQS8kSIVsSTJCk4EAC/INTgbUp0yfVDQKJJMKBCs5z6DRwHYfRh1Hed9AAORQJt1xoyQ6LAVkiAaYiZ248wKLfU1/m8UBXT5UQUhaAIglCFJqIrFIhOyGMUCHEc52AQVAEvyfNCxwkACBAOlvn8CBRGkEs+ytK1OmENACGgcyrmAGR7CIEhni0SQVxsPwKQIfgXlUGEAqC3yCFC/gsluGlLIZAwvJikLWH4aQdGADRQCQagPAUtpLQAHkxEUfJgXxHAgRwZGABLqtq+rJB4IgwwgCNIzgCqRSElqiDqhrrl8JsHEHIaRvakh+tNGKauGtqOvyohgkAAIIPhsJaZvLZqdtahqOpFfRlTgZNkTVPxpBs6D4ChYoDEklxnDLGkGlIa5bnuR4gA}{link to Lean live}]
\begin{note}
  The proof merged into the Mathlib library takes $Z$ as $\{0\} \times [-1,1]$
  instead of the singleton $\{(0,0)\}$.
  This, together with the fact that $T$ equals the closure of $S$,
  yields a stronger and more general result.
  The Mathlib version demonstrates that the entire vertical segment at $x = 0$
  lies in the closure of the oscillating curve, providing a more complete characterization
  of the topologist's sine curve. This stronger version shows that a closed set
  (specifically, the closure of $S$) can be connected but not path-connected.
  Showing that forming closure can destroy
  the property of path connectedness for subsets of a topological space.
\end{note}
\subsection{$T$ is not path-connected}
The main and most substantial part is showing that $T$ is not path-connected.
Showing this informally already requires constructing and
pointing out various steps in order to convince
an ideal reader.
One can argue informally by contradiction:
suppose a path exists in the topologist's sine curve $T$
connecting a point in $S$ to a point in $Z$.
As the path approaches the $y$-axis (where $x \to 0$),
the $y$-coordinate must oscillate infinitely between $-1$ and $1$
due to the behavior of $\sin(1/x)$ as $x \to 0^+$.
This infinite oscillation contradicts the continuity of the path,
which is a fundamental requirement for path-connectedness.
To be more precise, we need to construct a sequence
that it eventually oscillates, establishing the contradiction.
We start by setting up the theorem:
\begin{lstlisting}[language=lean]
theorem T_is_not_path_conn : ¬ (IsPathConnected T) := 
  by sorry
\end{lstlisting}
In mathematics, we normally define a path-connected space as follows.
\begin{definition}
  A topological space $X$ is said to be path-connected if for every two points $a, b \in X$, there
  exists a path, i.e., a continuous map $p : [0, 1] \to X$ such that $p(0) = a$ and $p(1) = b$.
\end{definition}
The interval $[0, 1]$ is the standard choice for the domain of paths.
In Mathlib, \lstinline[language=lean]|PathConnectedSpace X| is a type class that asserts the entire
topological space $X$ is path-connected, while
\lstinline[language=lean]|IsPathConnected S| is a predicate used to infer that a subset $S$
of a topological space is path-connected.
\begin{lstlisting}[language=lean]
def IsPathConnected (F : Set X) : Prop :=
  ∃ x ∈ F, ∀ ⦃y⦄, y ∈ F → JoinedIn F x y
\end{lstlisting}
The auxiliary predicate \lstinline[language=lean]|JoinedIn| is defined as:
\begin{lstlisting}[language=lean]
def JoinedIn (S : Set X) (x y : X) : Prop :=
  ∃ γ : Path x y, ∀ t, γ t ∈ S
\end{lstlisting}
where \lstinline[language=lean]|Path x y| denotes a continuous map $\gamma : [0,1] \to X$ with
$\gamma(0) = x$ and $\gamma(1) = y$.
Mathlib uses \lstinline[language=lean]|unitInterval| as the standard definition for
$[0,1]$ in constructions such as the definition of a path.
\newpage
Now let's start with the first part of the proof:
\begin{lstlisting}[language=lean]
theorem T_is_not_path_conn : ¬ (IsPathConnected T) := by
  -- Assume we have a path from z = (0, 0) to w = (1, sin(1))
  have hz : z ∈ T := Or.inr rfl
  have hw : w ∈ T := Or.inl ⟨1, ⟨zero_lt_one' ℝ, rfl⟩⟩
  intro p_conn
  apply IsPathConnected.joinedIn at p_conn
  specialize p_conn z hz w hw
  let p := JoinedIn.somePath p_conn
\end{lstlisting}
We introduce two points: $z = (0, 0)$ and $w = (1, \sin(1))$, and prove they are both in $T$.
Using \lstinline[language=lean]|intro p_conn|, we assume that $T$ is path-connected.
Notice that the goal is now \lstinline[language=lean]|False|, meaning we must find a contradiction.
The last three lines extract an explicit path \lstinline[language=lean]|p| connecting $z$ and $w$:
\begin{itemize}
  \item \lstinline[language=lean]|apply IsPathConnected.joinedIn at p_conn|
        transforms the path-connectedness assumption into the statement
        that any two points in $T$ are joined.
  \item \lstinline[language=lean]|specialize p_conn z hz w hw| specializes this
        to our specific points $z$ and $w$.
  \item \lstinline[language=lean]|let p := JoinedIn.somePath p_conn| extracts a concrete
        path from the existential statement.
\end{itemize}
Conrad's paper (\cite{Conrad_connnotpathconn}) defines a time $t_0 \in [0, 1]$
as the first time the path $p$ jumps from $(0,0)$ to the graph of $\sin(1/x)$, where
the x-coordinate map ($x: \mathbb{R}^2 \to \mathbb{R} $) of $p$ is positive.
$$
  t_0 = \inf\{t \in [0, 1] : x(p(t)) > 0\}
$$
The argument then uses the continuity of the $x$-coordinate map composed with the path $p$.
By continuity at $t_0$, we can find a neighborhood around $t_0$
where the path stays close to $(0,0)$. Specifically, with $\varepsilon = 1/2$,
there exists $\delta > 0$ such that for all $t$ with $|t - t_0| < \delta$,
we have $\|p(t) - p(t_0)\| < 1/2$.
We want to show the oscillating behavior around (0,0) indeed.
To simplify some steps, we instead define
$$
  t_0 = \sup\{t \in [0, 1] : x(p(t)) = 0\}
$$
to be the last time the path remains at $(0,0)$.
The same continuity argument applies with this definition.
\newpage
\begin{lstlisting}[language=lean]
-- Consider the composition of the x-coordinate map with p, which is continuous
have xcoord_pathcont : Continuous fun t ↦ (p t).1 := continuous_fst.comp p.continuous
-- Let t₀ be the last time the path is on the y-axis
let t₀ : unitInterval := sSup {t | (p t).1 = 0}
let xcoord_path := fun t => (p t).1
-- The x-coordinate of the path at t₀ is 0
have hpt₀_x : (p t₀).1 = 0 :=
  (isClosed_singleton.preimage xcoord_pathcont).sSup_mem ⟨0, by aesop⟩
-- By continuity of the path, we can find a δ > 0 such that
-- for all t in [t₀ - δ, t₀ + δ], ||p(t) - p(t₀)|| < 1/2
-- Hence the path stays in a ball of radius 1/2 around (0, 0)
obtain ⟨δ, hδ, ht⟩ : ∃ δ > 0, ∀ t, dist t t₀ < δ →
  dist (p t) (p t₀) < 1/2 :=
  Metric.eventually_nhds_iff.mp <| Metric.tendsto_nhds.mp (p.continuousAt t₀) _ one_half_pos
\end{lstlisting}
The final statement uses the \lstinline[language=lean]|obtain| tactic to extract witnesses from an existential statement.
This tactic destructures the existential quantifier $\exists \delta > 0, \ldots$ into concrete values:
\lstinline[language=lean]|δ| (the distance), \lstinline[language=lean]|hδ| (the proof that $\delta > 0$),
and \lstinline[language=lean]|ht| (the proof that the distance condition holds).
Since $\mathbb{R}^2$ is a metric space, we can work with the distance function \lstinline[language=lean]|dist : ℝ × ℝ → ℝ × ℝ → ℝ|,
which computes the Euclidean distance between two points.
The statement \lstinline[language=lean]|dist t t₀ < δ| expresses $|t - t_0| < \delta$ in the unit interval,
while \lstinline[language=lean]|dist (p t) (p t₀) < 1/2| expresses $\|p(t) - p(t_0)\| < 1/2$ in $\mathbb{R}^2$.
The proof itself leverages \lstinline[language=lean]|Metric| module.
\begin{itemize}
  \item \lstinline[language=lean]|p.continuousAt t₀| asserts that the path $p$ is
        continuous at $t_0$
  \item \lstinline[language=lean]|Metric.tendsto_nhds.mp| converts this
        to the metric space characterization:
        for any $\varepsilon > 0$, there exists $\delta > 0$ such that
        points within $\delta$ of $t_0$ map to points within $\varepsilon$ of $p(t_0)$
  \item \lstinline[language=lean]|Metric.eventually_nhds_iff.mp| further unpacks
        this into the $\forall t, dist\ t\ t_0 < \delta \to dist\ (p\ t)\ (p\ t_0) < \varepsilon$
        form
  \item We instantiate with $\varepsilon = 1/2$ using \lstinline[language=lean]|one_half_pos|
\end{itemize}
We can find a time $t_1$ greater than $t_0$ that remains in the neighborhood of $t_0$,
and obtain a point $a = x(p(t_1))) > 0$ which is positive.
\newpage
\begin{lstlisting}[language=lean]
-- Let t₁ be a time when the path is not on the y-axis
-- t₁ is in (t₀, t₀ + δ], hence t₁ > t₀
obtain ⟨t₁, ht₁⟩ : ∃ t₁, t₁ > t₀ ∧ dist t₀ t₁ < δ := by
  let s₀ := (t₀ : ℝ) -- cast t₀ from unitInterval to ℝ for manipulation
  let s₁ := min (s₀ + δ/2) 1
  have hs₀_delta_pos : 0 ≤ s₀ + δ/2 := add_nonneg t₀.2.1 (by positivity)
  have hs₁ : 0 ≤ s₁ := le_min hs₀_delta_pos zero_le_one
  have hs₁': s₁ ≤ 1 := min_le_right ..
  sorry
-- Let a = xcoord_path t₁ > 0
-- This follows from the definition of t₀ and t₀ < t₁
-- so t₁ must be in S, which has positive x-coordinate
let a := (p t₁).1
have ha : a > 0 := by
  obtain ⟨x, hxI, hx_eq⟩ : p t₁ ∈ S := by
    cases p_conn.somePath_mem t₁ with
    | inl hS => exact hS
    | inr hZ =>
      -- If p t₁ ∈ Z, then (p t₁).1 = 0
      have : (p t₁).1 = 0 := by rw [hZ]
      -- So t₁ ≤ t₀, contradicting t₁ > t₀
      have hle : t₁ ≤ t₀ := le_sSup this
      have hle_real : (t₁ : ℝ) ≤ (t₀ : ℝ) := Subtype.coe_le_coe.mpr hle
      have hgt_real : (t₁ : ℝ) > (t₀ : ℝ) := Subtype.coe_lt_coe.mpr ht₁.1
      linarith
  simpa only [a, ← hx_eq] using hxI
\end{lstlisting}
The code is quite convoluted in Lean, and i will omit a
detailed explanation as well as some part of it.
However, it's worth mentioning a few key technical points.
The type \lstinline[language=lean]|unitInterval| is a \textbf{subtype}
of $\mathbb{R}$,
defined as $\{x : \mathbb{R} \mid 0 \leq x \leq 1\}$.
In Lean, a subtype \lstinline[language=lean]|{x : α // P x}|
bundles a value $x$ of type $\alpha$
together with a proof that $x$ satisfies the predicate $P$.
Manipulating terms of \lstinline[language=lean]|unitInterval|
directly is challenging because
this type lacks many algebraic operations such as addition, minimum, etc.
Therefore, we cast to $\mathbb{R}$ (with \lstinline[language=lean]|let s₀ := (t₀ : ℝ)|)
to perform arithmetic operations, then cast back to \lstinline[language=lean]|unitInterval|
by providing proofs that the bounds $[0, 1]$ are satisfied (\lstinline[language=lean]|hs₁|, \lstinline[language=lean]|hs₁'|).
In the second case of the inner statment of have \lstinline[language=lean]|ha : a > 0|,
if $p(t_1) \in Z$, then $(p\ t_1).1 = 0$ by definition of $Z = \{(0,0)\}$.
This implies $t_1 \leq t_0$ by the definition of $t_0$ as the supremum.
However, we also have $t_1 > t_0$ from our construction of $t_1$ (\lstinline[language=lean]|ht₁.1|).
The tactic \lstinline[language=lean]|linarith|, an automated solver for linear arithmetic,
recognizes this contradiction by observing both
\lstinline[language=lean]|hle_real : (t₁ : ℝ) ≤ (t₀ : ℝ)| and
\lstinline[language=lean]|hgt_real : (t₁ : ℝ) > (t₀ : ℝ)|.
Since these statements are contradictory, \lstinline[language=lean]|linarith|
proves \lstinline[language=lean]|False|.
Lemmas like \lstinline[language=lean]|Subtype.coe_lt_coe|
allow us to transfer inequalities between the subtype and its underlying type,
needed for \lstinline[language=lean]|linarith|.

Finally, \lstinline[language=lean]|simpa only [a, ← hx_eq] using hxI| completes the proof.
The tactic \lstinline[language=lean]|simpa| combines simplification (\lstinline[language=lean]|simp|)
with assumption matching. The directive \lstinline[language=lean]|only [a, ← hx_eq]|
unfolds the definition of $a = (p\ t_1).1$ and rewrites using \lstinline[language=lean]|hx_eq|
in the reverse direction, transforming the goal from \lstinline[language=lean]|(p t₁).1 > 0|
to \lstinline[language=lean]|(sine_curve x).1 > 0|.
Since \lstinline[language=lean]|sine_curve x = (x, sin(1/x))|, this simplifies to \lstinline[language=lean]|x > 0|,
which is exactly the hypothesis \lstinline[language=lean]|hxI|.
The \lstinline[language=lean]|using hxI| clause applies this hypothesis to close the goal.

Next, the image $x(p([t_0, t_1]))$ is connected (as the continuous image of a connected set),
and it contains $0 = x(p(t_0))$ and $a = x(p(t_1))$.
Since every connected subset of $\mathbb{R}$ is an interval, we have
$$
  [0, a] \subseteq x(p([t_0, t_1]))
$$
This will be crucial for the next step, where we show that the path must oscillate.
\begin{lstlisting}[language=lean]
  -- The image x(p([t₀, t₁])) is connected and contains 0 and a
  -- Therefore [0, a] ⊆ x(p([t₀, t₁]))
  have Icc_of_a_b_sub_Icc_t₀_t₁: Set.Icc 0 a ⊆ xcoord_path '' Set.Icc t₀ t₁ :=
     IsConnected.Icc_subset
      ((isConnected_Icc (le_of_lt ht₁.1)).image _ xcoord_pathcont.continuousOn)
      (⟨t₀, left_mem_Icc.mpr (le_of_lt ht₁.1), hpt₀_x⟩)
      (⟨t₁, right_mem_Icc.mpr (le_of_lt ht₁.1), rfl⟩)
\end{lstlisting}
Now we construct a sequence that demonstrates the contradiction.
Recall that $\sin(\theta) = 1$ if and only if $\theta = \frac{(4k + 1)\pi}{2}$ for some $k \in \mathbb{Z}$.
Therefore, $(x, \sin(1/x)) = (x, 1)$ when
$$
  x = \frac{2}{(4k + 1)\pi}
$$
for $k \in \mathbb{N}$. As $k \to \infty$, these $x$-values approach 0,
so infinitely many of them lie in any interval $[0, a]$.
We define this sequence and establish its key properties:
\begin{lstlisting}[language=lean]
noncomputable def xs_pos_peak := fun (k : ℕ) => 2/((4 * k + 1) * Real.pi)
lemma xs_pos_peak_tendsto_zero : Tendsto xs_pos_peak atTop (𝓝 0) := sorry
lemma xs_pos_peak_nonneg : ∀ k : ℕ, 0 ≤ xs_pos_peak k := sorry
lemma sin_xs_pos_peak_eq_one (k : ℕ) : Real.sin ((xs_pos_peak k)⁻¹) = 1 := sorry
\end{lstlisting}
The crucial property is that this sequence eventually enters $[0, a]$:
\begin{lstlisting}[language=lean]
-- For any k ∈ ℕ, sin(1/xs_pos_peak(k)) = 1
-- Since xs_pos_peak converges to 0 as k → ∞,
-- there exist indices i ≥ 1 for which xs_pos_peak i ∈ [0, a]
have xpos_has_terms_in_Icc_of_a_b : ∃ i : ℕ, i ≥ 1 ∧ xs_pos_peak i ∈ Set.Icc 0 a := sorry
\end{lstlisting}
This gives us points on the topologist's sine curve with $y$-coordinate equal to $1$,
lying arbitrarily close to the $y$-axis.

Now we can establish the final contradiction.
Since $[0, a] \subseteq x(p([t_0, t_1]))$ by the previous argument,
and $\text{xs\_pos\_peak}(i) \in [0, a]$ for some $i$,
there must exist some $t' \in [t_0, t_1]$ such that $x(p(t')) = \text{xs\_pos\_peak}(i)$.
This means $p(t') = (\text{xs\_pos\_peak}(i), \sin(1/\text{xs\_pos\_peak}(i))) = (\text{xs\_pos\_peak}(i), 1)$,
so the $y$-coordinate of $p(t')$ equals $1$.
However, since $t' \in [t_0, t_1] \subseteq [t_0, t_0 + \delta)$,
we have $\text{dist}(t', t_0) < \delta$, which by our earlier continuity argument implies
$\|p(t') - p(t_0)\| < 1/2$.
But $\|p(t') - (0,0)\| \geq |(p(t')).2| = |1| = 1 > 1/2$,
yielding a contradiction.
\begin{lstlisting}[language=lean]
-- Show there exists time t' in [t₀, t₁] ⊆ [t₀, t₀ + δ) such that p(t') = (*, 1)
obtain ⟨t', ht', hpath_t'⟩ : ∃ t' ∈ Set.Icc t₀ t₁, (p t').2 = 1 := sorry
-- Derive the final contradiction using t', ht', hpath_t'
-- First show that p t₀ = (0, 0)
have hpt₀ : p t₀ = (0, 0) := sorry
-- t' is within δ of t₀ (since t' ∈ [t₀, t₁] and dist t₀ t₁ < δ)
have t'_close : dist t' t₀ < δ := by
  calc dist t' t₀
      ≤ dist t₁ t₀ := dist_right_le_of_mem_uIcc (Icc_subset_uIcc' ht')
    _ = dist t₀ t₁ := dist_comm _ _
    _ < δ := ht₁.2
-- By continuity, p(t') should be close to p(t₀)
have close : dist (p t') (p t₀) < 1/2 := ht t' t'_close
-- But p(t') has y-coordinate 1, so it's actually far from p(t₀) = (0, 0)
have far : 1 ≤ dist (p t') (p t₀) := by
  calc 1 = |(p t').2 - (p t₀).2| := by simp [hpath_t', hpt₀]
      _ ≤ ‖p t' - p t₀‖ := norm_ge_abs_snd
      _ = dist (p t') (p t₀) := by rw [dist_eq_norm]
-- This is a contradiction: 1 ≤ dist (p t') (p t₀) < 1/2
linarith
\end{lstlisting}
% The proof proceeds by deriving two contradictory bounds on $\text{dist}(p(t'), p(t_0))$:

% \begin{enumerate}
% \item \textbf{Extracting the critical time:} The first \lstinline[language=lean]|obtain| extracts a time $t' \in [t_0, t_1]$ where $(p\ t')_2 = 1$. This gives us three components:
% \begin{itemize}
%   \item \lstinline[language=lean]|t' : unitInterval| — the time value
%   \item \lstinline[language=lean]|ht' : t' ∈ Set.Icc t₀ t₁| — proof that $t' \in [t_0, t_1]$
%   \item \lstinline[language=lean]|hpath_t' : (p t').2 = 1| — proof that the $y$-coordinate is $1$
% \end{itemize}

% \item \textbf{Establishing the base point:} We prove \lstinline[language=lean]|hpt₀ : p t₀ = (0, 0)|, confirming that the path is at the origin at time $t_0$.

% \item \textbf{Showing proximity in time:} The statement \lstinline[language=lean]|t'_close| proves that $\text{dist}(t', t_0) < \delta$. The proof uses a \lstinline[language=lean]|calc| chain:
% \begin{itemize}
%   \item First, since $t' \in [t_0, t_1]$, we have $\text{dist}(t', t_0) \leq \text{dist}(t_1, t_0)$ (the distance from $t'$ to $t_0$ is at most the distance from $t_1$ to $t_0$)
%   \item By symmetry of distance, $\text{dist}(t_1, t_0) = \text{dist}(t_0, t_1)$
%   \item From our earlier work, $\text{dist}(t_0, t_1) < \delta$
% \end{itemize}

% \item \textbf{Upper bound from continuity:} The statement \lstinline[language=lean]|close| applies our earlier continuity result: since $\text{dist}(t', t_0) < \delta$, we have $\text{dist}(p(t'), p(t_0)) < 1/2$.

% \item \textbf{Lower bound from geometry:} The statement \lstinline[language=lean]|far| proves that $1 \leq \text{dist}(p(t'), p(t_0))$. The \lstinline[language=lean]|calc| chain shows:
% \begin{itemize}
%   \item $1 = |(p\ t')_2 - (p\ t_0)_2|$ by substituting $(p\ t')_2 = 1$ and $(p\ t_0)_2 = 0$
%   \item $|(p\ t')_2 - (p\ t_0)_2| \leq \|p(t') - p(t_0)\|$ by the fact that the norm dominates the absolute value of any component (\lstinline[language=lean]|norm_ge_abs_snd|)
%   \item $\|p(t') - p(t_0)\| = \text{dist}(p(t'), p(t_0))$ by the definition of distance in a normed space
% \end{itemize}
% This completes the proof by contradiction, showing that $T$ is not path-connected.
\subsection{$T$ is connected not path-connected}
Finally, we combine the two parts in the following concise and pleasant theorem:
\begin{lstlisting}[language=lean]
theorem T_is_conn_not_pathconn : IsConnected T ∧ ¬IsPathConnected T :=
  ⟨T_is_conn, T_is_not_path_conn⟩
\end{lstlisting}
And now, since this code compiles successfully, these two lines stand as verified witnesses
to the correctness of our entire proof.
This showcases the power of proof assistants and formal reasoning:
mathematics becomes not only more rigorous but also automatically verifiable.
Furthermore, the formalization becomes a learning tool in its own right.
Future readers can inspect each part of the code.
Here the full proof: [\href{https://live.lean-lang.org/#codez=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}{link to Lean live}]

