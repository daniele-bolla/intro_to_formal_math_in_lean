\section{Logic and Proposition as Types}
\subsection{First Order Logic}
Logic is the study of reasoning, branching into various systems.
We refer to \textbf{classical logic} as the one that underpins much
of traditional mathematics.
It's the logic of the ancient Greeks (not fair) and truthtables, and it remains
used nowadays for pedagogical reasons.
We first introduce \textbf{propositional logic}, which is the simplest
form of classical logic.
Later we will extend this to \textbf{predicate (or first-order) logic}, which includes
\textbf{predicates} and \textbf{quantifiers}.
In this setting, a \textbf{proposition} is a statement that is either true or false,
and a \textbf{proof} is a logical argument that establishes the truth of a
proposition.
Propositions can be combined with logical \textbf{connectives} such as ``and'' ($\wedge$),
``or'' ($\vee$), ``not'' ($\neg$),``false'' ($\bot$), ,``true'' ($\top$) ``implies'' ($\Rightarrow$),  and ``if and only if'' ($\Leftrightarrow$).
These connectives allow the creation of complex or compound propositions.
\newpage
Here how connectives are defined in Lean:
\begin{example}[LogicaL connectives in Lean]\mbox{}
  \begin{lstlisting}[language=lean]
    #check And (a b : Prop) : Prop
    #check Or (a b : Prop) : Prop
    #check True : Prop
    #check False : Prop
    #check Not (a : Prop) : Prop
    #check Iff (a b : Prop) : Prop
  \end{lstlisting}
  \lstinline[language=lean]|Prop| stands for proposition, and it is an
  essential component of Lean’s type system.
  For now, we can think of it as a special type whose
  inhabitants are proofs; somewhat
  paradoxically, a type of types.
\end{example}
Logic is often formalized through a framework known as the \textbf{natural deduction system},
developed by Gentzen in the 1930s (\cite{wadler2015propositions}).
This approach brings logic closer to a computable, algorithmic system.
It specifies rules for deriving
\textbf{conclusions} from \textbf{premises} (assumptions from other propositions),
called \textbf{inference rules}.
\begin{example}[Deductive style rule]
  Here is an hypothetical example of inference rule.
  \begin{prooftree}
    \AxiomC{$P_1$}
    \AxiomC{$P_2$}
    \AxiomC{$\cdots$}
    \AxiomC{$P_n$}
    \QuaternaryInfC{$C$}
  \end{prooftree}
  Where the $P_1, P_2, \ldots, P_n$, above the line, are hypothetical premises and, the hypothetical conclusion $C$ is below the line.
\end{example}
The inference rules needed are:
\begin{itemize}
  \item \textbf{Introduction rules} specify how to form compound propositions from simpler ones, and
  \item \textbf{Elimination rules} specify how to use compound propositions to derive information about their components.
\end{itemize}
Let's look at how we can define some connectives first using natural deduction.
\paragraph{Conjunction ($\land$)}
\paragraph{Introduction Rule}
\begin{prooftree}
  \AxiomC{$A$}
  \AxiomC{$B$}
  \RightLabel{$\land$-Intro}
  \BinaryInfC{$A \land B$}
\end{prooftree}
\paragraph{Elimination Rule}
\mbox{}\\[0.5em]
\noindent
\begin{minipage}[t]{0.48\textwidth}
  \vspace{0pt}
  \begin{prooftree}
    \AxiomC{$A \land B$}
    \RightLabel{$\land$-Elim$_1$}
    \UnaryInfC{$A$}
  \end{prooftree}
\end{minipage}\hfill
\begin{minipage}[t]{0.48\textwidth}
  \vspace{0pt}
  \begin{prooftree}
    \AxiomC{$A \land B$}
    \RightLabel{$\land$-Elim$_2$}
    \UnaryInfC{$B$}
  \end{prooftree}
\end{minipage}
\paragraph{Disjunction ($\lor$)}
\paragraph{Introduction Rule}
\mbox{}\\[0.5em]
\noindent
\begin{minipage}[t]{0.48\textwidth}
  \vspace{0pt}
  \begin{prooftree}
    \AxiomC{$A$}
    \RightLabel{$\lor$-Intro$_1$}
    \UnaryInfC{$A \lor B$}
  \end{prooftree}
\end{minipage}\hfill
\begin{minipage}[t]{0.48\textwidth}
  \vspace{0pt}
  \begin{prooftree}
    \AxiomC{$B$}
    \RightLabel{$\lor$-Intro$_2$}
    \UnaryInfC{$A \lor B$}
  \end{prooftree}
\end{minipage}
\paragraph{ Elimination (Proof by cases)}
\begin{prooftree}
  \AxiomC{$A \lor B$}
  \AxiomC{$[A] \vdash C$}
  \AxiomC{$[B] \vdash C$}
  \RightLabel{$\lor$-Elim}
  \TrinaryInfC{$C$}
\end{prooftree}
\paragraph{Implication ($\to$)}
\paragraph{Introduction Rule}
\begin{prooftree}
  \AxiomC{$[A] \vdash B$}
  \RightLabel{$\to$-Intro}
  \UnaryInfC{$A \to B$}
\end{prooftree}
\paragraph{ Elimination (Modus Ponens)}
\begin{prooftree}
  \AxiomC{$A \to B$}
  \AxiomC{$A$}
  \RightLabel{$\to$-Elim}
  \BinaryInfC{$B$}
\end{prooftree}
\begin{notation}
  We use $A \vdash B$ (called turnstile) to designate a
  deduction of $B$ from $A$.
  It is employed in Gentzen’s \textbf{sequent calculus}
  (\cite{girard1989proofs})
  and moslty used in type theory.
  The square brackets around a premise $[A]$ mean that the premise $A$ is meant to
  be \textbf{discharged} at the conclusion. The classical example is the
  introduction rule for the implication connective.
  To prove an implication $A \to B$, we assume $A$
  (shown as $[A]$), derive $B$ under this assumption, and then discharge the
  assumption $A$ to conclude that $A \to B$ holds without the assumption.
  The turnstile is predominantly used in judgments and type theory with
  the meaning of ``entails that''.
\end{notation}
\subsection{Primitive Types}
Type theory employs this porocedure too,
by referring to deduction
rules as \textbf{judments}.
A type judgment has the form $\Gamma \vdash t : T$,
meaning: under \textbf{context} $\Gamma$ (a list of typed variables),
the term $t$ has type $T$.
Using formal inference rules in the type judgment
system, such as \textbf{introduction} and \textbf{elimination} rules,
we can construct new compound types from existing ones.
\begin{example}[Judgment style rule]
  \mbox{}
  \begin{prooftree}
    \AxiomC{$\Gamma \vdash$}
    \AxiomC{$p_1:P_1$}
    \AxiomC{$p_2:P_2$}
    \AxiomC{$\cdots$}
    \AxiomC{$P_n$}
    \QuinaryInfC{$C$}
  \end{prooftree}
\end{example}
Technically, there are two more inference rules that we will not consider in this setting:
\textbf{formation rules}, used to declare that a type is well-formed, and
\textbf{computation rules}, which specify how a term will be evaluated.
Moreover, without going too deep into the jargon,
one specific judgment is
$\Gamma \vdash A \equiv B\ \text{type}$, which means ``types $A$ and $B$ are
\textbf{judgmentally (or definitionally) equal} in context $\Gamma$.''
Similarly for terms, $\Gamma \vdash t_1 \equiv t_2 : A$ means ``terms $t_1$ and $t_2$ are
judgmentally equal of type $A$ in context $\Gamma$.''
In Lean, the operator \lstinline[language=lean]|:=|
stands for definitional equality and is used by the kernel to verify proof equality.

Let's now construct new types from given types $A$ and $B$.
\paragraph{Product Type}
As a fundamental example, $A \times B$
denotes the type of pairs $(a, b)$ where $a : A$ and $b : B$,
called the \textbf{product type}.
\paragraph{Introduction Rule (pairing)}
\begin{prooftree}
  \AxiomC{$a : A$}
  \AxiomC{$b : B$}
  \BinaryInfC{$(a, b) : A \times B$}
\end{prooftree}

\noindent In Lean:
\begin{lstlisting}[language=lean]
Prod.mk a b : Prod A B   -- or A × B
(a, b) : A × B           
⟨a, b⟩ : A × B           
\end{lstlisting}
\paragraph{Elimination Rules (projections)}\mbox{}\\[0.5em]
\noindent
\begin{minipage}[t]{0.48\textwidth}
  \vspace{0pt}
  \begin{prooftree}
    \AxiomC{$p : A \times B$}
    \UnaryInfC{$\mathsf{fst}(p) : A$}
  \end{prooftree}
\end{minipage}\hfill
\begin{minipage}[t]{0.48\textwidth}
  \vspace{0pt}
  \begin{prooftree}
    \AxiomC{$p : A \times B$}
    \UnaryInfC{$\mathsf{snd}(p) : B$}
  \end{prooftree}
\end{minipage}

\noindent In Lean:
\begin{lstlisting}[language=lean]
  p.1 : A       -- or Prod.fst p
  p.2 : B       -- or Prod.snd p
\end{lstlisting}
\paragraph{Sum Type}
The \textbf{sum type} $A + B$ (also called a coproduct or disjoint union) consists of values that are
either of type $A$ (tagged with $\mathsf{inl}$) or
of type $B$ (tagged with $\mathsf{inr}$).
\paragraph{Introduction Rules (injections)}
\mbox{}\\[0.5em]
\noindent
\begin{minipage}[t]{0.48\textwidth}
  \vspace{0pt}
  \begin{prooftree}
    \AxiomC{$a : A$}
    \UnaryInfC{$\mathsf{inl}(a) : A + B$}
  \end{prooftree}
\end{minipage}\hfill
\begin{minipage}[t]{0.48\textwidth}
  \vspace{0pt}
  \begin{prooftree}
    \AxiomC{$b : B$}
    \UnaryInfC{$\mathsf{inr}(b) : A + B$}
  \end{prooftree}
\end{minipage}

\noindent In Lean:
\begin{lstlisting}[language=lean]
Sum.inl a : Sum A B   -- or A ⊕ B
Sum.inr b : Sum A B
\end{lstlisting}
\paragraph{Elimination Rule (case analysis)}
\begin{prooftree}
  \AxiomC{$p : A + B$}
  \AxiomC{$\begin{array}{c}  f : (A \implies C) \end{array}$}
  \AxiomC{$\begin{array}{c}  g : (B \implies C) \end{array}$}
  \TrinaryInfC{$\mathsf{cases}(p, f, g) : C$}
\end{prooftree}
In Lean, we can use the \lstinline[language=lean]|cases|:
\begin{lstlisting}[language=lean]
example (p : Sum A B) (f : A → C) (g : B → C) : C := by
  cases p with
  | inl x => f x
  | inr y => g y
\end{lstlisting}
\paragraph{Function Types}
The type of the form $A \to B$, used in the sum elimination rule
represents functions from $A$ to $B$.
\paragraph{Introduction Rule (lambda abstraction)}
\begin{prooftree}
  \AxiomC{$\begin{array}{c} x : A  \vdash  \Phi : B \end{array}$}
  \UnaryInfC{$\lambda x.\Phi : A \to B$}
\end{prooftree}
In Lean, lambda abstraction is written using \lstinline[language=lean]|fun| or \lstinline[language=lean]|λ|:
\begin{lstlisting}[language=lean]
fun (x : A) => Φ : A → B
-- or using λ notation
λ (x : A) => Φ : A → B
-- Example: identity function
def id : A → A := fun x => x
-- or
def id : A → A := λ x => x
\end{lstlisting}
\paragraph{Elimination Rule (application)}
\begin{prooftree}
  \AxiomC{$f : A \to B$}
  \AxiomC{$a : A$}
  \BinaryInfC{$f(a) : B$}
\end{prooftree}
In Lean, function application is written using juxtaposition:
\begin{lstlisting}[language=lean]
example (f : A → B) (a : A) : B := f a
\end{lstlisting}
Functions are a primitive concept in type theory,
and we provide a brief introduction here.
We can \textbf{apply} a function $f : A \to B$
to an element $a : A$ to obtain an element of $B$,
denoted $f(a)$. In type theory, it is common to omit
the parentheses and write the application simply
as $f\, a$.

There are two equivalent ways to construct function
types: either by direct definition or by using
$\lambda$-abstraction.
Introducing a function by definition means that
we introduce a function by giving it a name (let's say, $f$) and saying we define $f : A \to B$ by giving an equation
\begin{equation} \label{lambda_definition}
  f(x) \coloneqq \Phi
\end{equation}
where $x$ is a variable and $\Phi$ is an expression
which may use $x$. In order for this to be valid,
we have to check that $\Phi : B$ assuming $x : A$.
Now we can compute $f(a)$ by replacing the variable $x$ in $\Phi$ with $a$. As an example,
consider the function $f : \mathbb{N} \to \mathbb{N}$ which
is defined by $f(x) \coloneqq x + x$. Then $f(2)$ is \textbf{definitionally equal} to $2 + 2$.
If we don't want to introduce a name for the
function, we can use \textbf{$\lambda$-abstraction}.
Given an expression $\Phi$ of type $B$ which
may use $x : A$, as above, we write $\lambda(x : A). \Phi$ to
indicate the same function defined by
(\ref{lambda_definition}). Thus, we have
$$ (\lambda(x : A). \Phi) : A \to B. $$
% \begin{example}
%   The previously defined function  has the typing
%   judgment
%   $$ (\lambda(x : \mathbb{N}). x + x) : \mathbb{N} \to \mathbb{N}. $$
%   As another example, for any types $A$ and $B$
%   and any element $y : B$, we have a
%   \textbf{constant function}
%   $$ (\lambda(x : A). y) : A \to B. $$
%   The \textbf{identity function} on any
%   type $A$ is given by
%   $$ (\lambda(x : A). x) : A \to A. $$
% \end{example}
By convention, the ``scope'' of the variable
binding ``$\lambda x.$''
is the entire rest of the expression,
unless delimited with parentheses.
Thus, for instance, $\lambda x. x + x$
should be parsed as $\lambda x.(x + x)$,
not as $(\lambda x. x) + x$ .
Now a $\lambda$-abstraction is a function,
so we can apply it to an argument $a : A$.
We then have the following computation
rule ($\beta$-reduction), which is a
\textbf{definitional equality}:
$$ (\lambda x. \Phi)(a) \equiv \Phi' $$
where $\Phi'$ is the expression $\Phi$ in
which all occurrences of $x$ have been
replaced by $a$.
Continuing the above example, we have
$(\lambda x. x + x)(2) \equiv 2 + 2. $
% Note that from any function $f : A \to B$,
% we can construct a lambda abstraction
% function $\lambda x. f(x)$.
% Since this is by definition ``the function
% that applies $f$ to its argument'' we consider
% it to be definitionally
% equal to $f$ (\textbf{$\eta$-conversion}):
% $$ f \equiv (\lambda x. f(x)). $$
% This equality is the uniqueness principle
% for function types, because it shows that $f$
% is uniquely determined by its values.
% The introduction of functions by definitions
% with explicit parameters can be reduced to
% simple definitions by using $\lambda$-abstraction:
% i.e., we can read a definition of $f : A \to B$ by
% $$ f(x) \coloneqq \Phi
% $$
% as
% $$ f \coloneqq \lambda x. \Phi. $$
When performing calculations involving variables, we must carefully preserve the \textbf{binding structure} of expressions during substitution. Consider the function $f : \mathbb{N} \to (\mathbb{N} \to \mathbb{N})$ defined as:
$$ f(x) \coloneqq \lambda y. x + y $$
Suppose we have assumed $y : \mathbb{N}$ somewhere in our context. What is $f(y)$?
A naive approach would replace $x$ with $y$ directly in the expression $\lambda y. x + y$, yielding $\lambda y. y + y$. However, this substitution is \textbf{semantically incorrect} because it causes \textbf{variable capture}: the free variable $y$ (referring to our assumption) becomes bound by the $\lambda$-abstraction, fundamentally altering the expression's meaning.
The correct approach uses \textbf{$\alpha$-conversion} (variable renaming).
Since bound variables have only local scope, we can consistently rename them while preserving binding structure.
The expression $\lambda y. x + y$ is judgmentally equal to $\lambda z. x + z$ for any fresh variable $z$. Therefore:
$$ f(y) \equiv \lambda z. y + z $$
This phenomenon parallels familiar mathematical practice: if $f(x) \coloneqq \int_1^2 \frac{dt}{x-t}$,
then $f(t)$ equals $\int_1^2 \frac{ds}{t-s}$, not the ill-defined $\int_1^2 \frac{dt}{t-t}$.
Lambda abstractions bind dummy variables exactly as integrals do.
For functions of multiple variables, we employ \textbf{currying} (named after mathematician Haskell Curry). Instead of using product types, we represent a two-argument function as a function returning another function.
A function taking inputs $a : A$ and $b : B$ to produce output in $C$ has type:
$$ f : A \to (B \to C) \equiv A \to B \to C $$
where the arrow associates to the right by convention.
Given $a : A$ and $b : B$, we apply $f$ sequentially: first to $a$, then the result to $b$, obtaining $f(a)(b) : C$.
To simplify notation and avoid excessive parentheses, we adopt several conventions. We write $f(a)(b)$ as $f(a, b)$ for abbreviated application. Without parentheses entirely, $f \, a \, b$ means $(f \, a) \, b$ following left-associative application. For multi-parameter definitions, we write $f(x, y) \coloneqq \Phi$ where $\Phi : C$ under assumptions $x : A$ and $y : B$.
Using $\lambda$-abstraction, such definitions correspond to:
$$ f \coloneqq \lambda x. \lambda y. \Phi $$
Alternative notation using map symbols:
$$ f \coloneqq x \mapsto y \mapsto \Phi $$
This currying approach extends naturally to functions of three or more arguments, allowing us to represent any multi-argument function as a sequence of single-argument functions.
\begin{example}\mbox{}
  \begin{lstlisting}[language=lean]
def add : Nat -> (Nat -> Nat) := fun x => (fun y => x + y)
#eval add 3 4   -- Output: 7
\end{lstlisting}
  Theoretically, lambda evaluation proceeds in steps:
  \begin{align*}
    \text{add } 3\, 4 & \equiv (\text{add } 3)\, 4                         \\
                      & \equiv ((\lambda x.\, \lambda y.\, x + y)\, 3)\, 4 \\
                      & \equiv ((\lambda y.\, x + y)[x := 3])\, 4          \\
                      & \equiv (\lambda y.\, 3 + y)\, 4                    \\
                      & \equiv (3 + y)[y := 4]                             \\
                      & \equiv 3 + 4                                       \\
                      & \equiv 7
  \end{align*}
\end{example}
% \paragraph{Computation Rules}
% \[
%   \mathsf{match}\ \mathsf{inl}(a) \ \mathsf{with} \ \dots \equiv f(a)
%   \qquad
%   \mathsf{match}\ \mathsf{inr}(b) \ \mathsf{with} \ \dots \equiv g(b)
% \]

% \section{Judgments and Propositions}

% Logic is often formalized through a framework that distinguishes clearly between
% \textbf{judgments} and \textbf{propositions}, following Martin-Löf's foundational approach
% (\cite{martin-lof-1983}, \cite{plato:intuitionistic-type-theory}).
% A \textbf{judgment} represents something we may know — an object of knowledge that becomes
% evident once we have a proof of it.

% The most fundamental form of judgment in logic is ``\textit{A is true}'', where \( A \) is a
% proposition. This is formally written as \( A\ \text{true} \).  
% When we derive such judgments under assumptions, we write
% $$
% \Gamma \vdash A\ \text{true},
% $$
% where \( \Gamma \) represents our hypothetical assumptions
% (\cite{pfenning-natded}).

% \begin{example}[Judgment-Based Inference Rules]
% All logical reasoning can be expressed through \textbf{introduction} and
% \textbf{elimination} rules for judgments.
% For instance, conjunction is characterized as follows.

% \textbf{Introduction:} how to establish the judgment \( A \land B\ \text{true} \):

% \begin{prooftree}
%   \AxiomC{\(A\ \text{true}\)}
%   \AxiomC{\(B\ \text{true}\)}
%   \RightLabel{\(\land\text{-I}\)}
%   \BinaryInfC{\(A \land B\ \text{true}\)}
% \end{prooftree}

% \textbf{Elimination:} how to use the judgment \( A \land B\ \text{true} \):

% \begin{minipage}[t]{0.45\textwidth}
% \begin{prooftree}
%   \AxiomC{\(A \land B\ \text{true}\)}
%   \RightLabel{\(\land\text{-E}_1\)}
%   \UnaryInfC{\(A\ \text{true}\)}
% \end{prooftree}
% \end{minipage}\hfill
% \begin{minipage}[t]{0.45\textwidth}
% \begin{prooftree}
%   \AxiomC{\(A \land B\ \text{true}\)}
%   \RightLabel{\(\land\text{-E}_2\)}
%   \UnaryInfC{\(B\ \text{true}\)}
% \end{prooftree}
% \end{minipage}
% \end{example}

% \begin{notation}[The Turnstile Symbol]
% The symbol \( \vdash \) (the \textit{turnstile}) separates assumptions from conclusions
% in judgments.  
% $$
% \Gamma \vdash J
% $$
% means that the judgment \( J \) follows from the assumptions \( \Gamma \), or equivalently,
% that \( J \) is evident given evidence for \( \Gamma \).

% When assumptions are \textit{discharged} during reasoning, we indicate this with square
% brackets \([A]\).
% For example, to establish an implication \( A \rightarrow B \), we assume \( A \)
% (written \([A]\)), derive \( B \) under this assumption, then discharge \( A \):

% \begin{prooftree}
%   \AxiomC{\([A\ \text{true}]^{u}\)}
%   \AxiomC{\(\vdots\)}
%   \AxiomC{\(B\ \text{true}\)}
%   \RightLabel{\(\rightarrow\text{-I}^{u}\)}
%   \TrinaryInfC{\(A \rightarrow B\ \text{true}\)}
% \end{prooftree}
% \end{notation}

% This judgment-based framework extends naturally to \textbf{type theory}, where additional
% judgment forms are introduced.  
% While logic focuses on the judgment \( A\ \text{true} \),
% type theory introduces several fundamental forms 
% (\cite{plato:intuitionistic-type-theory}):

% \begin{align}
%   &\Gamma \vdash A\ \text{type}
%     && \text{(\(A\) is a well-formed type)} \notag \\
%   &\Gamma \vdash t : A
%     && \text{(\(t\) is a term of type \(A\))} \notag \\
%   &\Gamma \vdash A \equiv B\ \text{type}
%     && \text{(types \(A\) and \(B\) are judgmentally equal)} \notag \\
%   &\Gamma \vdash t_1 \equiv t_2 : A
%     && \text{(terms \(t_1\) and \(t_2\) are judgmentally equal)} \notag \\
% \end{align}

% The \textbf{context} \( \Gamma \) represents a list of assumptions about variables and
% their types:
% $$
% \Gamma = x_1 : A_1,\, x_2 : A_2,\, \ldots,\, x_n : A_n.
% $$

% \begin{example}[Unified Introduction/Elimination Pattern]
% Both logical connectives and type constructors follow the same
% \textit{introduction/elimination} pattern.
% For function types (corresponding to implication):

% \textbf{Formation:}
% \begin{prooftree}
%   \AxiomC{\(\Gamma \vdash A\ \text{type}\)}
%   \AxiomC{\(\Gamma \vdash B\ \text{type}\)}
%   \BinaryInfC{\(\Gamma \vdash A \rightarrow B\ \text{type}\)}
% \end{prooftree}

% \textbf{Introduction:}
% \begin{prooftree}
%   \AxiomC{\(\Gamma, x : A \vdash b : B\)}
%   \RightLabel{\(\rightarrow\text{-I}\)}
%   \UnaryInfC{\(\Gamma \vdash \lambda x : A.\, b : A \rightarrow B\)}
% \end{prooftree}

% \textbf{Elimination:}
% \begin{prooftree}
%   \AxiomC{\(\Gamma \vdash f : A \rightarrow B\)}
%   \AxiomC{\(\Gamma \vdash a : A\)}
%   \RightLabel{\(\rightarrow\text{-E}\)}
%   \BinaryInfC{\(\Gamma \vdash f(a) : B\)}
% \end{prooftree}

% This correspondence reveals a deep connection:
% logical implication and function types share the same structural rules,
% differing only in focus — whether on truth (\(A \rightarrow B\ \text{true}\))
% or on typing (\(\lambda x.\,b : A \rightarrow B\)).
% \end{example}

% \subsection*{References}
% Per Martin-Löf, \textit{Intuitionistic Type Theory}, 1983–1984. \\
% P. Dybjer, \textit{Intuitionistic Type Theory}, \textit{Stanford Encyclopedia of Philosophy}, 2016. \\
% Frank Pfenning, \textit{Logical Frameworks and Natural Deduction}, lecture notes. \\
% Additional formal presentations and lecture notes on type theory and natural deduction, CMU (various).


\subsection{Curry Howard isomorphism}
We have been preparing for this argument, and the reader will have surely
noticed a strong similarity when defining logical connectives
using deduction rules; they are remarkably similar to types
constructed using type judgments. For instance, function
types can be seen as implications.
This is not a coincidence, but rather a fundamental theorem
first proven by Haskell Curry and William Howard.
It forms the core of modern type theory and establishes
a deep connection between logic, computation, and mathematics.
The isomorphism states:
\begin{align*}
  \text{Propositions}        & \leftrightarrow \text{Types}              \\
  \text{Proofs}              & \leftrightarrow \text{Programs}           \\
  \text{Proof Normalization} & \leftrightarrow \text{Program Evaluation}
\end{align*}
\noindent\textbf{Implication} ($P \Rightarrow Q$) corresponds to the \textbf{function type} ($P \to Q$).
A proof of an implication is a function that transforms any proof
of the premise into a proof of the conclusion.
\noindent\textbf{Conjunction} ($P \land Q$) corresponds
to the \textbf{product type} ($P \times Q$).
A proof of a conjunction consists of a pair containing proofs of both conjuncts.
\noindent\textbf{Disjunction} ($P \lor Q$) corresponds
to the \textbf{sum type} ($P + Q$).
A proof of a disjunction is either a proof of the
first disjunct or a proof of the second disjunct.
Lean uses inference rules and type
judgments as well as computing connectives using each related type.
For instance, $A \land B$ can be represented as \lstinline[language=lean]|And(A, B)| or \lstinline[language=lean]|A ∧ B|.
Its introduction rule is constructed by
\lstinline[language=lean]|And.intro _ _| or simply
\lstinline[language=lean]|⟨_, _⟩| (underscores are placeholders).
The pair $A \land B$ can then be consumed using elimination
rules \lstinline[language=lean]|And.left| and \lstinline[language=lean]|And.right|.

\begin{example}\label{ex:conj_intro_2}
  Let's look at a simple Lean example:
  \begin{lstlisting}[language=lean]
    example (ha : a) (hb : b) : (a ∧ b) := And.intro ha hb
  \end{lstlisting}
  This means: given a proof of $a$ ( \lstinline[language=lean]|ha|)
  and a proof of $b$  ( \lstinline[language=lean]|hb|) ,
  we can form a proof of $(a \land b)$.
  \lstinline[language=lean]|And.intro| is implemented as:
  \begin{lstlisting}[language=lean]
    And.intro : p -> q -> (p ∧ q)
  \end{lstlisting}
  It says: if you give me a proof of $p$ and a proof of $q$,
  then I return a proof of $p \land q$.
  We therefore conclude the proof by directly giving
  \lstinline[language=lean]|And.intro ha hb|.
  Here is another way of writing the same statement:
  \begin{lstlisting}[language=lean]
    example (ha : a) (hb : b) : And(a, b) := ⟨ha, hb⟩
  \end{lstlisting}
\end{example}
For a more concrete example, let's look at how
proof normalization using a system of inference rules
corresponds to computation in Lean.
To reduce complexity of a \textbf{proof tree} in natural deduction,
one follows a
\textbf{top-down} approach,
unfolding each component to be proved step by step.
\begin{example}[Associativity of Conjunction]
  We prove that $(A \land B) \land C$ implies $A \land (B \land C)$.
  First, from the assumption $(A \land B) \land C$, we can derive $A$:
  \begin{prooftree}
    \AxiomC{$(A \land B) \land C$}
    \RightLabel{$\land E_1$}
    \UnaryInfC{$A \land B$}
    \RightLabel{$\land E_1$}
    \UnaryInfC{$A$}
  \end{prooftree}
  Second, we can derive $B \land C$:
  \begin{prooftree}
    \AxiomC{$(A \land B) \land C$}
    \RightLabel{$\land E_1$}
    \UnaryInfC{$A \land B$}
    \RightLabel{$\land E_2$}
    \UnaryInfC{$B$}
    \AxiomC{$(A \land B) \land C$}
    \RightLabel{$\land E_2$}
    \UnaryInfC{$C$}
    \RightLabel{$\land I$}
    \BinaryInfC{$B \land C$}
  \end{prooftree}
  Finally, combining these derivations we obtain $A \land (B \land C)$:
  \begin{prooftree}
    \AxiomC{$(A \land B) \land C \vdash A$}
    \AxiomC{$(A \land B) \land C \vdash B \land C$}
    \RightLabel{$\land I$}
    \BinaryInfC{$A \land (B \land C)$}
  \end{prooftree}
\end{example}
\begin{example}[Lean Implementation]
  Let us now implement the same proof in Lean.
  \newpage
  \begin{lstlisting}[language=lean]
theorem and_associative (a b c : Prop) : (a ∧ b) ∧ c → a ∧ (b ∧ c) :=
  fun h : (a ∧ b) ∧ c →
  -- First, from the assumption (a ∧ b) ∧ c, we can derive a:
  have hab : a ∧ b := h.left
  have ha : a := hab.left 
  -- Second, we can derive b ∧ c (here we only extract b and c and combine them in the next step)
  have hc : c := h.right
  have hb : b := hab.right
  -- Finally, combining these derivations we obtain a ∧ (b ∧ c)
  show a ∧ (b ∧ c) from ⟨ha, ⟨hb, hc⟩⟩
\end{lstlisting}
  We introduce the \lstinline[language=lean]|theorem| with the name
  \lstinline[language=lean]|and_associative|.
  The type signature \lstinline[language=lean]|(a ∧ b) ∧ c → a ∧ (b ∧ c)|
  represents our logical implication.
  Here, we construct the implication proof using a
  function (following the Curry Howard isomorphism) with the \lstinline[language=lean]|fun| keyword.
  The \lstinline[language=lean]|have| keyword introduces local
  lemmas within our proof scope, allowing us to break down complex
  reasoning into manageable intermediate steps,
  mirroring our natural deduction proof from before.
  Just before the keyword \lstinline[language=lean]|show|,
  the info view displays the following
  context and goal:
  \begin{lstlisting}[language=lean]
  a b c : Prop
  h : (a ∧ b) ∧ c
  hab : a ∧ b
  ha : a
  hc : c
  hb : b
  ⊢ a ∧ b ∧ c
\end{lstlisting}
  Finally, the \lstinline[language=lean]|show| keyword explicitly states what
  we are proving and verifies that our provided term has the correct type.
  In this case, \lstinline[language=lean]|show a ∧ (b ∧ c) from ⟨ha, ⟨hb, hc⟩⟩|
  asserts that we are constructing a proof of \lstinline[language=lean]|a ∧ (b ∧ c)|
  using the term \lstinline[language=lean]|⟨ha, ⟨hb, hc⟩⟩|.
  The \lstinline[language=lean]|show| keyword serves two
  purposes:
  it makes the proof more readable by explicitly
  documenting what is being proved at this step,
  and it performs a type check to ensure the provided
  proof term matches the stated
  goal up to \textbf{definitional equality}.
  Two types are definitionally equal in Lean when they
  are identical after computation
  and unfolding of definitions; in other words, when Lean's type checker
  can mechanically verify they are the same without requiring additional proof steps.
  Here, the goal \lstinline[language=lean]|⊢ a ∧ b ∧ c| is definitionally
  equal to \lstinline[language=lean]|a ∧ (b ∧ c)| due to how conjunction
  associates, so \lstinline[language=lean]|show| accepts this statement.
  If we had tried to use \lstinline[language=lean]|show| with a type that
  was only \textbf{propositionally} equal (requiring a proof to establish equality)
  but not definitionally equal, Lean would reject it.
\end{example}
\subsection{Predicate logic and dependency}
To capture more complex mathematical ideas, we extend our system from
propositional logic to \textbf{predicate logic}.
A \textbf{predicate} is a statement or proposition that depends on a variable.
In propositional logic we represent a proposition simply by $P$.
In predicate logic, this is generalized: a predicate is written as $P(a)$,
where $a$ is a variable. Notice that a predicate is just a function.
This extension allows us to introduce \textbf{quantifiers}:
$\forall$ (``for all'') and $\exists$ (``there exists'').
These quantifiers express that a given formula holds either for every object
or for at least one object, respectively.
In Lean if \lstinline[language=lean]|α| is any type, we can represent a
predicate \lstinline[language=lean]|P| on \lstinline[language=lean]|α| as
an object of type \lstinline[language=lean]|α → Prop|.
Thus given an \lstinline[language=lean]|x : α| (an element
with type \lstinline[language=lean]|α| )
\lstinline[language=lean]|P(x) : Prop| would be representative of a proposition
holding for \lstinline[language=lean]|x|.
% When introducing variables into a formal language we must keep in mind that the specific choice 
% of a variable name can be substituted without
% changing the meaning of the predicate or statement. This should feel familiar from mathematics,
% where the meaning of an expression does not depend on the names we assign to variables.
% Some variables are \textbf{bound} (constrained), while others remain \textbf{free} 
% (arbitrary, in programming often called "dummy" variables). 
% When substituting variables, it is important to ensure that this distinction is preserved.
% This phenomenon, called \textbf{variable capture}, parallels familiar mathematical practice: 
% if $f(x) \coloneqq \int_1^2 \frac{dt}{x-t}$, then $f(t)$ equals $\int_1^2 \frac{ds}{t-s}$, 
% not the ill-defined $\int_1^2 \frac{dt}{t-t}$. The same principle applies to predicate logic. For example, consider
% [[
% $\exists y.\,(y > x)$.
% ]]
% This states that for a given $x$ there exists a $y$ such that $y > x$. 
% If we naively substitute $y+1$ for $x$, we would obtain
% [[
% $\exists y.\,(y > y+1)$,
% ]]
% where the $y$ in $y+1$ has been \textbf{captured} by the quantifier $\exists y$. 
% This transforms the original statement from "there exists some $y$ greater than the free variable $x$" into the always-false statement 
% "there exists some $y$ greater than itself plus one."
% To avoid the probelm, in the above example, we would first rename the bound variable to something fresh 
% say $z$, obtaining $\exists z.\,(z > x)$, and then safely substitute to get $\exists z.\,(z > y+1)$.
% \begin{notation}
%   We use the notation $\phi[t/x]$ for \textbf{substitution}, meaning all occurrences of the free 
%   variable $x$ in formula (or expression) $\phi$ are replaced by term $t$.
% \end{notation}
% We can now present the inference rules for quantifiers.
% \paragraph{Universal Quantification ($\forall$)}
% \begin{itemize}
%     \item Introduction
%     \begin{prooftree}
%     \AxiomC{$A$}
%     \RightLabel{$\forall I$}
%     \UnaryInfC{$\forall x.\,A$}
%     \end{prooftree}
%     The variable $x$ must be arbitrary in the derivation of $A$. 
%     This rule captures statements like 
%     $\forall x \in \mathbb{N}$, $x$ has a successor, 
%     but would not apply to $\forall x \in \mathbb{N}$, $x$ is prime 
%     (since we cannot derive this for an arbitrary natural number).
%     \item Elimination
%     \begin{prooftree}
%     \AxiomC{$\forall x.\,A$}
%     \RightLabel{$\forall E$}
%     \UnaryInfC{$A[t/x]$}
%     \end{prooftree}
%     The conclusion $A[t/x]$ represents the substitution of term $t$ for variable $x$ in formula $A$. 
%     From a proof of $\forall x.\,A(x)$ we can infer $A(t)$ for any term $t$.
% \end{itemize}
% \paragraph{Existential Quantification ($\exists$)}
% \begin{itemize}
%     \item Introduction
%     \begin{prooftree}
%     \AxiomC{$A[t/x]$}
%     \RightLabel{$\exists I$}
%     \UnaryInfC{$\exists x.\,A$}
%     \end{prooftree}
%     The substitution premise means that if we can find a specific term $t$ for which $A(t)$ holds, 
%     then we can introduce the existential quantifier. 
%     The introduction rule requires a witness $t$ for which the predicate holds.
%     \item Elimination
%     \begin{prooftree}
%     \AxiomC{$\exists x.\,A$}
%     \AxiomC{$[A] \vdash B$}
%     \RightLabel{$\exists E$}
%     \BinaryInfC{$B$}
%     \end{prooftree}
%     To eliminate an existential quantifier, we assume $A$ holds for some witness 
%     and derive $B$ without making any assumptions about the specific witness.
% \end{itemize}

We can give an informal reading of the quantifiers as infinite logical operations:
\begin{align*}
  \forall x.\,P(x) & \equiv P(a) \land P(b) \land P(c) \land \ldots \\
  \exists x.\,P(x) & \equiv P(a) \lor P(b) \lor P(c) \lor \ldots
\end{align*}
The expression $\forall x.\, P(x)$ can be understood as a generalized form of conjunction.
It expresses that $P$ holds for all possible values of $x$.
Similarly, $\exists x.\, P(x)$ is a generalized disjunction, expressing that $P$ holds
for at least one value of $x$.
Under the Curry-Howard isomorphism, universal quantifiers correspond to
\textbf{dependent function types} (also called Pi types, written $\Pi$),
while existential quantifiers correspond to
\textbf{dependent pair types} (also called Sigma types, written $\Sigma$).
These are constructs from dependent type theory, which provides a way to interpret
predicates or, more generally, types depending on some data or variable.
% This generalizes concepts such as predicates and functions.
This time we are not going to involve deduction rules or type judgments.
Instead, we will extend the isomorphism
to quantifiers directly
by presenting the Lean syntax.
\begin{example}[Quantifiers in Lean]
  Lean expresses quantifiers as follows:
  \begin{lstlisting}[language=lean]
  ∀ (x : X), P x
  Forall (x : X), P x
  -- Equivalently, using Pi types
  Π (x : X), P x
  \end{lstlisting}
  \begin{lstlisting}[language=lean]
  ∃ x : α, p
  Exists (λ x : α => p)
  -- Equivalently, using Sigma types
  Σ x : α, p
  \end{lstlisting}
\end{example}
\begin{example}[Universal introduction in Lean]
  The \textbf{universal introduction rule} allows us to prove $\forall x, P(x)$
  by proving $P(x)$ for an \textbf{arbitrary} $x$.
  In Lean, this corresponds to lambda abstraction (constructing a function):
  \newpage
  \begin{lstlisting}[language=lean]
  example : ∀ n : Nat, n ≥ 0 :=
    fun n => Nat.zero_le n
  \end{lstlisting}
\end{example}
\begin{example}[Universal elimination in Lean]
  The \textbf{universal elimination rule} allows us to instantiate
  a universally quantified statement with a specific value.
  In Lean, this is simply function application:
  \begin{lstlisting}[language=lean]
  example (h : ∀ n : Nat, n ≥ 0) : 5 ≥ 0 :=
    h 5
  \end{lstlisting}
\end{example}
\begin{example}[Existential introduction in Lean]
  When introducing an \textbf{existential} proof,
  we need a \textbf{pair} consisting
  of a witness and a proof that this witness
  satisfies the statement.
  \begin{lstlisting}[language=lean]
  example (x : Nat) (h : x > 0) : ∃ y, y < x :=
    ⟨0, h⟩
  \end{lstlisting}
  Notice that \lstinline[language=lean]|⟨0, h⟩| is a product type holding
  data (the witness~0) and a proof that it satisfies the property.
\end{example}
\begin{example}[Existential elimination in Lean]
  The \textbf{existential elimination rule}
  (\lstinline[language=lean]|Exists.elim|) allows us to prove a proposition $Q$
  from $\exists x, P(x)$ by showing that $Q$ follows from $P(w)$
  for an \textbf{arbitrary} value $w$.
  The existential quantifier can be interpreted as an infinite disjunction,
  so existential elimination naturally corresponds to a \textbf{proof by cases}
  (with a single case).
  In Lean, this is done using \textbf{pattern matching}
  with \lstinline[language=lean]|cases|:
  \newpage
  \begin{lstlisting}[language=lean]
  example (h : ∃ n : Nat, n > 0) : ∃ n : Nat, n > 0 := by
    cases h with
    | intro witness proof => ⟨witness, proof⟩
  \end{lstlisting}
\end{example}


\subsection{Constructive Mathematics}

Mathematicians have traditionally worked within \textbf{classical logic},
using \textbf{sets} as the primary means of structuring mathematical objects.
In contrast, \textbf{type theory} does not take sets as its primitive notion,
nor is it built by first applying logic and then adding structure.
Instead, logic is internal to type theory and is based on \textbf{constructive}
(or \textbf{intuitionistic}) logic, introduced by Brouwer and formalized by
Heyting (see, e.g., \cite{girard1989proofs}).

A major point of departure from classical logic is that, in constructive logic,
statements cannot simply be classified as true or false;
their truth depends on whether a proof exists.
There are many conjectures, such as the Riemann Hypothesis,
for which we do not yet know whether a proof or disproof exists,
so we cannot say whether they are true or false.
Consequently, constructive logic does not universally accept principles such
as the \textbf{axiom of choice} or the \textbf{law of excluded middle}
(every proposition is either true or false) as axioms.
As a consequence, proof by contradiction does not work in this setting
without additional justification.

Constructive logic emphasizes that a statement is only
considered true if we can explicitly construct
a proof or provide a \textbf{witness} for it.
This is what makes constructive mathematics inherently \textbf{computable}.

We already touched on this concept in the previous section.
In particular, we presented the logical connectives via the
Brouwer--Heyting--Kolmogorov (BHK) interpretation.
Following this interpretation, negation is not a primitive type
but is instead constructed as a function $\text{Prop} \to \text{False}$.
We also emphasized that, constructively,
a proof of existence consists of a pair:
a witness together with a proof that the stated property holds for that witness.

\begin{example}[Constructive existence proof]
  We give a constructive proof in Lean that there exist natural numbers
  $a$ and $b$ such that $a + b = 7$:
  \begin{lstlisting}[language=lean]
  example : ∃ a b : Nat, a + b = 7 := by
    use 3, 4
  \end{lstlisting}
  The \lstinline[language=lean]|use| tactic (from Mathlib) provides
  explicit witnesses: $a = 3$ and $b = 4$.
  Lean then automatically evaluates the expression and verifies that
  $3 + 4 = 7$.
  This example is simple enough for Lean to complete the proof automatically.
\end{example}

In classical mathematics, one might attempt a proof by contradiction.
However, this approach is not directly accepted in constructive mathematics,
as it doesn't provide explicit witnesses for the claimed objects.
Nonetheless, while constructive at its core, Lean allows users to
invoke classical principles, such as contraposition or proof by contradiction,
through tactics like \lstinline[language=lean]|exfalso| or by importing
\lstinline[language=lean]|Classical|.

\begin{example}[Reasoning from false]
  Here is an example of deriving any proposition from a contradiction:
  \begin{lstlisting}[language=lean]
  example (p : Prop) (h : False) : p := by
    exfalso
    exact h
  \end{lstlisting}
  This example takes a proposition $p$ to prove and a false hypothesis $h$.
  The \lstinline[language=lean]|exfalso| tactic transforms the goal into
  $\vdash \mathsf{False}$, meaning we now need to derive a contradiction.
  Since we already have a false hypothesis $h$,
  we can provide it using the \lstinline[language=lean]|exact| tactic.
  This principle is known as \textit{ex falso quodlibet}
  (from falsehood, anything follows).
\end{example}

% \paragraph{Key Principles of Constructive Mathematics}
% Some fundamental principles that distinguish constructive from classical mathematics include:
% \begin{itemize}
%   \item The law of excluded middle $P \lor \neg P$ is not universally accepted.
%   \item $\forall (x: \mathbb{N}). (\text{isPrime}(x) \lor \neg \text{isPrime}(x))$
%         is constructively valid because primality is a decidable property.
%   \item $\forall (x: \mathbb{N}). (\text{halts}(x) \lor \neg \text{halts}(x))$
%         is not decidable (where $\text{halts}$ determines if a program terminates).
%   \item Double negation elimination $\neg \neg P \to P$ is not constructively valid in general,
%         though $P \to \neg \neg P$ always holds.
% \end{itemize}



\section{Describing and use properties}
It is interesting to note that a relation can be expressed as a function:
\lstinline[language=lean]|R : α → α → Prop|.
Similarly, when defining a predicate (\lstinline[language=lean]|P : α → Prop|) we must first declare
\lstinline[language=lean]|α : Type| to be some arbitrary type.
This is what is called \textbf{polymorphism}, more specifically \textbf{parametrical polymorphism}.
A canonical example is the identity function, written as
\lstinline[language=lean]|α → α|, where
\lstinline[language=lean]|α| is a type variable.
It has the same type for
both its domain and codomain, this means it can be
applied to booleans (returning a boolean), numbers (returning a number),
functions (returning a function), and so on.
In the same spirit, we can define a transitivity property of a relation as follows:
\begin{lstlisting}[language=lean]
def Transitive (α : Type) (R : α → α → Prop) : Prop :=
  ∀ x y z, R x y → R y z → R x z
\end{lstlisting}
To use \lstinline[language=lean]|Transitive|, we must provide both the type
\lstinline[language=lean]|α| and the relation itself.
For example, here is a proof of transitivity for the less-than relation on
$\mathbb{N}$ ( in Lean \lstinline[language=lean]|Nat| or \lstinline[language=lean]|ℕ|):
\begin{lstlisting}[language=lean]
theorem le_trans_proof : Transitive Nat (· ≤ · : Nat → Nat → Prop) :=
  fun x y z h1 h2 => Nat.le_trans h1 h2 -- this lemma is provided by Lean 
\end{lstlisting}
Looking at this code, we immediately notice that explicitly
passing the type argument \lstinline[language=lean]|Nat| is somewhat repetitive.
Lean allows us to omit it by letting the type inference mechanism fill it in automatically.
This is achieved by using \textbf{implicit arguments} with curly brackets:
\begin{lstlisting}[language=lean]
def Transitive {α : Type} (R : α → α → Prop) : Prop :=
  ∀ x y z, R x y → R y z → R x z
theorem le_trans_proof : Transitive (· ≤ · : Nat → Nat → Prop) :=
  fun x y z h1 h2 => Nat.le_trans h1 h2 
\end{lstlisting}
Lean's type inference system is quite powerful: in many cases, types can be completely
inferred without explicit annotations.
\begin{example}[Type Inference in Lean]\mbox{}
  \begin{lstlisting}[language=lean]
  def double (n : Nat) := n + n
  -- Lean infers return type is Nat because n : Nat and + : Nat → Nat → Nat
  def id {α : Type} (x : α) : α := x
  #check id 5        -- Lean infers α = Nat
  #check id "hello"  -- Lean infers α = String
  \end{lstlisting}
\end{example}
Let us now revisit the transitivity proof, but this time for the less-than-equal relation on
the rational numbers (\lstinline[language=lean]|Rat| or \lstinline[language=lean]|ℚ|) instead.
\newpage
\begin{lstlisting}[language=lean]
  import Mathlib

  theorem rat_le_trans : Transitive (· ≤ · :   Rat → Rat → Prop) :=
    fun _ _ _ h1 h2 => Rat.le_trans h1 h2
\end{lstlisting}
Here, \lstinline[language=lean]|Rat.le_trans| is the transitivity lemma
for \lstinline[language=lean]|≤| on rational numbers, provided by Mathlib.
We import Mathlib to access \lstinline[language=lean]|Rat|
and \lstinline[language=lean]|le_trans|.
Mathlib is the community‑driven mathematical
library for Lean, containing a large body of formalized mathematics
and ongoing development.
It is the defacto standard library for both programming and proving
in Lean \cite{mathlib2020}, we will dig into it as we go along.
Notice that we used a function to discharge the universal
quantifiers required by transitivity. The underscores indicate
unnamed variables that we do not use later. If we had named
them, say \lstinline|x y z|, then:
\lstinline[language=lean]|h1| would be a proof of \lstinline[language=lean]|x ≤ y|,
\lstinline[language=lean]|h2| would be a proof of \lstinline[language=lean]|y ≤ z|,
and \lstinline[language=lean]|Rat.le_trans h1 h2| produces a proof of \lstinline[language=lean]|x ≤ z|.
The \lstinline[language=lean]|Transitive| definition is imported from Mathlib and similarly
defined as before.
\begin{example}
  The code can be made more readable using \textbf{tactic mode}.
  In this mode, you use tactics,
  commands provided by Lean or defined by users, to
  carry out proof steps succinctly, avoid code repetition,
  and automate common patterns.
  This often yields shorter, clearer proofs than writing
  the full term by hand.
  \begin{lstlisting}[language=lean]
  import Mathlib

  theorem rat_le_trans : Transitive (· ≤ · : Rat → Rat → Prop) := by
    intro x y z hxy hyz
    exact Rat.le_trans hxy hyz
\end{lstlisting}
  This proof performs the same steps but is much easier to read.
  Using \lstinline[language=lean]|by| we enter Lean's tactic mode.
  Move your cursor just before \lstinline[language=lean]|by|.
  The goal is initially displayed as \lstinline[language=lean]|⊢ Transitive fun x1 x2 ↦ x1 ≤ x2|.
  The tactic \lstinline[language=lean]|intro| is mainly used to introduce
  variables and hypotheses corresponding to universal quantifiers
  and assumptions into the context (essentially deconstructing universal quantifiers and implications).
  Now position your cursor just before \lstinline[language=lean]|exact|
  and observe the info view again.
  The goal is now \lstinline[language=lean]|⊢ x ≤ z|, with the context
  showing the variables and hypotheses introduced by the previous tactic.
  The \lstinline[language=lean]|exact| tactic closes the goal
  by supplying the term \lstinline[language=lean]|Rat.le_trans hxy hyz| that exactly matches the goal
  (the specification of \lstinline[language=lean]|Transitive|).
  You can hover over each tactic to see its definition and documentation.
\end{example}
\subsection{Exploring Mathlib (The Rat structure)}
In these examples we cheated and have used predefined lemmas such as
\lstinline[language=lean]|Nat.le_trans| and
\lstinline[language=lean]|Rat.le_trans|, just to simplify the presentation.
We can now dig into the implementation of these lemmas.
Let's look at the source code of \lstinline[language=lean]|Rat.le_trans|.
The Mathlib 4 documentation website is at
\url{https://leanprover-community.github.io/mathlib4_docs}, and
the documentation for
\lstinline[language=lean]|Rat.le_trans| is at
\url{https://leanprover-community.github.io/mathlib4_docs/Mathlib/Algebra/Order/Ring/Unbundled/Rat.html#Rat.le_trans}.
Click the "source" link there to jump to the implementation in the Mathlib repository. In editors like
VS Code you can also jump directly to the definition (Ctrl+click; Cmd+click on macOS).
Another way to check source code is by using \lstinline[language=lean]|#print Rat.le_trans|.
\begin{lstlisting}[language=lean]
variable (a b c : Rat)
protected lemma le_trans (hab : a ≤ b) (hbc : b ≤ c) : a ≤ c := by
  rw [Rat.le_iff_sub_nonneg] at hab hbc
  have := Rat.add_nonneg hab hbc
  simp_rw [sub_eq_add_neg, add_left_comm (b + -a) c (-b), add_comm (b + -a) (-b), add_left_comm (-b) b (-a), add_comm (-b) (-a), add_neg_cancel_comm_assoc, ← sub_eq_add_neg] at this
  rwa [Rat.le_iff_sub_nonneg]
\end{lstlisting}
The proof uses several tactics and lemmas from Mathlib.
The \lstinline[language=lean]|rw| or \lstinline[language=lean]|rewrite| tactic
is very common and sintactically similar to
the mathematical practice of rewriting an expression using an equality.
In this case, with \lstinline[language=lean]|at|, we use it to rewrite the
hypotheses \lstinline[language=lean]|hab|
and \lstinline[language=lean]|hbc|
using another Mathlib's lemma \lstinline[language=lean]|Rat.le_iff_sub_nonneg|,
which states that for any two rational numbers \lstinline[language=lean]|x| and
\lstinline[language=lean]|y|, \lstinline[language=lean]|x ≤ y|
is equivalent to \lstinline[language=lean]|0 ≤ y - x|.
Thus we now have the hypotheses tranformerd to :
\begin{lstlisting}[language=lean]
  hab : 0 ≤ b - a
  hbc : 0 ≤ c - b
\end{lstlisting}
The \lstinline[language=lean]|have| tactic introduces an intermediate result.
If you omit a name, Lean assigns it the default name \lstinline[language=lean]|this|.
In our situation, from \lstinline[language=lean]|hab : a ≤ b| and \lstinline[language=lean]|hbc : b ≤ c|
we can derive that \lstinline[language=lean]|b - a| and \lstinline[language=lean]|c - b|
are nonnegative, hence their sum is nonnegative:
\begin{lstlisting}[language=lean]
  this : 0 ≤ b - a + (c - b)
\end{lstlisting}
The most involved step uses \lstinline[language=lean]|simp_rw| to
simplify the expression via a sequence of other existing Mathlib's lemmas.
The tactic \lstinline[language=lean]|simp_rw| is a variant of \lstinline[language=lean]|simp|:
it performs rewriting using the simp set (and any lemmas you provide), applying the rules
in order and in the given direction. Lemmas that \lstinline[language=lean]|simp| can use
are typically marked with the \lstinline[language=lean]|@[simp]| attribute.
This is particularly useful for simplifying algebraic expressions and equations.
After these simplifications we obtain:
\begin{lstlisting}[language=lean]
  this : 0 ≤ c - a
\end{lstlisting}
Clearly, the proof relies mostly on \lstinline[language=lean]|Rat.add_nonneg|.
Its source code is fairly involved and uses advanced features
that are beyond our current scope. Nevertheless, it highlights
an important aspect of formal mathematics in Mathlib.
Mathlib defines \lstinline[language=lean]|Rat| as an instance of
a linear ordered field, implemented via a normalized fraction
representation: a pair of integers (numerator and denominator)
with positive denominator and coprime numerator and denominator \cite{mathlibdoc}.
To achieve this, it uses a \textbf{structure}. In Lean, a structure is a dependent record
(or product type) type  used to group together related fields or properties as a single data type.
Unlike ordinary records, the type of later fields may depend on the values of earlier ones.
Defining a structure automatically introduces a constructor (usually mk) and projection
functions that retrieve (deconstruct) the values of its fields.
Structures may also include proofs expressing properties that the fields must satisfy.
\newpage
\begin{lstlisting}[language=lean]
  structure Rat where
    /-- Constructs a rational number from components.
    We rename the constructor to `mk'` to avoid a clash with the smart constructor. -/
    mk' ::
    /-- The numerator of the rational number is an integer. -/
    num : Int
    /-- The denominator of the rational number is a natural number. -/
    den : Nat := 1
    /-- The denominator is nonzero. -/
    den_nz : den ≠ 0 := by decide
    /-- The numerator and denominator are coprime: it is in "reduced form". -/
    reduced : num.natAbs.Coprime den := by decide
\end{lstlisting}
In order to work with rational numbers in Mathlib, we use the
\lstinline[language=lean]|Rat.mk'| constructor to create a rational number from
its numerator and denominator, if omitted the default would be \lstinline[language=lean]|Rat.mk|.
The fields \lstinline[language=lean]|den_nz| and \lstinline[language=lean]|reduced| are proofs that
the denominator is nonzero and that the numerator and denominator are coprime, respectively.
These proofs are automatically generated by Lean's \lstinline[language=lean]|decide| tactic, which can
solve certain decidable propositions (to be discussed in the next section).
\begin{example}
  Here is how we can define and manipulate rational numbers in Lean.
  \begin{lstlisting}[language=lean]
    def half : Rat := Rat.mk' 1 2
    def third : Rat := Rat.mk' 1 3
  \end{lstlisting}
\end{example}
When working with rational numbers, or more generally with structures, we must provide the
required proofs as arguments to the constructor (or Lean must be able to ensure them).
For instance \lstinline[language=lean]|Rat.mk' 1 0| or \lstinline[language=lean]|Rat.mk' 2 6|
would be rejected.
In the case of rationals, Mathlib unfolds the definition through
\lstinline[language=lean]|Rat.numDenCasesOn|. This principle states that, to prove a property of an
arbitrary rational number, it suffices to consider numbers of the form \lstinline[language=lean]|n /. d|
in canonical (normalized) form, with \lstinline[language=lean]|d > 0| and \lstinline[language=lean]|gcd n d = 1|.
This reduction allows mathlib to transform proofs about \lstinline[language=lean]|ℚ|
into proofs about \lstinline[language=lean]|ℤ| and \lstinline[language=lean]|ℕ|,
and then lift the result back to rationals.
\begin{example}
  We present a simplified implementation of addition
  non-negativity for rationals (\lstinline[language=lean]|Rat.add_nonneg| ),
  maintaining a similar approach: projecting everything to the natural numbers and
  integers first. To illustrate the proof technique clearly, we avoid using existing lemmas
  from the Rat module in Mathlib.
  Mathlib is indeed organized into modules by mathematical
  domain (e.g., Nat, Int, Rat).
  % Lemmas are typically namespaced (e.g., Rataddnonneg) 
  % and often marked protected to prevent namespace pollution. 

  We start by defining helper lemmas needed in the main proof.
  Given a natural number
  (which in this case represents the denominator of a rational number) that is not
  equal to zero, we prove it must be positive. This follows directly by
  applying the Mathlib lemma \lstinline[language=lean]|Nat.pos_of_ne_zero|:
  \newpage
  \begin{lstlisting}[language=lean]
  import Mathlib

  lemma nat_ne_zero_pos (den : ℕ) (h_den_nz : den ≠ 0) : 0 < den :=
    Nat.pos_of_ne_zero h_den_nz
\end{lstlisting}
  The naming convention follows Mathlib
  best practices aiming to be descriptive by indicating
  types and properties involved.

  The following lemma is slightly more involved.
  It states that if a rational number (num / den)
  is non-negative, then its numerator must also be non-negative:
  \begin{lstlisting}[language=lean]
lemma rat_num_nonneg {num : ℤ} {den : ℕ} (hden_pos : 0 < den)
    (h : (0 : ℚ) ≤ num / den) : 0 ≤ num := by
  contrapose! h
  have hden_pos_to_rat : (0 : ℚ) < den := Nat.cast_pos.mpr hden_pos
  have hnum_neg_to_rat : num < (0 : ℚ) := Int.cast_lt.mpr h
  exact div_neg_of_neg_of_pos hnum_neg_to_rat hden_pos_to_rat
\end{lstlisting}
  The lemma requires the denominator to be positive as well as the non-negativity of the rational number,
  expressed as \lstinline[language=lean]|num / den| where the types of
  \lstinline[language=lean]|num| and \lstinline[language=lean]|den| are inferred.
  First, notice the type annotation \lstinline[language=lean]|(0 : ℚ)|.
  This explicit type annotation on zero forces the entire equation to be casted
  into rational numbers.
  Without this annotation, Lean would infer \lstinline[language=lean]|0|
  as a natural number by default. However, since the main theorem we are proving concerns rational numbers,
  we must ensure all comparisons occur in \lstinline[language=lean]|ℚ|.
  The tactic \lstinline[language=lean]|contrapose!| does what you might expect: it proves a statement by contraposition. According to the documentation:
  \begin{itemize}
    \item \lstinline[language=lean]|contrapose| turns a goal \lstinline[language=lean]|P → Q| into \lstinline[language=lean]|¬ Q → ¬ P|
    \item \lstinline[language=lean]|contrapose!| turns a goal \lstinline[language=lean]|P → Q| into \lstinline[language=lean]|¬ Q → ¬ P| and pushes negations inside \lstinline[language=lean]|P| and \lstinline[language=lean]|Q| using \lstinline[language=lean]|push_neg|
    \item \lstinline[language=lean]|contrapose h| first reverts the local assumption \lstinline[language=lean]|h|, then uses \lstinline[language=lean]|contrapose| and \lstinline[language=lean]|intro h|
    \item \lstinline[language=lean]|contrapose! h| first reverts the local assumption \lstinline[language=lean]|h|, then uses \lstinline[language=lean]|contrapose!| and \lstinline[language=lean]|intro h|
  \end{itemize}
  In our case, \lstinline[language=lean]|contrapose! h| transforms the goal from
  proving \lstinline[language=lean]|0 ≤ num| to assuming \lstinline[language=lean]|num < 0|
  and proving \lstinline[language=lean]|num / den < 0|.
  We then introduce two local hypotheses. The first, \lstinline[language=lean]|hden_pos_to_rat|,
  proves that the denominator
  is positive when cast to rationals, using \lstinline[language=lean]|Nat.cast_pos|.
  The suffix \lstinline[language=lean]|.mpr| selects the ``modus ponens reverse''
  direction of the biconditional (the \lstinline[language=lean]|←|
  direction of the \lstinline[language=lean]|↔|).
  Next, we introduce \lstinline[language=lean]|hnum_neg_to_rat|,
  which expresses that the numerator is negative when cast to rationals, using \lstinline[language=lean]|Int.cast_lt| with \lstinline[language=lean]|.mpr| again.
  Finally, we apply \lstinline[language=lean]|div_neg_of_neg_of_pos|,
  which states that dividing a negative number by a positive number yields a
  negative result, thus completing the proof by contraposition.
  Note that we are allowing ourselves to use existing lemmas from Mathlib,
  such as \lstinline[language=lean]|div_neg_of_neg_of_pos| from the \lstinline[language=lean]|Field| module,
  but not from the \lstinline[language=lean]|Rat| module,
  to keep the presentation clear and focused on the main proof techniques.
  \newpage
  Now we can prove the main result:
  \begin{lstlisting}[language=lean]
lemma rat_add_nonneg (a b : Rat) : 0 ≤ a → 0 ≤ b → 0 ≤ a + b := by

  intro ha hb
  cases a with | div a_num a_den a_den_nz a_cop =>
  cases b with | div b_num b_den b_den_nz b_cop =>
  -- Goal: ⊢ 0 ≤ ↑a_num / ↑a_den + ↑b_num / ↑b_den
  rw[div_add_div] -- applies the addition formula requiring two new goals 
  · sorry 
  · sorry 
  · sorry
\end{lstlisting}
  \newpage
  We first introduce the two hypotheses \lstinline[language=lean]|ha| and
  \lstinline[language=lean]|hb| into the context using \lstinline[language=lean]|intro|.
  As mentioned earlier, a structure can be viewed as a product type or a record type with
  a single constructor. The tactic \lstinline[language=lean]|cases a with|
  exposes the fields of \lstinline[language=lean]|Rat|: the
  numerator ()\lstinline[language=lean]|a_num|), denominator
  (\lstinline[language=lean]|a_den|), the proof that the denominator is non-zero
  (\lstinline[language=lean]|a_den_nz|), and the coprimality condition
  (\lstinline[language=lean]|a_cop|). Notice how the goal transforms
  the rationals \lstinline[language=lean]|a| and \lstinline[language=lean]|b| into:
  \begin{lstlisting}[language=lean]
⊢ 0 ≤ ↑a_num / ↑a_den + ↑b_num / ↑b_den
\end{lstlisting}
  where \lstinline[language=lean]|↑| denotes type coercion from
  \lstinline[language=lean]|ℤ| or \lstinline[language=lean]|ℕ| to
  \lstinline[language=lean]|ℚ|.
  Now we rewrite the goal using \lstinline[language=lean]|rw [div_add_div]|,
  a theoprem from the \lstinline[language=lean]|Field| module,
  which applies the addition formula for division.
  Let us briefly examine the source code of this theorem:
  \begin{lstlisting}[language=lean]
variable [Semifield K] {a b d : K}

theorem div_add_div (a : K) (c : K) (hb : b ≠ 0) (hd : d ≠ 0) :
    a / b + c / d = (a * d + b * c) / (b * d) := ...
\end{lstlisting}
  The type \lstinline[language=lean]|K| here is assumed to be a
  \lstinline[language=lean]|Semifield|. The \lstinline[language=lean]|variable|
  keyword is a way to declare parameters that are potentially used across
  multiple theorems or definitions. We will explore Lean's powerful algebraic
  hierarchy and the meaning of the square brackets \lstinline[language=lean]|[ ]|
  in a later section.
  Using this rewrite is particularly time-saving, since otherwise one would have to
  establish the well-definedness of rational addition in terms of the underlying
  structure (a non-trivial task).
  This theorem requires proofs \lstinline[language=lean]|(hb : b ≠ 0)| and
  \lstinline[language=lean]|(hd : d ≠ 0)|, generating two additional side goals.
  We handle each goal separately using the focusing bullet \lstinline[language=lean]|·|.
  The first bullet addresses the main goal (proving the sum is non-negative),
  while the subsequent bullets discharge the non-zero denominator conditions.
  I have omitted the actual proofs, here, using \lstinline[language=lean]|sorry|,
  which we haven't mentioned before. \lstinline[language=lean]|sorry| is a useful
  feature of Lean that tells the system to accept an incomplete proof for the time being,
  allowing you to continue development without proving every detail immediately.
  We can now tackle the remaining goals:
  \newpage
  \begin{lstlisting}[language=lean]
· -- Goal: ⊢ 0 ≤ (↑a_num * ↑b_den + ↑a_den * ↑b_num) / (↑a_den * ↑b_den)
  have hnum_nonneg : (0 : ℚ) ≤ a_num * b_den + a_den * b_num := by
    have ha_num_nonneg := by
      have ha_den_pos := nat_ne_zero_pos a_den a_den_nz
      exact rat_num_nonneg ha_den_pos ha
    have hb_num_nonneg := by
      have hb_den_pos := nat_ne_zero_pos b_den b_den_nz
      exact rat_num_nonneg hb_den_pos hb
    apply add_nonneg -- works for any OrderedAddCommMonoid
    · apply mul_nonneg -- works for any OrderedSemiring
      · exact Int.cast_nonneg.mpr ha_num_nonneg
      · exact Nat.cast_nonneg b_den
    · apply mul_nonneg
      · exact Nat.cast_nonneg a_den
      · exact Int.cast_nonneg.mpr hb_num_nonneg

  have hden_nonneg : (0 : ℚ) ≤ a_den * b_den := by
    rw [← Nat.cast_mul]
    exact Nat.cast_nonneg (a_den * b_den)
  exact div_nonneg hnum_nonneg hden_nonneg

· exact Nat.cast_ne_zero.mpr a_den_nz -- Goal: ⊢ ↑a_den ≠ 0
· exact Nat.cast_ne_zero.mpr b_den_nz -- Goal: ⊢ ↑b_den ≠ 0
\end{lstlisting}

  We introduce two key hypotheses, \lstinline[language=lean]|hnum_nonneg|
  and \lstinline[language=lean]|hden_nonneg|, which will be required by
  \lstinline[language=lean]|div_nonneg| from the
  \lstinline[language=lean]|GroupWithZero| module.
  This lemma provides us with a term that directly validates our statement.
  Note that \lstinline[language=lean]|div_nonneg| is a generalized lemma that applies
  not only to rational numbers but to all ordered groups with zero that are
  also partially ordered.
  The hypothesis \lstinline[language=lean]|hnum_nonneg| proves that the numerator
  is non-negative by working with the coerced expressions in
  \lstinline[language=lean]|ℚ|. It uses \lstinline[language=lean]|add_nonneg| and
  \lstinline[language=lean]|mul_nonneg|, which are general theorems that work for
  any ordered additive commutative monoid and ordered semiring, respectively.
  The actual reasoning is done using integer-related theorems
  (via \lstinline[language=lean]|Int.cast_nonneg|) for the numerators and natural
  number theorems (via \lstinline[language=lean]|Nat.cast_nonneg|) for the denominators.
  The hypothesis \lstinline[language=lean]|hden_nonneg| proves that the
  denominator is non-negative by working entirely with natural numbers.
  We use the rewrite \lstinline[language=lean]|rw [← Nat.cast_mul]|,
  which moves the coercion (in this case from \lstinline[language=lean]|ℕ|
  to \lstinline[language=lean]|ℚ|) inside the multiplication:
  \lstinline[language=lean]|↑(m * n) = ↑m * ↑n|. The \lstinline[language=lean]|←|
  symbol means that we want the transformation from right to
  left (i.e., we apply the equality in reverse to move the cast inward).
  Type casts and coercions require these kinds of rewrite rules, not only
  for multiplication but also for addition and other operations, and
  similarly for \lstinline[language=lean]|ℤ| or other numerical types.
  These lemmas, such as \lstinline[language=lean]|Nat.cast_mul|,
  \lstinline[language=lean]|Int.cast_add|, etc., ensure that algebraic operations
  commute with type coercions.
\end{example}
\subsection{Coercions and Type Casting}
We extensively used type casting and coercions in this proof, which requires some
explanation \cite{lewis_madelaine_simplifying_casts_coercions_2020}.
Lean's type system lacks subtyping, means that types
like \lstinline[language=lean]|ℕ|, \lstinline[language=lean]|ℤ|, and \lstinline[language=lean]|ℚ|
are distinct and do not have a subtype relationship. In order to translate between these types,
we need to use explicit type casts or rely on automatic coercions. For example,
natural numbers (\lstinline[language=lean]|ℕ|) can be coerced to integers (\lstinline[language=lean]|ℤ|),
and integers can be coerced to rational numbers (\lstinline[language=lean]|ℚ|).
Casting and coercion are related but distinct concepts:
\begin{itemize}
  \item \textbf{Casting} refers to the explicit conversion of a value from one type to another,
        typically using functions like \lstinline[language=lean]|Int.cast| or \lstinline[language=lean]|Nat.cast|.
        These functions have accompanying lemmas that preserve properties across type conversions,
        such as \lstinline[language=lean]|Int.cast_lt| and \lstinline[language=lean]|Nat.cast_pos|.
  \item \textbf{Coercion}, on the other hand, is a more general mechanism that allows
        Lean to automatically convert between types when needed.
        More generally, in expressions like \lstinline[language=lean]|x + y| where \lstinline[language=lean]|x|
        and \lstinline[language=lean]|y| are of different types,
        Lean will automatically coerce them to a common type. For example, if \lstinline[language=lean]|x : ℕ|
        and \lstinline[language=lean]|y : ℤ|, then \lstinline[language=lean]|x|
        will be coerced to \lstinline[language=lean]|ℤ|.
\end{itemize}
The notation \lstinline[language=lean]|↑| denotes an explicit coercion
(in between cast and coercion).
To illustrate the expected behavior of coercion simplification, consider
the expression \lstinline[language=lean]|↑m + ↑n < (10 : ℤ)|,
where \lstinline[language=lean]|m, n : ℕ| are cast to \lstinline[language=lean]|ℤ|.
The expected normal form is \lstinline[language=lean]|m + n < (10 : ℕ)|,
since \lstinline[language=lean]|+|,
<,
and the numeral \lstinline[language=lean]|10| are polymorphic
(i.e., they can work with any numerical type such as \lstinline[language=lean]|ℤ|
or \lstinline[language=lean]|ℕ|). The simplification should proceed as follows:
\begin{enumerate}
  \item Replace the numeral on the right with the cast of a natural number:
        \lstinline[language=lean]|↑m + ↑n < ↑(10 : ℕ)|
  \item Factor \lstinline[language=lean]|↑| to the outside on the left:
        \lstinline[language=lean]|↑(m + n) < ↑(10 : ℕ)|
  \item Eliminate both casts to obtain an inequality over \lstinline[language=lean]|ℕ|:
        \lstinline[language=lean]|m + n < (10 : ℕ)|
\end{enumerate}
Lean provides tactics like \lstinline[language=lean]|norm_cast|
to simplify expressions involving such coercions.
The \lstinline[language=lean]|norm_cast| tactic normalizes casts
by pushing them outward and eliminating redundant coercions, often simplifying
proofs significantly by reducing goals to their ``native'' types.

\subsection{Type Classes and Algebraic Hierarchy in Lean}

In our proof of \lstinline[language=lean]|rat_add_nonneg|,
we used many generalized lemmas from Mathlib, such as \lstinline[language=lean]|add_nonneg|,
\lstinline[language=lean]|mul_nonneg|, and \lstinline[language=lean]|div_nonneg|,
which apply to a wide range of types beyond just rational numbers.
Similarly, in our earlier work with natural numbers, we used \lstinline[language=lean]|Nat.le_trans|,
a theorem specifically for natural numbers that is part of Lean's core library
(\lstinline[language=lean]|lean/Init/Prelude.lean|). Mathlib is built on top of this base library.
However, the transitivity property holds not only for naturals but also for integers,
reals, and, in fact, for any partially ordered set.
Rather than duplicating this theorem for each type, Mathlib provides a general
lemma \lstinline[language=lean]|le_trans| that works for any type \lstinline[language=lean]|α|
endowed with a partial ordering.
This is achieved through \textbf{type classes}, Lean's mechanism for defining and working with
abstract algebraic structures in an ad hoc polymorphic manner.
Type classes provide a powerful and flexible way to specify properties and operations that can be
shared across different types, thereby enabling polymorphism and code reuse. Ad hoc polymorphism
arises when a function or operator is defined over several distinct types, with behavior that varies
depending on the type.
A standard example \cite{wadler_blott_ad_hoc_polymorphism_1988} is overloaded multiplication:
the same symbol \lstinline[language=lean]|*| denotes multiplication of integers
(e.g., \lstinline[language=lean]|3 * 3|) and of floating-point numbers
(e.g., \lstinline[language=lean]|3.14 * 3.14|).
By contrast, parametric polymorphism occurs when a function is defined over a range of types
but acts uniformly on each of them. For instance, the \lstinline[language=lean]|List.length|
function applies in the same way to a list of integers and to a list of floating-point numbers.

Under the hood, a type class is a structure.
An important aspect of structures, and hence type classes, is that they support hierarchy
and composition through inheritance. For example, mathematically, a monoid is a semigroup
with an identity element, and a group is a monoid with inverses. In Lean, we can express this
by defining a \lstinline[language=lean]|Monoid| structure that extends the
\lstinline[language=lean]|Semigroup| structure, and a
\lstinline[language=lean]|Group| structure that extends the
\lstinline[language=lean]|Monoid| structure using the
\lstinline[language=lean]|extends| keyword:
\begin{lstlisting}[language=lean]
-- A semigroup has an associative binary operation
structure Semigroup (α : Type*) where
  mul : α → α → α
  mul_assoc : ∀ a b c : α, mul (mul a b) c = mul a (mul b c)

-- A monoid extends semigroup with an identity element  
structure Monoid (α : Type*) extends Semigroup α where
  one : α
  one_mul : ∀ a : α, mul one a = a
  mul_one : ∀ a : α, mul a one = a

-- A group extends monoid with inverses
structure Group (α : Type*) extends Monoid α where
  inv : α → α
  mul_left_inv : ∀ a : α, mul (inv a) a = one
\end{lstlisting}
The symbol \lstinline[language=lean]|*| in \lstinline[language=lean]|(α : Type*)|
indicates a universe variable (we will discuss universes later). Sometimes,
to avoid inconsistencies between types (such as Girard's paradox),
universes must be specified explicitly. This is an example of universe polymorphism.
Thus we have now seen all the polymorphism flavors in Lean: parametric, ad hoc, and universe polymorphism.

Type classes are defined using the \lstinline[language=lean]|class| keyword,
which is syntactic sugar for defining a structure. Thus, the previous example
can be rewritten using type classes:
\newpage
\begin{lstlisting}[language=lean]
-- A semigroup has an associative binary operation
class Semigroup (α : Type*) where
  mul : α → α → α
  mul_assoc : ∀ a b c : α, mul (mul a b) c = mul a (mul b c)

-- A monoid extends semigroup with an identity element  
class Monoid (α : Type*) extends Semigroup α where
  one : α
  one_mul : ∀ a : α, mul one a = a
  mul_one : ∀ a : α, mul a one = a

-- A group extends monoid with inverses
class Group (α : Type*) extends Monoid α where
  inv : α → α
  mul_left_inv : ∀ a : α, mul (inv a) a = one
\end{lstlisting}
The main difference is that type classes support \textbf{instance resolution}.
We use the keyword \lstinline[language=lean]|instance| to declare that a particular type is an
instance of a type class, which inherits the properties and operations defined in the type class.
Instances can be automatically inferred by Lean's type inference system,
allowing for concise and expressive code.
For example, we can declare that \lstinline[language=lean]|ℤ| is a group under addition:
\begin{lstlisting}[language=lean]
instance : Group ℤ where
  mul := Int.add
  one := 0
  inv := Int.neg
  mul_assoc := Int.add_assoc
  one_mul := Int.zero_add
  mul_one := Int.add_zero
  mul_left_inv := Int.neg_add_cancel
\end{lstlisting}
Now, any theorem proven for an arbitrary \lstinline[language=lean]|Group α|
automatically applies to \lstinline[language=lean]|ℤ| without any additional work.
This mechanism is particularly useful for defining and working with order structures
like preorders and partial orders. Mathematically, a preorder consists
of a set \lstinline[language=lean]|P| and a binary relation \lstinline[language=lean]|≤|
on \lstinline[language=lean]|P| that is reflexive and transitive \cite{mathinlean}.
\newpage
\begin{lstlisting}[language=lean, caption=Preorder Type Class in Lean]
-- A preorder is a reflexive, transitive relation `≤` with `<` defined in terms of `≤`
class Preorder (α : Type*) extends LE α, LT α where
  le_refl : ∀ a : α, a ≤ a
  le_trans : ∀ a b c : α, a ≤ b → b ≤ c → a ≤ c
  lt := fun a b => a ≤ b ∧ ¬ b ≤ a
  lt_iff_le_not_ge : ∀ a b : α, a < b ↔ a ≤ b ∧ ¬ b ≤ a := by intros; rfl

instance [Preorder α] : Lean.Grind.Preorder α where
  le_refl := Preorder.le_refl
  le_trans := Preorder.le_trans _ _ _
  lt_iff_le_not_le := Preorder.lt_iff_le_not_ge _ _
\end{lstlisting}

The \lstinline[language=lean]|class Preorder| declares a type class over a type
\lstinline[language=lean]|α|, bundling the \lstinline[language=lean]|≤|
and \lstinline[language=lean]|<| relations
(inherited via \lstinline[language=lean]|extends LE α, LT α|)
with the preorder axioms: reflexivity (\lstinline[language=lean]|le_refl|)
and transitivity (\lstinline[language=lean]|le_trans|).
The field \lstinline[language=lean]|lt| provides a default definition of strict
inequality in terms of \lstinline[language=lean]|≤|,
and the theorem \lstinline[language=lean]|lt_iff_le_not_ge| characterizes this relationship,
proved automatically via reflexivity (\lstinline[language=lean]|by intros; rfl|).
The \lstinline[language=lean]|instance| declaration connects the \lstinline[language=lean]|Preorder|
class to Lean's \lstinline[language=lean]|Grind| tactic automation, which allows automatic
reasoning with preorder properties during proof search.
Returning to our rational number proof, this explains why lemmas
like \lstinline[language=lean]|add_nonneg| and \lstinline[language=lean]|mul_nonneg|
work seamlessly: \lstinline[language=lean]|ℚ| is an instance of
\lstinline[language=lean]|OrderedSemiring|, which extends
\lstinline[language=lean]|Preorder| and other algebraic structures,
automatically providing all their theorems.


We have roughly seen how Lean constructively builds the rational numbers
from naturals and integers. Using the power of structures and type classes,
Mathlib generalizes these concepts further to develop rich mathematical theories.
However, when dealing with real numbers, the approach taken includes
the use of the axiom of choice, which, as we discussed in
constructive mathematics, is not directly accepted constructively.
When constructive methods are insufficient, Lean provides classical
axioms through the \lstinline[language=lean]|Classical| module.
For instance, the law of excluded middle become available:
\begin{lstlisting}[language=lean]
open Classical

example (p : Prop) : p ∨ ¬p := em p 
\end{lstlisting}
Using classical axioms comes at a cost: definitions and theorems that
depend on them must be marked \lstinline[language=lean]|noncomputable|.
For example, many operations on real numbers, such as computing the sine
function requires this marker:
\begin{lstlisting}[language=lean]
noncomputable def realSin (x : ℝ) : ℝ := Real.sin x
\end{lstlisting}
% In contrast, Lean uses the \lstinline[language=lean]|Decidable| typeclass
% to express when a proposition can be computed algorithmically.
% For decidable propositions, the \lstinline[language=lean]|decide| tactic
% can automatically prove them by computation:
% \begin{lstlisting}[language=lean]
% #eval decide (5 < 10)     -- true (computable!)
% #eval decide (isPrime 17)  -- true

% example (α : Type) [DecidableEq α] [Fintype α] (P : α → Prop) 
%   [DecidablePred P] : Decidable (∀ x, P x) := inferInstance
% \end{lstlisting}
% This distinction between computable and noncomputable extends to how we work
% with sets.

In the next section, I will present an example of formalization that requires working with
real numbers as well as topological spaces, which provide the foundational tools for
real analysis concepts like continuity and convergence.
Topological spaces in Mathlib are built upon the concept of open sets
using the \lstinline[language=lean]|TopologicalSpace| type class,
which can be extended to define metric spaces and normed spaces.
This hierarchical organization allows definitions and theorems to be reused across
different mathematical domains:
\begin{lstlisting}[language=lean]
class TopologicalSpace (α : Type*) where
  IsOpen : Set α → Prop
  isOpen_univ : IsOpen univ
  isOpen_inter : ∀ s t, IsOpen s → IsOpen t → IsOpen (s ∩ t)
  isOpen_sUnion : ∀ s, (∀ t ∈ s, IsOpen t) → IsOpen (sUnion s)
\end{lstlisting}
Key topological concepts used in our formalization include connectedness
(formalized as \lstinline[language=lean]|IsConnected|),
path-connectedness (using \lstinline[language=lean]|IsPathConnected|
and the unit interval type \lstinline[language=lean]|unitInterval|),
continuous functions (\lstinline[language=lean]|Continuous f|,
with local variants \lstinline[language=lean]|ContinuousAt|
and \lstinline[language=lean]|ContinuousOn|), and closure operations
(\lstinline[language=lean]|closure : Set α → Set α|).
Lean treats sets as predicates
(\lstinline[language=lean]|Set α := α → Prop|),
where set membership is simply function application.


