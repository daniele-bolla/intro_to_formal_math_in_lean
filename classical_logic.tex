
\section{Classical Logic}

Logic is the study of reasoning, branching into various systems.
We refer to \textbf{classical logic} as the one that underpins much 
of traditional mathematics. 
It's the logic of the ancient Greeks (not fair) and truthtables, and it remains 
used nowadays for pedagogical reasons.
We first introduce \textbf{propositional logic}, which is the simplest 
form of classical logic. 
Later we will extend this to \textbf{predicate (or first-order) logic}, which includes 
quantifiers and predicates.
In this setting, a \textbf{proposition} is a statement that is either true or false, 
and a \textbf{proof} is a logical argument that establishes the truth of a 
proposition.
Propositions are constructed via \textbf{formulas} built from 
\textbf{propositional variables} 
(also called atomic propositions) combined with logical \textbf{connectives} such as ``and'' ($\wedge$), 
``or'' ($\vee$), ``not'' ($\neg$), ``implies'' ($\Rightarrow$), and ``if and only if'' ($\Leftrightarrow$). 
These connectives allow the creation of complex or compound propositions.
\begin{definition}[Propositional Formula](\cite{thompson1999types})
A \textbf{propositional formula} is either:
\begin{itemize}
    \item A \textbf{propositional variable}: $X_0, X_1, X_2, \ldots$, or
    \item A \textbf{compound formula} formed by combining formulas using connectives:
    \[
        (A \wedge B), \quad (A \Rightarrow B), \quad (A \vee B), \quad \bot, \quad (A \Leftrightarrow B), \quad (\neg A)
    \]
    where $A$ and $B$ are formulas themselves.
\end{itemize}
\end{definition}

We are going to describe classical logic though a formal framework 
called \textbf{natural deduction system}  developed by
Gentzen in the 1930s (\cite{wadler2015propositions}). It specifies 
rules for deriving 
\textbf{conclusions} from \textbf{premises} (assumptions from other propositions), 
called \textbf{inference rules}.

\begin{example}[Deductive style rule]
Here is an hypothetical example of inference rule.
\begin{prooftree}
  \AxiomC{$P_1$}
  \AxiomC{$P_2$}
  \AxiomC{$\cdots$}
  \AxiomC{$P_n$}
  \QuaternaryInfC{$C$}
\end{prooftree}
Where the $P_1, P_2, \ldots, P_n$, above the line, are hypothetical premises and, the hypothetical conclusion $C$ is below the line.
\end{example}
The inference rules needed are:
\begin{itemize}
    \item \textbf{Introduction rules} specify how to form compound propositions from simpler ones, and
    \item \textbf{Elimination rules} specify how to use compound propositions to derive information about their components.
\end{itemize}
Let's look at how we can define some connectives.
\paragraph{Conjunction ($\land$)}
\begin{itemize}
    \item Introduction
    \begin{prooftree}
        \AxiomC{$A$}
        \AxiomC{$B$}
        \RightLabel{$\land$-Intro}
        \BinaryInfC{$A \land B$}
    \end{prooftree}
    \item Elimination
    \noindent
      \begin{minipage}[t]{0.5\textwidth}
        \begin{prooftree}
          \AxiomC{$A \land B$}
          \RightLabel{$\land$-Elim$_1$}
          \UnaryInfC{$A$}
        \end{prooftree}
      \end{minipage}\hfill
      \begin{minipage}[t]{0.5\textwidth}
        \begin{prooftree}
          \AxiomC{$A \land B$}
          \RightLabel{$\land$-Elim$_2$}
          \UnaryInfC{$B$}
        \end{prooftree}
      \end{minipage}
\end{itemize}
\paragraph{Disjunction ($\lor$)}
\begin{itemize}
    \item Introduction
          \begin{minipage}[t]{0.5\textwidth}
    \begin{prooftree}
        \AxiomC{$A$}
        \RightLabel{$\lor$-Intro$_1$}
        \UnaryInfC{$A \lor B$}
    \end{prooftree}
      \end{minipage}\hfill
      \begin{minipage}[t]{0.5\textwidth}
    \begin{prooftree}
        \AxiomC{$B$}
        \RightLabel{$\lor$-Intro$_2$}
        \UnaryInfC{$A \lor B$}
    \end{prooftree}
      \end{minipage}
    \item Elimination (Proof by cases)
    \begin{prooftree}
        \AxiomC{$A \lor B$}
        \AxiomC{$[A] \vdash C$}
        \AxiomC{$[B] \vdash C$}
        \RightLabel{$\lor$-Elim}
        \TrinaryInfC{$C$}
    \end{prooftree}
\end{itemize}
\paragraph{Implication ($\to$)}
\begin{itemize}
    \item Introduction
    \begin{prooftree}
        \AxiomC{$[A] \vdash B$}
        \RightLabel{$\to$-Intro}
        \UnaryInfC{$A \to B$}
    \end{prooftree}
    \item Elimination (Modus Ponens)
    \begin{prooftree}
        \AxiomC{$A \to B$}
        \AxiomC{$A$}
        \RightLabel{$\to$-Elim}
        \BinaryInfC{$B$}
    \end{prooftree}
\end{itemize}

\begin{notation}
We use $A \vdash B$ (called turnstile) to designate a deduction of $B$ from $A$. 
It is employed in Gentzen’s \textbf{sequent calculus} (\cite{girard1989proofs}), 
whereas in natural deduction the corresponding symbol is\[
\begin{array}{c}
A \\
\vdots \\
B
\end{array}
\]
There are some minor differences, in fact, which I don't fully understand.
The square brackets around a premise $[A]$ mean that the premise $A$ is meant to 
be \textbf{discharged} at the conclusion. The classical example is the 
introduction rule for the implication connective.
To prove an implication $A \to B$, we assume $A$ 
(shown as $[A]$), derive $B$ under this assumption, and then discharge the 
assumption $A$ to conclude that $A \to B$ holds without the assumption. 
The turnstile is predominantly used in judgments and type theory with 
the meaning of ``entails that''. 
\end{notation}

Lean has its own syntax for connectives and their relative inference rules.
For instance $A \land B$ can be presented as \lstinline[language=lean]|And(A, B)| or \lstinline[language=lean]|A ∧ B|,
. Its untroduction rule is constructed by
 \lstinline[language=lean]|And.intro _ _| or shortly
 \lstinline[language=lean]|⟨_, _⟩| (underscore are placeholder for assumptions or "propositional functions"). 
 The pair $A \land B$ can be then consumed using elimination 
rules \lstinline[language=lean]|And.left| and \lstinline[language=lean]|And.right|.
\begin{example}\label{ex:conj_intro}
    Let's look at our first Lean example
  \begin{lstlisting}[language=lean]
    example (H_A : A) (H_B : B) : (A ∧ B) := And.intro H_A H_B
  \end{lstlisting}
Lean aims to resemble the language used in mathematics. 
For instance, when defining a function or expression, one can use keywords such as 
\lstinline[language=lean]|theorem| or \lstinline[language=lean]|def|.
Here, I used \lstinline[language=lean]|example|, which is handy for defining anonymous expressions 
for demonstration purposes. 
After that comes the statement to be proved:
\begin{lstlisting}[language=lean]
  (H_A : A) (H_B : B) : (A ∧ B) 
\end{lstlisting}
Meaning given a proof of $A$ and a proof of $B$ we can form a proof of $(A \land B)$.
The operator \lstinline[language=lean]|:=| assigns a value (or return an expression) for the statement which
 "has to be a proof of it".
\lstinline[language=lean]|And.intro| is implemented as:
\begin{lstlisting}[language=lean]
  And.intro: p → q → (p ∧ q).
\end{lstlisting}
It says: if you give me a proof of $p$ and a proof of $q$, 
then i return a proof of $p \land q$.
We therefore conclude the proof by directly giving 
\lstinline[language=lean]|And.intro H_A H_B|.
Here another way of writing the same statment.
\begin{lstlisting}[language=lean]
  example (H_p : p) (H_B : B) : And(A, B) := ⟨H_p, H_B⟩
\end{lstlisting}
\end{example}

This system of inference rules allows us to construct proofs in an 
algorithmic and systematical way, organized in what is called a \textbf{proof tree}. 
To reduce complexity, we follow a 
\textbf{top-down} approach (see \cite{thompson1999types} and 
\cite{nordstrom1990programming}).
This methodology forms the basis of \textbf{proof assistants} like Lean, 
Coq, and Agda, which help 
verify the correctness of mathematical proofs by checking each step 
against these rules.
We will see later that Lean, in fact, provides an info view of the proof tree 
which helps us 
understand and visualize 
the proof structure.
Let's examine a concrete example of a proof.
\begin{example}[Associativity of Conjunction]
We prove that $(A \land B) \land C$ implies $A \land (B \land C)$.
First, from the assumption $(A \land B) \land C$, we can derive $A$:
\begin{prooftree}
  \AxiomC{$(A \land B) \land C$}
  \RightLabel{$\land E_1$}
  \UnaryInfC{$A \land B$}
  \RightLabel{$\land E_1$}
  \UnaryInfC{$A$}
\end{prooftree}
Second, we can derive $B \land C$:
\begin{prooftree}
  \AxiomC{$(A \land B) \land C$}
  \RightLabel{$\land E_1$}
  \UnaryInfC{$A \land B$}
  \RightLabel{$\land E_2$}
  \UnaryInfC{$B$}
  \AxiomC{$(A \land B) \land C$}
  \RightLabel{$\land E_2$}
  \UnaryInfC{$C$}
  \RightLabel{$\land I$}
  \BinaryInfC{$B \land C$}
\end{prooftree}
Finally, combining these derivations we obtain $A \land (B \land C)$:
\begin{prooftree}
  \AxiomC{$(A \land B) \land C \vdash A$}
  \AxiomC{$(A \land B) \land C \vdash B \land C$}
  \RightLabel{$\land I$}
  \BinaryInfC{$A \land (B \land C)$}
\end{prooftree}
\end{example}
\begin{example}[Lean Implementation]
Let us now implement the same proof in Lean.
\begin{lstlisting}[language=Lean, caption=Associativity of Conjunction in Lean]
theorem and_associative : (A ∧ B) ∧ C → A ∧ (B ∧ C) :=
  fun h : (A ∧ B) ∧ C =>
    -- First, from the assumption (A ∧ B) ∧ C, we can derive A:
    have ab : A ∧ B := h.left -- extracts (derive) a proof of (A ∧ B) from the assumption
    have a : A := ab.left -- extracts A from (A ∧ B)
    -- Second, we can derive B ∧ C (here we only extract b and c and combine them in the next step) 
    have c : C := h.right
    have b : B := ab.right
    -- Finally, combining these derivations we obtain A ∧ (B ∧ C)
    show A ∧ (B ∧ C) from ⟨a, ⟨b, c⟩⟩
\end{lstlisting}
We introduce the \lstinline[language=lean]|theorem| with the name 
\lstinline[language=lean]|and_associative|, 
which can be referenced in subsequent proofs. 
The type signature \lstinline[language=lean]|(A ∧ B) ∧ C → A ∧ (B ∧ C)| 
represents our logical implication.
The \lstinline[language=lean]|:=| operator introduces the 
proof term that establishes the theorem's validity. 
In the previous code example this proof was directly given.
Here, we construct it using a function with the \lstinline[language=lean]|fun| keyword. 
Why a function? We have already encountered the Curry-Howard correspondence in Lean 
previously, though without explicitly stating it. 
According to this correspondence, a proof of an implication can be 
understood as a function that takes a hypothesis as input and produces 
the desired conclusion as output. We will revisit this concept in more 
detail later.
The \lstinline[language=lean]|have| keyword introduces local 
lemmas within our proof scope, allowing us to break down complex 
reasoning into manageable intermediate steps, mirroring our natural deduction proof of before.
Finally, the \lstinline[language=lean]|show| keyword presents our final result. 
\end{example}

To capture more complex mathematical ideas, we extend our system from 
propositional logic to \textbf{predicate logic}.  
A \textbf{predicate} is a statement or proposition that depends on a variable.
In propositional logic we represent a proposition simply by $P$.  
In predicate logic, this is generalized: a predicate is written as $P(a)$, 
where $a$ is a variable. Notice that a predicate is just a function.
This extension allows us to introduce \textbf{quantifiers}:  
$\forall$ (``for all'') and $\exists$ (``there exists'').  
These quantifiers express that a given formula holds either for every object 
or for at least one object, respectively.
In Lean if \lstinline[language=lean]|α| is any type, we can represent a 
predicate \lstinline[language=lean]|P| on \lstinline[language=lean]|α| as 
an object of type \lstinline[language=lean]|α → Prop|.
\lstinline[language=lean]|Prop| stands for proposition, and it is an 
essential component of Lean’s type system.
For now, we can think of it as a special type; somewhat 
paradoxically, a type of types. 
Thus given an \lstinline[language=lean]|x : α| (an element
with type \lstinline[language=lean]|α| ) 
\lstinline[language=lean]|P(x) : Prop| would be representative of a proposition holding for \lstinline[language=lean]|x|.

When introducing variables into a formal language we must keep in mind that the specific choice 
of a variable name can be substituted without 
changing the meaning of the predicate or statement. This should feel familiar from mathematics, 
where the meaning of an expression does not depend on the names we assign to variables.
Some variables are \textbf{bound} (constrained), while others remain \textbf{free} 
(arbitrary, in programming often called "dummy" variables). 
When substituting variables, it is important to ensure that this distinction is preserved.
This phenomenon, called \textbf{variable capture}, parallels familiar mathematical practice: 
if $f(x) \coloneqq \int_1^2 \frac{dt}{x-t}$, then $f(t)$ equals $\int_1^2 \frac{ds}{t-s}$, 
not the ill-defined $\int_1^2 \frac{dt}{t-t}$. The same principle applies to predicate logic. For example, consider
\[
\exists y.\,(y > x).
\]
This states that for a given $x$ there exists a $y$ such that $y > x$. 
If we naively substitute $y+1$ for $x$, we would obtain
\[
\exists y.\,(y > y+1),
\]
where the $y$ in $y+1$ has been \textbf{captured} by the quantifier $\exists y$. 
This transforms the original statement from "there exists some $y$ greater than the free variable $x$" into the always-false statement 
"there exists some $y$ greater than itself plus one."
To avoid the probelm, in the above example, we would first rename the bound variable to something fresh 
say $z$, obtaining $\exists z.\,(z > x)$, and then safely substitute to get $\exists z.\,(z > y+1)$.
We use the notation $\phi[t/x]$ for \textbf{substitution}, meaning all occurrences of the free 
variable $x$ in formulab (or expression) $\phi$ are replaced by term $t$.
We can now present the inference rules for quantifiers.
\paragraph{Universal Quantification ($\forall$)}
\begin{itemize}
    \item Introduction
    \begin{prooftree}
    \AxiomC{$A$}
    \RightLabel{$\forall I$}
    \UnaryInfC{$\forall x.\,A$}
    \end{prooftree}
    The variable $x$ must be arbitrary in the derivation of $A$. 
    This rule captures statements like 
    $\forall x \in \mathbb{N}$, $x$ has a successor, 
    but would not apply to $\forall x \in \mathbb{N}$, $x$ is prime 
    (since we cannot derive this for an arbitrary natural number).
    \item Elimination
    \begin{prooftree}
    \AxiomC{$\forall x.\,A$}
    \RightLabel{$\forall E$}
    \UnaryInfC{$A[t/x]$}
    \end{prooftree}
    The conclusion $A[t/x]$ represents the substitution of term $t$ for variable $x$ in formula $A$. 
    From a proof of $\forall x.\,A(x)$ we can infer $A(t)$ for any term $t$.
\end{itemize}
\paragraph{Existential Quantification ($\exists$)}
\begin{itemize}
    \item Introduction
    \begin{prooftree}
    \AxiomC{$A[t/x]$}
    \RightLabel{$\exists I$}
    \UnaryInfC{$\exists x.\,A$}
    \end{prooftree}
    The substitution premise means that if we can find a specific term $t$ for which $A(t)$ holds, 
    then we can introduce the existential quantifier. 
    The introduction rule requires a witness $t$ for which the predicate holds.
    \item Elimination
    \begin{prooftree}
    \AxiomC{$\exists x.\,A$}
    \AxiomC{$[A] \vdash B$}
    \RightLabel{$\exists E$}
    \BinaryInfC{$B$}
    \end{prooftree}
    To eliminate an existential quantifier, we assume $A$ holds for some witness 
    and derive $B$ without making any assumptions about the specific witness.
\end{itemize}

We can give an informal reading of the quantifiers as infinite logical operations:
\begin{align*}
\forall x.\,A(x) &\equiv A(a) \land A(b) \land A(c) \land \ldots \\
\exists x.\,A(x) &\equiv A(a) \lor A(b) \lor A(c) \lor \ldots
\end{align*}
The expression $\forall x.\, P(x)$ can be understood as  generalized form of implication. 
If $P$ is any proposition, then $\forall x.\, P$ expresses that $P$ holds 
regardless of the choice of $x$. When $p$ depends on $x$, this captures the 
idea that we can derive $p$ from any assumption about $x$.
Morover, there is a duality between universal and existential quantification.
We shall develop all this dicussions further after 
exploring their computational (type theoretical) meaning.

\begin{example}
  Lean espresses quantifiers as follow.
  \begin{lstlisting}[language=Lean, caption=For All]
  ∀ (x : X), P x
  forall (x : X), P x -- another notation
  \end{lstlisting}
  \begin{lstlisting}[language=Lean, caption=Exists]
  ∃ (x : X), P x
  exist (x : X), P x -- another notation
  \end{lstlisting}
  Where \lstinline[language=lean]|x| is a varible with a type \lstinline[language=lean]|X|,
  and \lstinline[language=lean]|P x| is a proposition, or predicate, holding for \lstinline[language=lean]|x|.
\end{example}

When introducing an \textbf{existential} proof, 
we need a \textbf{pair} consisting 
of a witness and a proof that this witness 
satisfies the statement.
\begin{lstlisting}[language=lean]
example (x : Nat) (h : x > 0) : ∃ y, y < x :=
  Exists.intro 0 h -- or shortly ⟨0, h⟩
\end{lstlisting}
The \textbf{existential elimination rule} 
( \lstinline[language=lean]|Exists.elim|) performs the opposite operation. 
It allows us to prove a proposition \(q\) 
from \(\exists x : \alpha, p\,x\) by showing 
that \(q\) follows from \(p\,w\) for an \textbf{arbitrary} 
value \(w\).
Roughly speaking, since we know there is an \(x\) 
satisfying \(p\,x\), we \textbf{name} it \(w\). 
If \(q\) does not mention \(w\), then showing that \(q\) 
follows from \(p\,w\) is tantamount to showing \(q\) 
follows from the existence of any such \(x\).
The existential rules can be interpreted as an infinite 
disjunction, 
so that existential elimination naturally corresponds to a \textbf{proof by cases} (with only one single case). 
In Lean, this reasoning is carried out using \textbf{pattern matching}, 
a known mechanism in functional programming for dealing with cases,  
with \lstinline[language=lean]|let| or \lstinline[language=lean]|match|, as well as by using \lstinline[language=lean]|cases| or 
\lstinline[language=lean]|rcases| construct. 
For example
\begin{lstlisting}[language=lean]
example (h : ∃ n : Nat, n > 0) : ∃ n : Nat, n > 0 :=
  match h with
  | ⟨witness, proof⟩ => ⟨witness, proof⟩
\end{lstlisting}
The \textbf{universal quantifier} may be regarded as a generalized function.
Accordingly, In Lean, universal elimination is simply function application.
For example:
\begin{lstlisting}[language=lean]
example : ∀ n : Nat, n ≥ 0 :=
  fun n => Nat.zero_le n
\end{lstlisting}

Functions are primitive objects in type theory.
For example it is interesting to note that a relation is expressed as a function.
\lstinline[language=lean]|R : α → α → Prop|.
Let's 

\begin{lstlisting}[language=lean]
def Transitive (α : Type) (R : α → α → Prop) : Prop :=
  ∀ x y z, R x y → R y z → R x z
\end{lstlisting}
Proof of transitivity for the less-than relation on \(\mathbb{N}\):
\begin{lstlisting}[language=lean]
theorem lt_trans_proof : Transitive (· < · : Nat → Nat → Prop) :=
  fun x y z h1 h2 => Nat.lt_trans h1 h2
\end{lstlisting}
% (Good point for introducing polymorhpism, type inference?)

% In the latter examples, we explicitly introduce variables with type 
% annotations such as \lstinline[language=lean]|n : Nat|. 
% In earlier examples, this annotation was omitted. 
% Indeed, Lean has a powerful type inference system that allows it to 
% infer the types of objects automatically.
% For instance, consider Example~\ref{ex:conj_intro}:
% \begin{lstlisting}[language=lean]
% example (H_A : A) (H_B : B) : (A ∧ B) := And.intro H_A H_B
% \end{lstlisting}
% Here, the types \(A, B\) and \(A \land B\) 
% are all inferred by Lean to belong to \(\mathtt{Prop}\).
% Another powerfull feature is polymorhpism.
% Given we can now express quantifier it may sometimes being usefull create 
% expression involving quantiifers for differen t types and not just Nat for instance.
% Polymorphims is a way to have types as vcariable. Meaning theyt can be replaced.
